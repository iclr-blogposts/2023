<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://iclr-blogposts.github.io/2023/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2023/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-09T00:20:22+02:00</updated><id>https://iclr-blogposts.github.io/2023/feed.xml</id><title type="html">ICLR Blogposts 2023</title><subtitle>Home to the 2023 ICLR Blogposts track </subtitle><entry><title type="html">Decay No More</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/adamw/" rel="alternate" type="text/html" title="Decay No More"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/adamw</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/adamw/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Weight decay is a regularization technique in machine learning which scales down the weights in every step. It dates back at least to the 1990’s and the work of Krogh and Hertz <d-cite key="Krogh1991"></d-cite> and Bos and Chug <d-cite key="Bos1996"></d-cite>.</p> <p>In <code class="language-plaintext highlighter-rouge">Pytorch</code>, weight decay is one simple line which typically is found somewhere in the <code class="language-plaintext highlighter-rouge">step</code>-method:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]:</span>
  <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">add_</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">decay</span><span class="p">)</span></code></pre></figure> <p>Subtracting a multiple of the weight can be seen as taking a step into the negative gradient direction of the squared norm of the weight. This relates weight decay to \(\ell_2\)-regularization.</p> <p>The exact mechanism of weight decay is still puzzling the machine learning community:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">The story of weight decay in pictures:<br/><br/>weight decay ...<br/>1) improves data efficiency by &gt; 50%<br/>2) is frequently found in the best hyperparam configs<br/>3) is among the most important hparams to tune<br/>4) is also tricky to tune <a href="https://t.co/PjWpk3pJxz">pic.twitter.com/PjWpk3pJxz</a></p>&mdash; Sebastian Raschka (@rasbt) <a href="https://twitter.com/rasbt/status/1614327550058328064?ref_src=twsrc%5Etfw">January 14, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">There is a gaping hole in the literature regarding the purpose of weight decay in deep learning. Nobody knows what weight decay does! AFAIK, the last comprehensive look at weight decay was this 2019 paper <a href="https://t.co/7WDBZojsm0">https://t.co/7WDBZojsm0</a>, which argued that weight decay <a href="https://t.co/qUpCbfhFRf">https://t.co/qUpCbfhFRf</a></p>&mdash; Jeremy Cohen (@deepcohen) <a href="https://twitter.com/deepcohen/status/1617274166570528769?ref_src=twsrc%5Etfw">January 22, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>The paper by Zhang et al. <d-cite key="Zhang2019"></d-cite> - which is the one mentioned in the second tweet - gives a comprehensive overview of weight decay and its effect on generalization, in particular in the interplay with Batch Normalization <code class="language-plaintext highlighter-rouge">(BN)</code> <d-cite key="Ioffe2015"></d-cite>. Batch Normalization describes a module of a network that normalizes the output of the previous layer to have zero mean and variance of one (or a variant of this with learnable mean and variance). We will not go into the details here but refer to <a href="https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/">this blog post</a> <d-cite key="pieterjan2022normalizationisdead"></d-cite> for the interested reader.</p> <p>We want to summarize two findings of <d-cite key="Zhang2019"></d-cite>:</p> <ul> <li>On the one hand, weight decay has (in theory) no effect on layers with <code class="language-plaintext highlighter-rouge">(BN)</code>. This is simply due to the fact that <code class="language-plaintext highlighter-rouge">(BN)</code> makes the output invariant to a rescaling of the weights.</li> </ul> <blockquote> Weight decay is widely used in networks with Batch Normalization (Ioffe &amp; Szegedy, 2015). In principle, weight decay regularization should have no effect in this case, since one can scale the weights by a small factor without changing the network’s predictions. Hence, it does not meaningfully constrain the network’s capacity. —Zhang et al., 2019 </blockquote> <ul> <li>However, the experiments of the paper show that weight decay on layers with <code class="language-plaintext highlighter-rouge">(BN)</code> can nevertheless improve accuracy. The authors argue that this is due to an effectively larger learning rate.</li> </ul> <p>This blog post will summarize the development of weight decay specifically for <span style="font-family:monospace">Adam</span>. We try to shed some light on the following questions:</p> <ol> <li>What is the difference between <span style="font-family:monospace">Adam</span> and its weight decay version <span style="font-family:monospace">AdamW</span>? Does the existing literature give a clear answer to the question when (and why) <span style="font-family:monospace">AdamW</span> performs better?</li> <li>Is the weight decay mechanism of <span style="font-family:monospace">AdamW</span> just <em>one more trick</em> or can we actually motivate it from an optimization perspective?</li> <li>The last section is somewhat explorational: could we come up with different formulas for a weight decay version of <span style="font-family:monospace">Adam</span>? By doing so, we will see that <span style="font-family:monospace">AdamW</span> already combines several advantages for practical use.</li> </ol> <h3 id="notation">Notation</h3> <p>We denote by \(\alpha &gt; 0\) the initial learning rate. We use \(\eta_t &gt; 0\) for a learning rate schedule multiplier. By this, the effective learning rate in iteration \(t\) is \(\alpha \eta_t\). We use \(\lambda &gt; 0\) for the weight decay parameter.</p> <h2 id="adam">Adam</h2> <p><span style="font-family:monospace">Adam</span> uses an exponentially moving average (EMA) of stochastic gradients, typically denoted by \(m_t\), and of the elementwise squared gradients, denoted by \(v_t\).</p> <p>We denote with \(\hat m_t\) and \(\hat v_t\) the EMA estimates with bias correction (see <d-cite key="Kingma2015"></d-cite>), this means</p> \[\hat m_t = \frac{m_t}{1-\beta_1^t}, \quad \hat v_t = \frac{v_t}{1-\beta_2^t}\] <p>where \(\beta_1, \beta_2 \in [0,1)\). The update formula of <span style="font-family:monospace">Adam</span> is given by</p> \[w_t = w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>How would <span style="font-family:monospace">Adam</span> handle regularization? The first approach to this was to simply add the regularization term \(\frac{\lambda}{2}\|w\|^2\) on top of the loss, do backpropagation and then compute the <span style="font-family:monospace">Adam</span> step as outlined above. This is usually referred to as <span style="font-family:monospace">AdamL2</span>. However, Loshchilov and Hutter <d-cite key="Loshchilov2019"></d-cite> showed that this can be suboptimal and one major contribution to alleviate this was the development of <span style="font-family:monospace">AdamW</span>.</p> <h2 id="adamw">AdamW</h2> <p>For training with \(\ell_2\)-regularization, Loshchilov and Hutter proposed <span style="font-family:monospace">AdamW</span> in 2019 <d-cite key="Loshchilov2019"></d-cite> as an alternative to <span style="font-family:monospace">AdamL2</span>. In the paper, the update formula is given as</p> \[\tag{AdamW} w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>While for <span style="font-family:monospace">Adam</span> several results for convex and nonconvex problems are established <d-cite key="Defossez2022, Reddi2018"></d-cite>, theoretical guarantees for <span style="font-family:monospace">AdamW</span> have been explored - to the best of our knowledge - only very recently <d-cite key="Anonymous2023"></d-cite>. Despite this, the method has enjoyed considerable practical success: for instance, <span style="font-family:monospace">AdamW</span> is implemented in the machine learning libraries Tensorflow <d-cite key="Abadi2015"></d-cite> and Pytorch <d-cite key="Paszke2019"></d-cite>. Another example is the <code class="language-plaintext highlighter-rouge">fairseq</code> library, developped by Facebook Research, which implements many SeqToSeq models. In their codebase, when <span style="font-family:monospace">Adam</span> is specified with weight decay, <span style="font-family:monospace">AdamW</span> is used by default (see <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adam.py">here</a>).</p> <p>We summarize the empirical findings of <d-cite key="Loshchilov2019"></d-cite> as follows:</p> <ul> <li> <p><span style="font-family:monospace">AdamW</span> improves generalization as compared to <span style="font-family:monospace">AdamL2</span> for image classification tasks. In the paper, the authors use a ResNet model <d-cite key="He2016"></d-cite> for the CIFAR10 and Imagenet32 dataset.</p> </li> <li> <p>Another advantage of <span style="font-family:monospace">AdamW</span> is stated in the abstract of <d-cite key="Loshchilov2019"></d-cite>:</p> </li> </ul> <blockquote> We provide empirical evidence that our proposed modification decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam [...]. —Loshchilov and Hutter, 2019 </blockquote> <p>What the authors mean by <em>decoupling</em> is that if we plot the test accuracy as a heatmap of learning rate and weight decay, the areas with high accuracy are more rectangular; the best learing rate is not too sensitive to the choice of weight decay. We illustrate this conceptually in the plot below which is inspired by Figure 2 in <d-cite key="Loshchilov2019"></d-cite>. The advantage of a decoupled method is that if one of the two hyperparameters is changed, the optimal value for the other one might still be identical and does not need to be retuned - this could reduce a 2D grid search to two 1D line searches.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-adamw/heatmap-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-adamw/heatmap-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-adamw/heatmap-1400.webp"/> <img src="/2023/assets/img/2023-05-01-adamw/heatmap.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig. 1: Heatmap of the test accuracy (bright = good accuracy) depending on learning rate and weight decay parameter choice. </div> <p>When revisiting the literature on <span style="font-family:monospace">AdamW</span> we made an interesting practical observation: the <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html">Pytorch implementation</a> of <span style="font-family:monospace">AdamW</span> is actually slightly different to the algorithm proposed in the paper. In Pytorch, the following is implemented:</p> \[w_t = (1-\eta_t \alpha \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>The difference is that the decay factor in the code is \(1-\eta_t \alpha \lambda\) instead of \(1-\eta_t \lambda\) in the paper. Clearly, this is equivalent as we can simply reparametrize the weight decay factor \(\lambda\) to make up for this. However, as the default learning rate \(\alpha=0.001\) is rather small, this means that practicioners might need to choose rather high values of \(\lambda\) in order to get sufficiently strong decay. Moreover, this leaves a certain ambiguity when tuned values for \(\lambda\) are reported in the literature.</p> <h2 id="follow-up-work">Follow-up work</h2> <p>In a recent article, Zhuang et al. revisit the <span style="font-family:monospace">AdamW</span> method and try to explain its practical success <d-cite key="Zhuang2022"></d-cite>. One of their central arguments is that <span style="font-family:monospace">AdamW</span> is approximately equal to <span style="font-family:monospace">Adam</span> with a proximal update for \(\ell_2\)-regularization.</p> <p>Before explaining this in detail, we first want to summarize the empirical findings of <d-cite key="Zhuang2022"></d-cite>:</p> <ul> <li>When <code class="language-plaintext highlighter-rouge">(BN)</code> is <em>deactivated</em>, <span style="font-family:monospace">AdamW</span> achieves better generalization compared to <span style="font-family:monospace">AdamL2</span> for image classification with a standard ResNet architecture <d-cite key="He2016"></d-cite>.</li> <li>When <code class="language-plaintext highlighter-rouge">(BN)</code> is <em>activated</em>, the test accuracy of <span style="font-family:monospace">AdamW</span> and <span style="font-family:monospace">AdamL2</span> are on par. Moreover, the best accuracy is achieved for no weight decay, i.e. \(\lambda=0\).</li> </ul> <p>The second result is somewhat stunning as it seems to contradict the results in <d-cite key="Loshchilov2019"></d-cite>, which had shown that <span style="font-family:monospace">AdamW</span> generalizes better than <span style="font-family:monospace">AdamL2</span>.<d-footnote>It seems like the AdamW-paper also used (BN) in their experiments, see https://github.com/loshchil/AdamW-and-SGDW.</d-footnote></p> <p>Comparing the details of the experimental setups, we presume the following explanations for this:</p> <ul> <li> <p>The model that is trained in <d-cite key="Loshchilov2019"></d-cite> is slightly different as it uses a Shake-Shake-Image ResNet <d-cite key="He2016, Gastaldi2017"></d-cite>.</p> </li> <li> <p>From Figure 4 in <d-cite key="Loshchilov2019"></d-cite>, one can observe that the improvement in accuracy for the CIFAR-10 dataset becomes noticeable very late in the training (see also Section 4.3 in <d-cite key="Loshchilov2019"></d-cite>). Thus, depending on the number of epochs after which training is stopped, one can reach different conclusions.</p> </li> </ul> <h2 id="proxadam">ProxAdam</h2> <p>The paper by Zhuang et al. <d-cite key="Zhuang2022"></d-cite> does not only compare <span style="font-family:monospace">AdamL2</span> to <span style="font-family:monospace">AdamW</span> experimentally, but it also provides a mathematical motivation for weight decay. In order to understand this, we first need to introduce the <strong>proximal operator</strong>, a central concept of convex analysis.</p> <h3 id="a-short-introduction-to-proximal-operators">A short introduction to proximal operators</h3> <p>Proximal algorithms have been studied for decades in the context of (non-smooth) optimization, way before machine learning was a thing. The groundwork of this field has been laid by R. Tyrrell Rockafellar from the 1970’s onwards <d-cite key="Rockafellar1976,Rockafellar1998"></d-cite>. If \(\varphi: \mathbb{R}^n \to \mathbb{R}\) is convex then the proximal operator is defined as</p> \[\mathrm{prox}_\varphi(x) := \mathrm{argmin}_{z \in \mathbb{R}^n} \varphi(z) + \frac12 \|z-x\|^2.\] <p>If \(\varphi\) is non-smooth, we can not simply compute a gradient step - hence we have to deal with non-smooth terms in a different way. For many classical regularization functions (e.g. the \(\ell_1\)-norm), the proximal operator can be computed in closed form. This makes it a key ingredient of optimization algorithms for non-smooth, regularized problems. Assume that we want to minimize the sum of a differentiable loss \(f\) and a convex regularizer \(\varphi\), i.e.</p> \[\min_{w \in \mathbb{R}^n} f(w) + \varphi(w).\] <p>The proximal gradient method in this setting has the update formula</p> \[w_{t} = \mathrm{prox}_{\alpha \varphi}\big(w_{t-1}- \alpha \nabla f(w_{t-1})\big),\] <p>where \(\alpha&gt;0\) is a step size (<em>aka</em> learning rate). An equivalent way of writing this (which will become useful later on) is<d-footnote>This can be proven using the definition of the proximal operator and completing the square.</d-footnote></p> \[\tag{1} w_{t} = \mathrm{argmin}_y \langle y-w_{t-1}, \nabla f(w_{t-1})\rangle + \varphi(y) + \frac{1}{2\alpha}\|y-w_{t-1}\|^2.\] <h3 id="weight-decay-as-a-proximal-operator">Weight decay as a proximal operator</h3> <p>For \(\ell_2\)-regularization \(\varphi(w) = \frac{\lambda}{2}\|w\|^2\), the proximal operator at \(w\) is given by \(\frac{1}{1+\lambda}w = (1-\frac{\lambda}{1+\lambda})w\). Based on this, the authors of <d-cite key="Zhuang2022"></d-cite> propose a proximal version of <span style="font-family:monospace">Adam</span> called <span style="font-family:monospace">ProxAdam</span>. It is given by</p> \[\tag{ProxAdam} w_t = \big(1- \frac{\lambda\eta_t}{1+\lambda\eta_t} \big)w_{t-1} - \frac{\eta_t \alpha}{1+\lambda\eta_t} \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>Knowing this, we can now understand why <span style="font-family:monospace">AdamW</span> is approximately a proximal version of <span style="font-family:monospace">Adam</span>. Using the first-order Taylor-approximation \(\frac{ax}{1+bx}\approx ax\) for small \(x\), applied to the coefficients in front of \(w_{t-1}\) and \(\frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\) gives the formula</p> \[w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\] <p>which is equal to <span style="font-family:monospace">AdamW</span>. The argument we just presented is exactly how <d-cite key="Zhuang2022"></d-cite> concludes that <span style="font-family:monospace">AdamW</span> \(\approx\) <span style="font-family:monospace">ProxAdam</span>.</p> <h3 id="changing-the-norm">Changing the norm</h3> <p>There is one more way of interpreting proximal methods. Let us begin with a simple example: Define the diagonal matrix \(D_t := \mathrm{Diag}(\epsilon + \sqrt{\hat v_t})\). Then, the <span style="font-family:monospace">Adam</span> update can be equivalently written<d-footnote>This can be proven by first-order optimality and solving for $w_t$. We will do a similar calculation further below.</d-footnote> as</p> \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>In other words, <span style="font-family:monospace">Adam</span> takes a proximal step of a linear function, but with the adaptive norm \(D_t\). This change in norm is what makes <span style="font-family:monospace">Adam</span> different from <span style="font-family:monospace">SGD</span> with (heavy-ball) momentum.</p> <p>The update formula of <span style="font-family:monospace">ProxAdam</span> can also be written as a proximal method:</p> \[\tag{P1} w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2\alpha}\|y\|_{D_t}^2 + \frac{1}{2 \eta_t \alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>In fact, the first-order optimality conditions of (P1) are</p> \[0 = \hat m_t + \frac{\lambda}{\alpha} D_t w_t + \frac{1}{\eta_t \alpha}D_t (w_t-w_{t-1}).\] <p>Solving for \(w_t\) (and doing simple algebra) gives</p> \[\tag{2} w_t = (1+\lambda \eta_t)^{-1}\big[w_{t-1} - \eta_t \alpha D_t^{-1} \hat m_t\big]\] <p>which is equal to <span style="font-family:monospace">ProxAdam</span>.</p> <p>What is slightly surprising here is the term \(\alpha^{-1}\|y\|_{D_t}^2\) in (P1) - we might have expected the regularization term to be used with the standard \(\ell_2\)-norm. This leads us to our final section.</p> <h2 id="adamw-is-scale-free"><span style="font-family:monospace">AdamW</span> is scale-free</h2> <p>As an alternative to (P1), we could replace \(\alpha^{-1}\|y\|_{D_t}^2\) by \(\|y\|^2\) and update</p> \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2}\|y\|^2 + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>Again, setting the gradient of the objective to zero and solving for \(w_t\) we get</p> \[w_t = \big(\mathrm{Id} + \eta_t \lambda \alpha D_t^{-1}\big)^{-1} \big[w_{t-1} - \eta_t\alpha D_t^{-1} \hat m_t \big].\] <p>Comparing this to (2) we see that the second factor is the same, but the decay factor now also depends on \(D_t\) and \(\alpha\). Let us call this method <span style="font-family:monospace">AdamP</span>.</p> <p>Now the natural question is whether <span style="font-family:monospace">AdamP</span> or <span style="font-family:monospace">ProxAdam</span> (or <span style="font-family:monospace">AdamW</span> as its approximation) would be superior. One answer to this is that we would prefer a <em>scale-free</em> algorithm: with this we mean that if the loss function would be multiplied by a positive constant, we could still run the method with exactly the same parameters and obtain the same result. <span style="font-family:monospace">Adam</span> for example is scale-free and in <d-cite key="Zhuang2022"></d-cite> it is explained that <span style="font-family:monospace">ProxAdam</span>/<span style="font-family:monospace">AdamW</span> are, too. The reason for this is the following: looking at (P1) we see that if the loss is scaled by \(c&gt;0\), then \(\hat m_t\) and \(D_t\) are scaled by \(c\) (if we neglect the \(\epsilon\) in \(D_t\)). Hence, the objective in (P1) is multiplied by \(c\) which implies that <span style="font-family:monospace">ProxAdam</span> for \(\epsilon=0\) is invariant to scaling for the same values of \(\lambda,\alpha,\eta_t\). Now, for (P2) the story is different, as here the second term \(\frac{\lambda}{2}\|y\|^2\) is not scaled by \(c\), but the other terms are. We would need to rescale \(\lambda\) by \(c\) to obtain the identical update. As a consequence, <span style="font-family:monospace">AdamP</span> would <strong>not be scale-free</strong> and this makes it less attractive as a method. We should point out that scale-freeness is rather a practical advantage that requires less tuning when changing the model or dataset - it does not imply that the test accuracy would be different when both methods are tuned.</p> <p>To verify this, we ran a simple experiment on a ResNet20 for CIFAR10 with <code class="language-plaintext highlighter-rouge">(BN)</code> deactivated. For <span style="font-family:monospace">AdamW</span> (the <code class="language-plaintext highlighter-rouge">Pytorch</code> version) and <span style="font-family:monospace">AdamP</span> we tested the learning rates <code class="language-plaintext highlighter-rouge">[1e-3,1e-2,1e-1]</code> and weight decay <code class="language-plaintext highlighter-rouge">[1e-5,1e-4,1e-3,1e-2]</code>. From the plots below, we can see that both methods approximately achieve the same accuracy for the best configurations<d-footnote>The best configurations all have learning rate 1e-3.</d-footnote>. The only difference - in this very simple example - is that <span style="font-family:monospace">AdamP</span> seems to arrive at a model with smaller norm for the configurations with high accuracy (see right plot). Hence, its regularization seems to be stronger.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-adamw/resnet20val_score-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-adamw/resnet20val_score-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-adamw/resnet20val_score-1400.webp"/> <img src="/2023/assets/img/2023-05-01-adamw/resnet20val_score.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-adamw/resnet20model_norm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-adamw/resnet20model_norm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-adamw/resnet20model_norm-1400.webp"/> <img src="/2023/assets/img/2023-05-01-adamw/resnet20model_norm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For the sake of completeness, we also add a <code class="language-plaintext highlighter-rouge">Pytorch</code> implementation of <span style="font-family:monospace">AdamP</span> in the <a href="#appendix">Appendix</a>.</p> <h2 id="summary">Summary</h2> <ul> <li> <p>Weight decay can be seen as a proximal way of handling \(\ell_2\)-regularization. Therefore, it is not a different <em>type</em> of regularization itself but rather a different <em>treatment</em> of regularization in the optimization method. As a consequence, <span style="font-family:monospace">AdamW</span> is an (almost) proximal version of <span style="font-family:monospace">Adam</span>.</p> </li> <li> <p>Whether or not weight decay brings advantages when used <em>together with</em> <code class="language-plaintext highlighter-rouge">(BN)</code> seems to depend on several factors of the model and experimental design. However, in all experiments we discussed here <span style="font-family:monospace">AdamW</span> performed better or at least on par to <span style="font-family:monospace">AdamL2</span>.</p> </li> <li> <p>The second conclusion suggests that proximal algorithms such as <span style="font-family:monospace">AdamW</span> seem to be favourable. Together with the scale-free property that we described in the final section, this makes <span style="font-family:monospace">AdamW</span> a robust method and explains its practical success.</p> </li> </ul> <h2 id="acknowledgements">Acknowledgements</h2> <p>Special thanks go to Robert M. Gower and the anonymous reviewers for their constructive feedback.</p> <p><a name="appendix"></a></p> <h2 id="appendix">Appendix</h2> <p>Below you find a <code class="language-plaintext highlighter-rouge">Pytorch</code> implementation of <span style="font-family:monospace">AdamP</span>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>


<span class="k">class</span> <span class="nc">AdamP</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">
    Arguments:
        params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
        lr (float, optional): learning rate (default: 1e-3)
        betas (Tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.9, 0.999))
        eps (float, optional): term added to the denominator to improve
            numerical stability (default: 1e-8)
        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
        
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid learning rate: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid epsilon value: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid beta parameter at index 0: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid beta parameter at index 1: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">weight_decay</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid weight_decay value: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">))</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">_init_lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="k">return</span>
   

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        </span><span class="sh">"""</span>
        
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization
</span>                <span class="k">if</span> <span class="sh">'</span><span class="s">step</span><span class="sh">'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values
</span>                    <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
                    <span class="c1"># Exponential moving average of squared gradient values
</span>                    <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
                    
                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg</span><span class="sh">'</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">exp_avg_sq</span><span class="sh">'</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">betas</span><span class="sh">'</span><span class="p">]</span>

                <span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">state</span><span class="p">[</span><span class="sh">'</span><span class="s">step</span><span class="sh">'</span><span class="p">]</span>

                
                <span class="c1"># Decay the first and second moment running average coefficient
</span>                <span class="n">exp_avg</span><span class="p">.</span><span class="nf">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">).</span><span class="nf">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span>
                <span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">).</span><span class="nf">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span>
                <span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">div</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)).</span><span class="nf">sqrt</span><span class="p">().</span><span class="nf">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">eps</span><span class="sh">'</span><span class="p">])</span>

                <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">]</span>
                <span class="n">lmbda</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="sh">'</span><span class="s">weight_decay</span><span class="sh">'</span><span class="p">]</span>

                <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">lr</span><span class="o">/</span><span class="n">bias_correction1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lmbda</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">div_</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">lr</span><span class="o">*</span><span class="n">lmbda</span><span class="o">/</span><span class="n">D</span><span class="p">)</span> <span class="c1"># adaptive weight decay
</span>
            

        <span class="k">return</span> <span class="n">loss</span></code></pre></figure>]]></content><author><name>Fabian Schaipp</name></author><summary type="html"><![CDATA[Weight decay is among the most important tuning parameters to reach high accuracy for large-scale machine learning models. In this blog post, we revisit AdamW, the weight decay version of Adam, summarizing empirical findings as well as theoretical motivations from an optimization perspective.]]></summary></entry><entry><title type="html">Autoregressive Renaissance in Neural PDE Solvers</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/autoregressive-neural-pde-solver/" rel="alternate" type="text/html" title="Autoregressive Renaissance in Neural PDE Solvers"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/autoregressive-neural-pde-solver</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/autoregressive-neural-pde-solver/"><![CDATA[<h2 id="introduction">Introduction</h2> <blockquote> Improving PDE solvers has trickle down benefits to a vast range of other fields. </blockquote> <p>Partial differential equations (PDEs) play a crucial role in modeling complex systems and understanding how they change over time and in space.</p> <p>They are used across physics and engineering, modeling a wide range of physical phenomena like heat transfer, sound waves, electromagnetism, and fluid dynamics, but they can also be used in finance to model the behavior of financial markets, in biology to model the spread of diseases, and in computer vision to model the processing of images.</p> <p>They are particularly interesting in deep learning!</p> <ol> <li><span style="color:#9444e2;">Neural networks can be used to model complex PDEs.</span></li> <li><span style="color:#9444e2;">Embedding knowledge of a PDE into a neural network can help it generalize better and/or use less data</span></li> <li><span style="color:#9444e2;">PDEs can help explain, interpret, and design neural networks.</span></li> </ol> <p>Despite their long history dating back to equations first formalized by Euler over 250 years ago, finding numerical solutions to PDEs continues to be a challenging problem.</p> <p>The recent advances in machine learning and artificial intelligence have opened up new possibilities for solving PDEs in a more efficient and accurate manner. These developments have the potential to revolutionize many fields, leading to a better understanding of complex systems and the ability to make more informed predictions about their behavior.</p> <p>The background and problem set up precedes a brief look into classical and neural solvers, and finally discusses the message passing neural PDE solver (MP-PDE) introduced by Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>.</p> <h2 id="background">Background</h2> <h3 id="lets-brush-up-on-the-basics">Let's brush up on the basics…</h3> <p><em>The notation and definitions provided match those in the paper for consistency, unless otherwise specified.</em></p> <div> <p> Ordinary differential equations (ODEs) describe how a function changes with respect to a <span style="color:#9444e2">single independent variable</span> and its derivatives. In contrast, PDEs are mathematical equations that describe the behavior of a dependent variable as it changes with respect to <span style="color:#9444e2">multiple independent variables</span> and their derivatives. </p> <p> Formally, for one time dimension and possibly multiple spatial dimensions denoted by \(\textbf{x}=[x_{1},x_{2},x_{3},\text{...}]^{\top} \in \mathbb{X}\), a general (temporal) PDE may be written as </p> <p> $$\partial_{t}\textbf{u}= F\left(t, \textbf{x}, \textbf{u},\partial_{\textbf{x}}\textbf{u},\partial_{\textbf{xx}}\textbf{u},\text{...}\right) \qquad (t,\mathbf{x}) \in [0,T] \times \mathbb{X}$$ </p> <p> The \(\partial\) is a partial derivative operator which can be understood as "a small change in". For example, the \(\partial_{t}\textbf{u}\) term refers to how much an infinitesmally small change in \(t\) changes \(\textbf{u}\). Below is an explicit definition for some arbitrary function \(f(x,y)\): $$\frac{\partial f(x,y)}{\partial x} = \lim_{h \to 0} \frac{f(x+h,y) - f(x,y)}{h}$$ </p> <ul> <li>Initial condition: \(\mathbf{u}(0,\mathbf{x})=\mathbf{u}^{0}(\mathbf{x})\) for \(\mathbf{x} \in \mathbb{X}\)</li> <li>Boundary conditions: \(B[ \mathbf{u}](t,x)=0\) for \((t,\mathbf{x}) \in [0,T] \times \partial \mathbb{X}\)</li> </ul> </div> <div class="fake-img l-gutter"> <p> Many equations are solutions to such PDEs alone. For example, the wave equation is given by \(\partial_{tt}u = \partial_{xx}u\). You will find that any function in the form \(u(x,t)=F(x-ct)+\) \(G(x+ct)\) is a potential solution. Initial conditions are used to specify how a PDE "starts" in time, and boundary conditions determine the value of the solution at the boundaries of the region where the PDE is defined. </p> </div> <details><summary>Types of boundary conditions</summary> Dirichlet boundary conditions prescribe a fixed value of the solution at a particular point on the boundary of the domain. Neumann boundary conditions, on the other hand, prescribe the rate of change of the solution at a particular point on the boundary. There are also mixed boundary conditions, which involve both Dirichlet and Neumann conditions, and Robin boundary conditions, which involve a linear combination of the solution and its derivatives at the boundary. </details> <p><br/></p> <div class="l-body-outset"> <iframe src="/2023/assets/html/2023-05-01-autoregressive-neural-pde-solver/slider.html" frameborder="0" scrolling="no" height="750px" width="100%"></iframe> </div> <div class="caption"> Example of the wave equation PDE \(\partial^{2}_{t}u = c^{2}\partial^{2}_ {\mathbf{x}}u\) solved using finite differences. Drag the slider to watch it evolve in time! </div> <p>The study of PDEs is in itself split into many broad fields. Briefly, these are two other important properties in addition to the initial and boundary conditions:</p> <details><summary>Linearity</summary> <ul> <li>Linear: the highest power of the unknown function appearing in the equation is one (i.e., a linear combination of the unknown function and its derivatives)</li> <li>Nonlinear: the highest power of the unknown function appearing in the equation is greater than one</li> </ul> </details> <p><br/></p> <details><summary>Homogeneity</summary> For an example PDE \(u_t - u_xx = f(x, t)\): <ul> <li>Homogeneous: PDEs with no constant terms (i.e., the right-hand side \(f(x,t)=0\)) and express a balance between different physical quantities</li> <li>Inhomogeneous: PDEs with a non-zero constant term \(f(x,t)\neq0\) on the right-hand side and describe how an external factor affects the balance</li> </ul> </details> <p><br/></p> <p>PDEs can be either linear or nonlinear, homogeneous or inhomogeneous, and can contain a combination of constant coefficients and variable coefficients. They can also involve a variety of boundary conditions, such as Dirichlet, Neumann, and Robin conditions, and can be solved using analytical, numerical, or semi-analytical methods <d-cite key="straussPartialDifferentialEquations2007"></d-cite>.</p> <hr style="width:40%"/> <p>Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite> follow precedence set by Li et al. <d-cite key="liFourierNeuralOperator2021"></d-cite> and Bar-Sinai et al. <d-cite key="bar-sinaiLearningDatadrivenDiscretizations2019"></d-cite>to focus on <span style="color:#9444e2;">PDEs written in conservation form</span>:</p> <p style="text-align:center;"> \(\partial_{t} \mathbf{u} + \nabla \cdot \mathbf{J}(\mathbf{u}) = 0\) </p> <ul> <li><p>\(J\) is the flux, or the amount of some quantity that is flowing through a region at a given time</p> </li> <li><p>\(\nabla \cdot J\) is the divergence of the flux, or the amount of outflow of the flux at a given point</p> </li> </ul> <p>Additionally, they consider <span style="color:#9444e2;">Dirichlet and Neumann</span> boundary conditions.</p> <h3 id="solving-pdes-the-classical-way">Solving PDEs the classical way</h3> <p>A brief search in a library will find numerous books detailing how to solve various types of PDEs. </p> <details><summary>Analytical methods: an exact solution to a PDE can be found by mathematical means <d-cite key="straussPartialDifferentialEquations2007"></d-cite>.</summary><br/> <ul> <li>Separation of Variables<ul> <li>This method involves expressing the solution as the product of functions of each variable, and then solving each function individually. It is mainly used for linear PDEs that can be separated into two or more ordinary differential equations.</li> </ul> </li> <li>Green&#39;s Functions<ul> <li>This method involves expressing the solution in terms of a Green&#39;s function, which is a particular solution to a homogeneous equation with specified boundary conditions.</li> </ul> </li> </ul> </details> <p><br/></p> <details><summary>Semi-analytical methods: an analytical solution is combined with numerical approximations to find a solution <d-cite key="bartelsNumericalApproximationPartial"></d-cite>.</summary><br/> <ul> <li>Perturbation methods<ul> <li>This method is used when the solution to a PDE is close to a known solution or is a small deviation from a known solution. The solution is found by making a perturbation to the known solution and solving the resulting equation analytically.</li> </ul> </li> <li>Asymptotic methods<ul> <li>In this method, the solution is represented as a series of terms that are solved analytically. The solution is then approximated by taking the leading terms of the series.</li> </ul> </li> </ul> </details> <p><br/></p> <blockquote> Very few PDEs have analytical solutions, so numerical methods have been developed to approximate PDE solutions over a wider range of potential problems. </blockquote> <h4 id="numerical-methods">Numerical Methods</h4> <p>Often, approaches for temporal PDEs follow the <span style="color:#9444e2;">method of lines (<abbr title="method of lines">MOL</abbr>)</span>.</p> <p>Every point of the discretization is then thought of as a separate ODE evolving in time, enabling the use of ODE solvers such as Runge-Kutta methods.</p> <details><summary>1. Discretizing the problem</summary><br/> <p> In the most basic case (<span style="color:#9444e2;">a regular grid</span>), arbitrary spatial and temporal resolutions \(\mathbf{n_{x}}\) and \(n_{t}\) can be chosen and thus used to create a grid where \(\mathbf{n_{x}}\) is a vector containing a resolution for each spatial dimension. </p> <hr style="width:40%"/> <p> The domain may also be <span style="color:#9444e2;">irregularly sampled, resulting in a grid-free discretization</span>. This is often the case with real-world data that comes from scattered sensors, for example. </p> <p>Finite difference methods (FDMs) or any other discretization technique can be used to discretize the time domain. </p> <p> One direction of ongoing research seeks to determine discretization methods which can result in more efficient numerical solvers (for example, take larger steps in flatter regions and smaller steps in rapidly changing regions). </p> </details> <p><br/></p> <details><summary>2. Estimating the spatial derivatives</summary><br/> <p> A popular choice when using a gridded discretization is the <span style="color:#9444e2;">finite difference method (FDM)</span>. Spatial derivative operators are replaced by a stencil which indicates how values at a finite set of neighboring grid points are combined to approximate the derivative at a given position. This stencil is based on the Taylor series expansion. </p> <p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/fdm_animation.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/fdm_animation.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/fdm_animation.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/fdm_animation.gif" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> <div class="caption"> Credits: Augusto Peres, Inductiva <d-cite key="HeatHeatEquation"></d-cite>. </div> <hr style="width:40%"/> <p> The <span style="color:#9444e2;">finite volume method (FVM)</span> is another approach which works for irregular geometries. Rather than requiring a grid, the computation domain can be divided into discrete, non-overlapping control volumes used to compute the solution for that portion <d-cite key="bartelsNumericalApproximationPartial"></d-cite>. </p> <p> For every control volume, a set of equations describing the balance of some physical quantities (in essence, estimating the flux at control volume boundaries) can be solved which results in the approximated spatial derivative. </p> <p> While this method <span style="color:#9444e2;">only works for conservation form equations</span>, it can handle complex problems with irregular geometries and fluxes that are difficult to handle with other numerical techniques such as the <abbr title="finite difference method">FDM</abbr>. </p> <hr style="width:40%"/> <p> In the <span style="color:#9444e2;">pseudospectral method (PSM)</span>, PDEs are solved pointwise in physical space by using basis functions to approximate the spatial derivatives <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. The pseudospectral method and the Galerkin method are two common examples of spectral methods which use basis functions satisfying various conditions depending on the specific algorithm being applied. While the <abbr title="finite difference method">FDM</abbr> considers local information to construct approximations, spectral methods determine global solutions and have exponential convergence. </p> <p> These methods are well-suited for solving problems with <span style="color:#9444e2;">smooth solutions and periodic boundary conditions</span>, but their performance drops for irregular or non-smooth solutions, as well as problems with more degrees of freedom where their global nature results in high dimensional dense matrix computations. </p> </details> <p><br/></p> <details><summary>3. Time updates</summary><br/> The resulting problem is a set of temporal ODEs which can be solved with classical ODE solvers such as any member of the Runge-Kutta method family. </details> <p><br/></p> <h4 id="limitations-of-classical-methods">Limitations of Classical Methods</h4> <p>The properties of a PDE, such as its order, linearity, homogeneity, and boundary conditions, determine its solution method. <span style="color:#9444e2;">Different methods have been developed based on the different properties and requirements of the problem at hand.</span> Brandstetter at al. categorizes these requirements into the following <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>:</p> <div> <table> <thead> <tr> <th>User</th> <th>Structural</th> <th>Implementational</th> </tr> </thead> <tbody> <tr> <td>Computation efficiency, computational cost, accuracy, guarantees (or uncertainty estimates), generalization across PDEs</td> <td>Spatial and temporal resolution, boundary conditions, domain sampling regularity, dimensionality</td> <td>Stability over long rollouts, preservation of invariants</td> </tr> </tbody> </table> <p> The countless combinations of requirements resulted in what Bartels defines as a <span style="color:#9444e2;">splitter field</span> <d-cite key="bartelsNumericalApproximationPartial"></d-cite>: a specialized classical solver is developed for each sub-problems, resulting in many specialized tools rather than a single one. </p> <p> These methods, while effective and mathematically proven, often come at high computation costs. Taking into account that PDEs often exhibit chaotic behaviour and are sensitive to any changes in their parameters, <span style="color:#ff4f4b;">re-running a solver every time a coefficient or boundary condition changes in a single PDE can be computationally expensive</span>. </p> <p> One key example which limits grid-based classical solvers is the <span style="color:#9444e2;">Courant-Friedrichs-Lewy (CFL) condition</span>, which states that the maximum time step size should be proportional to the minimum spatial grid size. According to this condition, as the number of dimensions increases, the size of the temporal step must decrease and therefore numerical solvers become very slow for complex PDEs. </p> </div> <table> <thead> <tr> <th>Algorithm</th> <th>Equation</th> <th>Boundary conditions</th> <th>Complexity</th> </tr> </thead> <tbody> <tr> <td>Classical FDM/FEM/FVM</td> <td>general</td> <td>general</td> <td>poly\(((\frac{1}{\varepsilon})^{d})\)</td> </tr> <tr> <td>Adaptive FDM/FEM <d-cite key="babuskaHpVersionFinite1987"></d-cite></td> <td>general</td> <td>general</td> <td>poly\(((\log(\frac{1}{\varepsilon}))^{d})\)</td> </tr> <tr> <td>Spectral method <d-cite key="gheorghiuSpectralMethodsDifferential2007,shenSpectralMethodsAlgorithms2011"></d-cite></td> <td>general</td> <td>general</td> <td>poly\(((\log(\frac{1}{\varepsilon}))^{d})\)</td> </tr> <tr> <td>Sparse grid FDM/FEM <d-cite key="bungartzSparseGrids2004,zengerSparseGrids1991"></d-cite></td> <td>general</td> <td>general</td> <td>poly\(((\frac{1}{\varepsilon})(\log(\frac{1}{\varepsilon}))^{d})\)</td> </tr> <tr> <td>Sparse grid spectral method <d-cite key="shenEfficientSpectralSparse2010,shenEfficientSpectralSparse2012"></d-cite></td> <td>elliptic</td> <td>general</td> <td>poly\((\log(\frac{1}{\varepsilon})(\log \log(\frac{1}{\varepsilon}))^{d})\)</td> </tr> </tbody> </table> <div class="caption"> Table showing (polynomial) computational complexity of some common numerical methods, including finite difference method (FDM), finite elements method (FEM), finite volume method (FVM), spectral method, and some of their variants for \(d\)-dimensional PDEs with error tolerance ε. Note that every method has an exponential dependency on the dimenAdapted from <d-cite key="childsHighprecisionQuantumAlgorithms2021"></d-cite>. </div> <h3 id="neural-solvers">Neural Solvers</h3> <p> Neural solvers offer some very desirable properties that may serve to unify some of this splitter field. Neural networks can <span style="color:#9444e2;">learn and generalize to new contexts</span> such as different initial/boundary conditions, coefficients, or even different PDEs entirely <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. They can also circumvent the CFL condition, making them a promising avenue for solving highly complex PDEs such as those found in weather prediction. For a review which contextualizes physics informed machine learning with regards to classical problems and methods, see <d-cite key="mengWhenPhysicsMeets2022"></d-cite> </p> <p> Though most methods lie along a spectrum from classical leaning to end-to-end neural, a naive yet illustrative categorization into three groupings is shown below. </p> <p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/PDEchart-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/PDEchart-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/PDEchart-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/PDEchart.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> <h4 id="fully-neuraluniversal-function-approximators">Fully Neural/Universal Function Approximators</h4> <p>The term fully neural here refers to methods which rely on the universal function approximation theory such that a sufficiently complex network can represent any arbitrary function. Many common fully neural methods are also known as neural operators which <span style="color:#9444e2;">model the solution of a PDE as an operator that maps inputs to outputs</span>. The problem is set such that a neural operator \(\mathcal{M}\) satisfies \(\mathcal{M}(t,\mathbf{u}^{0}) = \mathbf{u}(t)\) where \(\mathbf{u}^{0}\) are the initial conditions <d-cite key="luDeepONetLearningNonlinear2021, brandstetterMessagePassingNeural2022a"></d-cite>. The idea of using deep learning techniques to solve differential equations has a long history, including Dissanayake’s and Phan-Thien’s attempt to use <abbr title="multilayer perceptron">MLP</abbr>s as universal approximators to solve PDEs, and arguably includes any work involving incorporating prior knowledge into models in general <d-cite key="dissanayakeNeuralnetworkbasedApproximationsSolving1994,psichogiosHybridNeuralNetworkfirst1992,lagarisArtificialNeuralNetworks1998"></d-cite>. Simple <abbr title="multilayer perceptron">MLP</abbr>s, CNNs, RNNs, and other networks used to map input vectors to output vectors are naive examples of finite-dimensional operators.</p> <p>Raissi et al. officially coined the physics-informed neural network (PINN) in 2017 <d-cite key="raissiPhysicsinformedNeuralNetworks2019"></d-cite>. The problem is set such that the network \(\mathcal{N}\) satisfies \(\mathcal{N}(t,\mathbf{u}^{0}) = \mathbf{u}(t)\) where \(\mathbf{u}^{0}\) are the initial conditions. The main principle behind <abbr title="physics informed neural network">PINN</abbr>s is to enforce the governing physical laws of the problem on the network’s predictions by adding loss term(s) to the network’s objective function.</p> <p>For a typical loss function \(\theta = \text{argmin}_{\theta} \mathcal{L}(\theta)\)</p> <p>the loss with a physics prior may be defined as follows:</p> \[\mathcal{L}(\theta) = \omega_{\mathcal{F}} \mathcal{L}_{\mathcal{F}}(\theta) + \omega_{\mathcal{B}} \mathcal{L}_{\mathcal{B}}(\theta) + \omega_{d} \mathcal{L}_{\text{data}}(\theta)\] <table> <thead> <tr> <th>Term</th> <th>Definition</th> <th>Effect</th> <th> </th> </tr> </thead> <tbody> <tr> <td>\(\mathcal{L}_{\mathcal{B}}\)</td> <td>Loss wrt. the initial and/or boundary conditions</td> <td>Fits the known data over the network</td> <td> </td> </tr> <tr> <td>\(\mathcal{L}_{\mathcal{F}}\)</td> <td>Loss wrt. the PDE</td> <td>Enforces DE \(\mathcal{F}\) at collocation points; Calculating using autodiff to compute derivatives of \(\mathbf{\hat{u}_{\theta}(\mathbf{z})}\)</td> <td> </td> </tr> <tr> <td>\(\mathcal{L}_{\text{data}}\)</td> <td>Validation of known data points</td> <td>Fits the known data over the NN and forces \(\mathbf{\hat{u}}_{\theta}\) to match measurements of \(\mathbf{u}\) over provided points</td> <td>–&gt;</td> </tr> </tbody> </table> <p>Since the network maps input variables to output variables which are both finite-dimensional and dependent on the grid used to discretize the problem domain, it is considered a finite dimensional neural operator. The paper gained a lot of traction and inspired many architectures which now fall under the <abbr title="physics informed neural network">PINN</abbr> family; for a more thorough review see <d-cite key="cuomoScientificMachineLearning2022"></d-cite>, and for <a href="https://www.physicsbaseddeeplearning.org/intro.html">hands-on examples visit this digital book</a> <d-cite key="thuereyPhysicsbasedDeepLearning2022"></d-cite>.</p> <p>The success of this loss-based approach is apparent when considering the rapid growth of papers which extend the original iteration of the <abbr title="physics informed neural network">PINN</abbr>. However, Krishnapriyan et al. <d-cite key="krishnapriyanCharacterizingPossibleFailure2021"></d-cite> has shown that even though standard fully-connected neural networks are theoretically capable of representing any function given enough neurons and layers, a <abbr title="physics informed neural network">PINN</abbr> may still fail to approximate a solution due to the complex loss landscapes arising from soft PDE constraints.</p> <p>The DeepONet architecture is a seminal example of an infinite dimensional neural operator in contrast to the finite dimensional <abbr title="physics informed neural network">PINN</abbr> <d-cite key="luDeepONetLearningNonlinear2021"></d-cite>. It consists of one or multiple branch net(s) which encode discrete inputs to an input function space, and a single trunk net which receives the query location to evaluate the output function. The model maps from a fixed, finite dimensional grid to an infinite dimensional output space.</p> <p>Since the development of the DeepONet, many novel neural operators have emerged which generalize this finite-infinite dimensional mapping to an infinite-infinite dimensional mapping<d-cite key="liNeuralOperatorGraph2020,liPhysicsinformedNeuralOperator2021,goswamiPhysicsInformedDeepNeural2022,rahmanUshapedNeuralOperators2022,tripuraWaveletNeuralOperator2022,fanaskovSpectralNeuralOperators2022,pathakFourCastNetGlobalDatadriven2022"></d-cite>, including the <span style="color:#9444e2;">Fourier Neural Operator (FNO)</span> <d-cite key="liFourierNeuralOperator2021"></d-cite>. It operates within Fourier space and takes advantage of the convolution theorem to place the integral kernel in Fourier space as a convolutional operator.</p> <div> <p> These global integral operators (implemented as Fourier space convolutional operators) are combined with local nonlinear activation functions, resulting in an architecture which is <span style="color:#9444e2;">highly expressive yet computationally efficient, as well as being resolution-invariant</span>. </p> <p> While the vanilla <abbr title="Fourier neural operator">FNO</abbr> required the input function to be defined on a grid due to its reliance on the FFT, further work developed mesh-independent variations as well <d-cite key="kovachkiNeuralOperatorLearning2022"></d-cite>. </p> </div> <div class="fake-img l-gutter"> <p> Convolution Theorem </p> <p> The Fourier transform of the convolution of two signals is equal to the pointwise product of their individual Fourier transforms </p> </div> <p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/FNO-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/FNO-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/FNO-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/FNO.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> <div class="caption"> <abbr title="Fourier neural operator">FNO</abbr> architecture. For more details, see <a href="https://zongyi-li.github.io/blog/2020/fourier-pde/">this blogpost</a>. Credits: Li et al. <d-cite key="liFourierNeuralOperator2021"></d-cite>. </div> <p> Neural operators are able to operate on multiple domains and can be completely data-driven. </p> <p> However, these models <span style="color:#ff4f4b;">do not tend to predict out-of-distribution \(t\)</span> and are therefore limited when dealing with temporal PDEs. Another major barrier is their relative <span style="color:#ff4f4b;">lack of interpretability and guarantees</span> compared to classical solvers. </p> <h4 id="neural-augmented-classical-methods">Neural-Augmented Classical Methods</h4> <p>A parallel line of research involves using deep learning as a tool to improve classical numerical methods for solving PDEs. One avenue involves modifying existing iterative methods: while neural operator methods directly mapped inputs to outputs, <span style="color:#9444e2;">autoregressive methods take an iterative approach instead</span>. For example, iterating over time results in a problem such as \(\mathbf{u}(t+\Delta t) = \mathcal{A}(\Delta t, \mathbf{u}(t))\) where \(\mathcal{A}\) is some temporal update <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>.</p> <div class="l-body-outset"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div class="vertical-center" style="background-color:white"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/rnn-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/rnn-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/rnn-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/rnn.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/wavenet.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/wavenet.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/wavenet.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/wavenet.gif" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Similarly to <abbr title="recurrent neural networks">RNN</abbr>s (left), autoregressive models take previous time steps to predict the next time step. However, autoregressive models (right) are entirely feed-forward and take the previous predictions as inputs rather than storing them in some hidden state. Credits: RNN diagram from Colah's Blog <d-cite key="UnderstandingLSTMNetworks"></d-cite>, WaveNet from Deepmind Blog <d-cite key="WaveNetGenerativeModel"></d-cite> </div> </div> <p>Three autoregressive systems mentioned by Brandstetter et al. are hybrid methods which use neural networks to predict certain parameters for finite volume, multigrid, and iterative finite elements methods. <span style="color:#9444e2;">All three retain a (classical) computation grid which makes them somewhat interpretable</span> <d-cite key="bar-sinaiLearningDatadrivenDiscretizations2019, greenfeldLearningOptimizeMultigrid2019a, hsiehLearningNeuralPDE2019"></d-cite>.</p> <div class="fake-img l-gutter"> <p> Other autoregressive models include PixelCNN for images, WaveNet for audio, and the Transformer for text. </p> </div> <p>Hsieh et al. <d-cite key="hsiehLearningNeuralPDE2019"></d-cite>, for example, develops a neural network-accelerated iterative finite elements method. Most significantly, their approach offers theoretical guarantees of convergence and correctness. Their problem formulation focuses on solving a single linear PDE class for variable discretization, boundary conditions, and source/forcing terms. For any PDE with an existing linear iterative solver, a learned iterator can replace a handcrafted classical iterator.</p> <p>Similarly, Um et al. <d-cite key="umSolverintheLoopLearningDifferentiable2020a"></d-cite> proposed using a neural network component to learn the error or deviation from the path of an iterative solver. Using this component, the iterative method can be “pushed” back onto the true PDE solution.</p> <p>Another way deep learning can be leveraged in classical methods is characterized by <d-cite key="meurisMachinelearningbasedSpectralMethods2023"></d-cite> and also highlights the deeply interconnected nature of these novel developments. The conventional spectral method rewrites a PDE in terms of the sum of basis functions; Meuris et al. use a DeepONet to discover candidate functions to be used as basis functions. Though mathematical work is required to mold the extracted function (from the DeepONet) to a basis function satisfying certain desirable properties, it expands the use of the spectral method toward complex domains where we might not have known appropriate basis functions.</p> <p>However, augmented classical systems have not gained the acclaim seen by their fully neural counterparts as a whole.</p> <p>This is on one hand due to their <span style="color:#ff4f4b;">limitations in generalization</span>. In Hsieh et al.’s case, an existing numerical method must be used to craft a complementary neural iterator <d-cite key="hsiehLearningNeuralPDE2019"></d-cite>. Another major concern is the <span style="color:#ff4f4b;">accumulation of error</span> in iterative methods, which is particularly detrimental for PDE problems that often exhibit chaotic behavior <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. Overarching both neural component and neural-optimized methods, however, is the tradeoff between marginal improvements to classical methods and what tends to be a non-trivial amount of manual work required to implement such methods.</p> <h4 id="classical-inspired-neural-methods">Classical-Inspired Neural Methods</h4> <p>Ruthotto and Haber released an impactful study in 2018 which interprets residual neural networks (ResNets) as PDEs, and addresses some of their challenges using PDE theory <d-cite key="ruthottoDeepNeuralNetworks2018"></d-cite>. A standard ResNet has skip connections which in effect add a previous layer’s output directly to the calculation of a future layer’s output. Given input features \(\mathbf{Y}_{0}=\mathbf{Y}\) and a ResNet with \(N\) layers, the output of the \(j\)th layer is used to calculate that of the next:</p> \[\mathbf{Y}_{j+1}=\mathbf{Y}_{j}+f(\theta^{(j)},\mathbf{Y}_{j})\] <p>This formulation also describes a typical forward Euler discretization with a step size \(\delta_{t}=1\). Based on this continuous interpretation of a ResNet layer, PDEs from control theory can be used to develop novel networks with specific and expected behaviours like smoothing or even memory reduction <d-cite key="ruthottoDeepNeuralNetworks2018"></d-cite>.</p> <p>This is an example of a strong classical-inspired neural method which allowed us to systematically develop novel architectures. Since then, PDE interpretations of neural network architectures have been expanded to encompass embedding PDEs into architectures themselves, and building architectures to mimic classical PDE solvers.</p> <p>The Graph Neural Diffusion (GRAND) model introduced by Chamberlain et al. demonstrates that <span style="color:#9444e2;">graph neural networks (GNNs) can be crafted using differential equations</span> (like diffusion processes) where the spatial derivative is analogous to the difference between node features, and the temporal update is a continuous counterpart to the layer index <d-cite key="chamberlainGRANDGraphNeural2021a"></d-cite>. From these two principles and their derivations of diffusion PDEs on graphs, Chamberlain et al. design networks which ameliorate common <abbr title="graph neural network">GNN</abbr> pitfalls like oversmoothing (which occurs as the number of layers increases). Note that here, the emphasis is not in outputting the solution of a PDE directly but rather using a PDE to influence or bias the output toward an expected result, somewhat more similarly to how a <abbr title="physics informed neural network">PINN</abbr> biases the output to obey a specified PDE.</p> <p>Later, the PDE-GCN model extends GRAND by deriving differential operators on manifolds which are then discretized on graphs to then build not only diffusion, but hyperbolic PDE-inspired <abbr title="graph neural network">GNN</abbr>s as well <d-cite key="eliasofPdegcnNovelArchitectures2021"></d-cite>. The discretized nonlinear diffusion and nonlinear hyperbolic PDEs call back to Ruthotto et al.’s comparison to ResNet updates and are used to define the titular PDE-inspired graph convolutional network (GCN) layer. Interestingly, mixing both diffusion and hyperbolic variants can allow one to discover which is more prominent to a task by retrieving a parameter which weights how much one network dynamic contributes to the output.</p> <p>This category of models highlights the diverse ways that PDEs are used in deep learning. Not only can these networks be tested on mathematical datasets, but they provide valuable interpretations and performance improvements when used in non-geometric tasks like node classification and even protein-protein interactions in biology.</p> <h2 id="message-passing-neural-pde-solver-mp-pde">Message Passing Neural PDE Solver (MP-PDE)</h2> <p>Brandstetter et al. propose a <span style="color:#9444e2;">fully neural PDE solver which capitalizes on neural message passing</span>. The overall architecture is laid out below, consisting of an <abbr title="multilayer perceptron">MLP</abbr> encoder, a <abbr title="graph neural network">GNN</abbr> processor, and a CNN decoder.</p> <div class="l-body-outset" style="background-color:white"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/MP-PDE-Solver-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/MP-PDE-Solver-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/MP-PDE-Solver-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/MP-PDE-Solver.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Overall MP-PDE architecture. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> <p>At its core, this model is autoregressive and thus faces the same challenge listed above. Two key contributions of this work are the <span style="color:#9444e2;">pushforward trick and temporal bundling which mitigate the potential butterfly effect of error accumulation</span><d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. The network itself, being fully neural, is capable of generalization across many changes as well.</p> <h3 id="the-pushforward-trick-and-temporal-bundling">The Pushforward Trick and Temporal Bundling</h3> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/pushforward3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/pushforward3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/pushforward3-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/pushforward3.jpg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Pushforward trick compared to one-step and unrolled training. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> </div> <p>During testing, the model uses current time steps (first from data, then <span style="color:#9444e2;">from its own predictions</span>) to approximate the next time step.</p> <p>This results in a distribution shift problem because the inputs are no longer solely from ground truth data: <span style="color:#9444e2;">the distribution learned during training will always be an approximation of the true data distribution</span>. The model will appear to overfit to the one-step training distribution and perform poorly the further it continues to predict.</p> <p>An adversarial-style stability loss is added to the one-step loss so that the training distribution is brought closer to the test time distribution <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>:</p> <details><summary style="text-align:center;"><span style="summary-math"> \(L_{\text{one-step}} =\) <span style="color:#23a15c;">\(\mathbb{E}_{k}\)</span> <span style="color:#928b54;">\(\mathbb{E}_{\mathbf{u^{k+1}|\mathbf{u^{k},\mathbf{u^{k} \sim p_{k}}}}}\)</span> \([\) <span style="color:#5588e0;">\(\mathcal{L}\)</span> \((\) <span style="color:#9444e2;">\(\mathcal{A}(\mathbf{u}^{k})\)</span> \(,\) <span style="color:#46b4af;">\(\mathbf{u}^{k+1}\)</span>\(]\) </span> </summary> <p> The <span style="color:#5588e0;">loss function</span> is used to evaluate the difference between the <span style="color:#9444e2;">temporal update</span> and the <span style="color:#46b4af;">expected next state</span>, and the overall one-step loss is calculated as the expected value of this loss over <span style="color:#23a15c;">all time-steps</span> and <span style="color:#928b54;">all possible next states</span>. </p> </details> <p><br style="line-height:5px"/></p> <p style="text-align:center;"> \(L_{\text{stability}} = \mathbb{E}_{k}\mathbb{E}_{\mathbf{u^{k+1}|\mathbf{u^{k},\mathbf{u^{k} \sim p_{k}}}}}[\mathbb{E}_{\epsilon | \mathbf{u}^{k}} [\mathcal{L}(\mathcal{A}(\mathbf{u}^{k}+\) <span style="color:#faad18;">\(\epsilon\)</span> \()),\mathbf{u}^{k+1}]]\) </p> <p style="text-align:center;"> \(L_{\text{total}} = L_{\text{one-step}} + L_{\text{stability}}\) </p> <p> The stability loss is largely based off the one-step loss, but now assumes that the temporal update uses <span style="color:#faad18;">noisy data</span>. </p> <p> The pushforward trick lies in the choice of <span style="color:#faad18;">\(\epsilon\)</span> such that \(\mathbf{u}^{k}+\epsilon = \mathcal{A}(\mathbf{u}^{k-1})\), similar to the test time distribution. Practically, it is implemented to be <span style="color:#9444e2;">noise from the network itself</span> so that as the network improves, the loss decreases. </p> <p> Necessarily, the noise of the network must be known or calculated to implement this loss term. So, <span style="color:#9444e2;">the model is unrolled for 2 steps</span> but only backpropagated over the most recent unroll step, which already has the neural network noise <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. In essence, the one-step training has a clean input and noisy output whereas the pushforward trick has both noisy input and noisy output with the \(\epsilon\) term capturing the noise. </p> <p> While the network could be unrolled during training, this not only slows the training down but also might result in the network learning shortcuts across unrolled steps. </p> <p><strong>Temporal bundling</strong></p> <div class="row mt-3"> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/NN-AR-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/NN-AR-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/NN-AR-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/NN-AR.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/temporalbundling-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/temporalbundling-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/temporalbundling-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/temporalbundling.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Temporal bundling compared to neural operators and autoregressive models. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> <p>This trick complements the previous by <span style="color:#9444e2;">reducing the amount of times the test time distribution changes</span>. Rather than predicting a single value at a time, the MP-PDE predicts multiple time-steps at a time, as seen above <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>.</p> <h3 id="network-architecture">Network Architecture</h3> <p><abbr title="graph neural network">GNN</abbr>s have been used as PDE solvers in a variety of works <d-cite key="liNeuralOperatorGraph2020, eliasofPdegcnNovelArchitectures2021, iakovlevLearningContinuoustimePDEs2021"></d-cite>; however, in this implementation, <span style="color:#9444e2;">links can be drawn directly from the <abbr title="method of lines">MOL</abbr> to each component of the network architecture centering around the use of a message passing algorithm.</span></p> <table> <thead> <tr> <th>Classical Numerical Method</th> <th>MP-PDE Network Component</th> </tr> </thead> <tbody> <tr> <td>Partitioning the problem onto a grid</td> <td>Encoder <br/><em>Encodes a vector of solutions into node embeddings</em></td> </tr> <tr> <td>Estimating the spatial derivatives</td> <td>Processor <br/><em>Estimates spatial derivatives via message passing</em></td> </tr> <tr> <td>Time updates</td> <td>Decoder <br/><em>Combines some representation of spatial derivatives smoothed into a time update</em></td> </tr> </tbody> </table> <ol> <li>Encoder <p> The encoder is implemented as a two-layer <abbr title="multilayer perceptron">MLP</abbr> which computes an embedding for each node \(i\) to cast the data to a <span style="color:#9444e2;">non-regular integration grid</span>: </p> <details><summary style="text-align:center"><span style="summary-math"> \(\mathbf{f}_{i}^{0} = \epsilon^{v}([\mathbf{u}_{i}^{k-K:k},\mathbf{x}_{i},t_{k},\theta_{PDE}])\) </span> </summary> where \(\mathbf{u}_{i}^{k-K:k}\) is a vector of previous solutions (the length equaling the temporal bundle length), \(\mathbf{x}_{i}\) is the node's position, \(t_{k}\) is the current timestep, and \(\theta_{PDE}\) holds equation parameters. </details> </li> <li> Processor <p> The node embeddings from the encoder are then used in a message passing <abbr title="graph neural network">GNN</abbr>. <a id="spatialderivative" style="text-decoration:none;">The message passing algorithm, which approximates spatial derivatives, is run \(M\) steps using the following updates:</a> </p> <details><summary style="text-align:center"><span style="summary-math"> \(\text{edge } j \to i \text{ message:} \qquad \mathbf{m}_{ij}^{m} =\) <span style="color:#ae46b4;">\(\phi\)</span> \((\) <span style="color:#b4a546;">\(\mathbf{f}_{i}^{m}, \mathbf{f}_{j}^{m},\)</span> <span style="color:steelblue;">\(\mathbf{u}_{i}^{k-K:k}-\mathbf{u}_{j}^{k-K:k}\)</span>, <span style="color:#6546b4;">\(\mathbf{x}_{i}-\mathbf{x}_{j}\)</span>, <span style="color:#46b4af;">\(\theta_{PDE}\)</span> \())\) </span> </summary> The <span style="color:#6546b4;">difference in spatial coordinates</span> helps enforce translational symmetry and, combined with the <span style="color:steelblue;">difference in node solutions</span>, relates the message passing to a local difference operator. The addition of the <span style="color:#46b4af;">PDE parameters</span> is motivated by considering what the MP-PDE should generalize over: by adding this information in multiple places, flexibility can potentially be learned since all this information (as well as the <span style="color:#b4a546;">node embeddings</span>) is fed through <span style="color:#ae46b4;">a two-layer <abbr title="multilayer perceptron">MLP</abbr></span>. In addition, the solution of a PDE at any timestep must respect the boundary condition (the same as in classical methods for BVPs), so adding the <span style="color:#46b4af;">PDE parameters</span> in the edge update provides knowledge of the boundary conditions to the neural solver. </details> <br/> <details><summary style="text-align:center;"><span style="summary-math"> \(\text{node } i \text{ update:} \qquad\) <span style="color:#ff4f4b;">\(\mathbf{f}_{i}^{m+1}\)</span> \(=\) <span style="color:#928b54;">\(\psi\)</span> \((\) <span style="color:#5588e0;">\(\mathbf{f}^{m}_{i}\)</span>, <span style="color:#722e4e;">\(\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}^{m}\)</span>, <span style="color:#46b4af;">\(\theta_{PDE}\)</span> \()\) </span> </summary> The <span style="color:#ff4f4b;">future node embedding</span> is updated using <span style="color:#5588e0;">the current node embedding</span>, <span style="color:#722e4e;">the aggregation of all received messages</span>, and (again) the <span style="color:#46b4af;">PDE parameters</span>. This information is also fed through <span style="color:#928b54;">a two-layer <abbr title="multilayer perceptron">MLP</abbr></span>. </details><br/> <p> Bar-Sinai et al. explores the relationship between <abbr title="finite difference method">FDM</abbr> and <abbr title="finite volume method">FVM</abbr> as used in the method of lines <d-cite key="bar-sinaiLearningDatadrivenDiscretizations2019"></d-cite>. In both methods, the \(n^{th}\) order derivative at a point \(x\) is approximated by </p> <p style="text-align:center;"> \(\partial^{(n)}_{x}u \approx \sum_{i} a^{(n)}_{i} u_{i}\) </p> <p> for some precomputed coefficients \(a^{(n)}_{i}\). <span style="color:#9444e2;">The right hand side parallels the message passing scheme</span>, which aggregates the local difference (<span style="color:steelblue;">\(\mathbf{u}_{i}^{k-K:k}-\mathbf{u}_{j}^{k-K:k}\)</span> in the edge update) and other (learned) embeddings over neighborhoods of nodes. </p> <p> This relationship gives an intuitive understanding of the message passing <abbr title="graph neural network">GNN</abbr>, which mimics <abbr title="finite difference method">FDM</abbr> for a single layer, <abbr title="finite volume method">FVM</abbr> for two layers, and <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> for three layers <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> is a numerical interpolation scheme used to reconstruct the solution at cell interfaces in <abbr title="finite volume method">FVM</abbr>. </p> <p> While the interpretation is desirable, how far this holds in the actual function of the <abbr title="message passing graph neural network">MP-GNN</abbr> is harder to address. The concepts of the nodes as integration points and messages as local differences break down as the nodes and edges update. In addition, the furthest node that contributes a message from for any point is at \(n\) edges away for the \(n^{th}\) layer (or a specified limit). This results in a very coarse and potentially underinformed approximation for the first layer which is then propagated to the next layers. However, both the updates use two layer <abbr title="multilayer perceptron">MLP</abbr>s which (although abstracting away from their respective interpretations) may in effect learn optimal weightings to counterbalance this. </p> </li> <li> Decoder <p> The approximated spatial derivatives are then <span style="color:#9444e2;">combined and smoothed using a 1D CNN</span> which outputs a bundle of next time steps (recall temporal bundling) \(\mathbf{d}_{i}\). The solution is then updated: </p> <p style="text-align:center;"> \(\mathbf{u}^{k+l}_{i} = u^{k}_{i} + (t_{k+l}-t_{k})\mathbf{d}^{l}_{i}\) </p> <p> Some precedence is seen, for example, in classical linear multistep methods which (though effective) face stability concerns. Since the CNN is adaptive, it appears that it avoids this issue <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </p> </li> </ol> <h3 id="results">Results</h3> <details><summary>Quantitative measures: accumulated error, runtime</summary> <p> Accumulated error: \(\frac{1}{n_{x}} \sum_{x,t} MSE\) </p> <p> Runtime (s): Measured time taken to run for a given number of steps. </p> </details> <blockquote> As a general neural PDE solver, the <abbr title="message passing graph neural network">MP-GNN</abbr> surpasses even the current state-of-the-art <abbr title="Fourier neural operator">FNO</abbr>. </blockquote> <p>For example, after training a neural model and setting up an instance of <abbr title="method of lines">MOL</abbr>, this is a brief comparison of how they can generalize without re-training.</p> <table> <thead> <tr> <th>Generalization to...</th> <th><abbr title="message passing graph neural network">MP-GNN</abbr></th> <th><abbr title="Fourier neural operator">FNO</abbr></th> <th>Classical (<abbr title="method of lines">MOL</abbr>)</th> </tr> </thead> <tbody> <tr> <td>New PDEs</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Different resolutions</td> <td>Yes</td> <td>Yes</td> <td>No (unless downsampling)</td> </tr> <tr> <td>Changes in PDE parameters</td> <td>Yes</td> <td>Yes</td> <td>Sometimes</td> </tr> <tr> <td>Non-regular grids</td> <td>Yes</td> <td>Some</td> <td>Yes (dependent on implementation)</td> </tr> <tr> <td>Higher dimensions</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> </tbody> </table> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/shock_formation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/shock_formation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/shock_formation-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/shock_formation.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Demonstration of shock formation using MP-PDE from different training data resolutions. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> </div> <p>This experiment exemplifies the MP-PDE’s ability to model shocks (where both the <abbr title="finite difference method">FDM</abbr> and PSM methods fail) across multiple resolutions. Even at a fifth of the resolution of the ground truth, both the small and large shocks are captured well.</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/2dshock-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/2dshock-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/2dshock-1400.webp"/> <img src="/2023/assets/img/2023-05-01-autoregressive-neural-pde-solver/2dshock.jpg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Demonstration of shock formation using MP-PDE from different training data resolutions. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> </div> <p>The same data is displayed in 2D to show the time evolution. After about 7.5s, the error accumulation is large enough to visibly diverge from the ground truth. The predictions become unreliable due to error accumulation.</p> <p>In practice, this survival time should be empirically found (as seen here) to determine how long the solution is reliable. However, the ground truth would be needed for comparison, rendering this as another chicken-egg problem.</p> <table> <thead> <tr> <th colspan="2"></th> <th colspan="4" style="border-left:1px solid lightgrey;">Accumulated Error</th> <th colspan="2" style="border-left:1px solid lightgrey;">Runtime [s]</th> </tr> </thead> <tbody> <tr> <td colspan="2"> \(\quad (n_{t},n_{x})\) </td> <td style="border-left:1px solid lightgrey;">WENO5</td> <td>FNO-RNN</td> <td style="border-left:1px solid lightgrey;">FNO-PF</td> <td>MP-PDE</td> <td style="border-left:1px solid lightgrey;">WENO5</td> <td>MP-PDE</td> </tr> <tr> <td><b>E1</b></td> <td>(250,100)</td> <td style="border-left:1px solid lightgrey;">2.02</td> <td>11.93</td> <td style="border-left:1px solid lightgrey;">0.54</td> <td>1.55</td> <td style="border-left:1px solid lightgrey;">1.9</td> <td>0.09</td> </tr> <tr> <td><b>E1</b></td> <td>(250, 50)</td> <td style="border-left:1px solid lightgrey;">6.23</td> <td>29.98</td> <td style="border-left:1px solid lightgrey;">0.51</td> <td>1.67</td> <td style="border-left:1px solid lightgrey;">1.8</td> <td>0.08</td> </tr> <tr> <td><b>E1</b></td> <td>(250, 40)</td> <td style="border-left:1px solid lightgrey;">9.63</td> <td>10.44</td> <td style="border-left:1px solid lightgrey;">0.57</td> <td>1.47</td> <td style="border-left:1px solid lightgrey;">1.7</td> <td>0.08</td> </tr> <tr> <td><b>E2</b></td> <td>(250, 100)</td> <td style="border-left:1px solid lightgrey;">1.19</td> <td>17.09</td> <td style="border-left:1px solid lightgrey;">2.53</td> <td>1.58</td> <td style="border-left:1px solid lightgrey;">1.9</td> <td>0.09</td> </tr> <tr> <td><b>E2</b></td> <td>(250, 50)</td> <td style="border-left:1px solid lightgrey;">5.35</td> <td>3.57</td> <td style="border-left:1px solid lightgrey;">2.27</td> <td>1.63</td> <td style="border-left:1px solid lightgrey;">1.8</td> <td>0.09</td> </tr> <tr> <td><b>E2</b></td> <td>(250, 40)</td> <td style="border-left:1px solid lightgrey;">8.05</td> <td>3.26</td> <td style="border-left:1px solid lightgrey;">2.38</td> <td>1.45</td> <td style="border-left:1px solid lightgrey;">1.7</td> <td>0.08</td> </tr> <tr> <td><b>E3</b></td> <td>(250, 100)</td> <td style="border-left:1px solid lightgrey;">4.71</td> <td>10.16</td> <td style="border-left:1px solid lightgrey;">5.69</td> <td>4.26</td> <td style="border-left:1px solid lightgrey;">4.8</td> <td>0.09</td> </tr> <tr> <td><b>E3</b></td> <td>(250, 50)</td> <td style="border-left:1px solid lightgrey;">11.71</td> <td>14.49</td> <td style="border-left:1px solid lightgrey;">5.39</td> <td>3.74</td> <td style="border-left:1px solid lightgrey;">4.5</td> <td>0.09</td> </tr> <tr> <td><b>E3</b></td> <td>(250, 40)</td> <td style="border-left:1px solid lightgrey;">15.97</td> <td>20.90</td> <td style="border-left:1px solid lightgrey;">5.98</td> <td>3.70</td> <td style="border-left:1px solid lightgrey;">4.4</td> <td>0.09</td> </tr> </tbody> </table> <div class="caption"> Table of experiment results adapted from paper. Credits: Brandstetter et al. <d-cite key="brandstetterMessagePassingNeural2022a"></d-cite>. </div> <details><summary>Abbreviations</summary> <table> <thead> <tr> <th>Shorthand</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td><strong>E1</strong></td> <td>Burgers&#39; equation without diffusion</td> </tr> <tr> <td><strong>E2</strong></td> <td>Burgers&#39; equation with variable diffusion</td> </tr> <tr> <td><strong>E3</strong></td> <td>Mixed equation, see below</td> </tr> <tr> <td>\(n_{t}\)</td> <td>Temporal resolution</td> </tr> <tr> <td>\(n_{x}\)</td> <td>Spatial resolution</td> </tr> <tr> <td>WENO5</td> <td>Weighted Essentially Non-Oscillatory (5th order)</td> </tr> <tr> <td><abbr title="Fourier neural operator">FNO</abbr>-<abbr title="recurrent neural networks">RNN</abbr></td> <td>Recurrent variation of <abbr title="Fourier neural operator">FNO</abbr> from original paper</td> </tr> <tr> <td><abbr title="Fourier neural operator">FNO</abbr>-PF</td> <td><abbr title="Fourier neural operator">FNO</abbr> with the pushforward trick added</td> </tr> <tr> <td>MP-PDE</td> <td>Message passing neural PDE solver</td> </tr> </tbody> </table> <p> The authors form a general PDE in the form </p> <p style="text-align:center;"> \([\partial_{t}u + \partial_{x}(\alpha u^{2} - \beta \partial_{x} u + \gamma \partial_{xx} u)](t,x) = \delta (t,x)\) </p> <p style="text-align:center;"> \(u(0,x) = \delta(0,x)\) </p> <p> such that \(\theta_{PDE} = (\alpha, \beta, \gamma)\) and different combinations of these result in the heat equation, Burgers' equation, and the KdV equation. \(\delta\) is a forcing term, allowing for greater variation in the equations being tested. </p> </details> <p>For this same experiment, the error and runtimes were recorded when solving using <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr>, the recurrent variant of the <abbr title="Fourier neural operator">FNO</abbr> (<abbr title="Fourier neural operator">FNO</abbr>-<abbr title="recurrent neural networks">RNN</abbr>), the <abbr title="Fourier neural operator">FNO</abbr> with the pushforward trick (<abbr title="Fourier neural operator">FNO</abbr>-PF), and the MP-PDE.</p> <blockquote> The pushforward trick is successful in mitigating error accumulation. </blockquote> <p>Comparing the accumulated errors of <abbr title="Fourier neural operator">FNO</abbr>-<abbr title="recurrent neural networks">RNN</abbr> and the <abbr title="Fourier neural operator">FNO</abbr>-PF across all experiments highlights the advantage of the pushforward trick. While the MP-PDE outperforms all other tested methods in the two generalization experiments <strong>E2</strong> and <strong>E3</strong>, the <abbr title="Fourier neural operator">FNO</abbr>-PF is most accurate for <strong>E1</strong>.</p> <p>When solving a single equation, the <abbr title="Fourier neural operator">FNO</abbr> likely performs better, though both <abbr title="Fourier neural operator">FNO</abbr>-PF and MP-PDE methods outperform <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr>.</p> <blockquote> Neural solvers are resolution-invariant. </blockquote> <p>As \(n_{x}\) is decreased, <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> performs increasingly worse whereas all the neural solvers remain relatively stable.</p> <blockquote> Neural solver runtimes are constant to resolution. </blockquote> <p>Additionally, the runtimes of <abbr title="Weighted Essentially Non-Oscillatory (5th order)">WENO5</abbr> decrease (likely proportionally) since fewer steps require fewer calculations, but the MP-PDE runtimes again appear relatively stable.</p> <h3 id="comparing-interpretations">Comparing Interpretations</h3> <p>The way the MP-PDE is constructed parallels how both GRAND and the PDE-GCN are built. All three architectures follow a basic premise of mirroring the <abbr title="method of lines">MOL</abbr> and describe certain mechanisms in their respective systems which mimic spatial discretisations and temporal discretisations.</p> <p>The spatial derivative is discretized by a <abbr title="graph neural network">GNN</abbr> in the MP-PDE and by the message passing algorithm (consisting of node and edge updates within one layer of a <abbr title="graph neural network">GNN</abbr>) in the GRAND and PDE-GCN. In the MP-PDE, the spatial derivatives are in effect parameterized by the node and edge updates (the former which Brandstetter et al. highlight takes the difference in solutions \(u_{i}=u_{j}\)) detailed above, both of which are generic <abbr title="multilayer perceptron">MLP</abbr>s. In comparison, both GRAND and PDE-GCN (using the diffusion variant) come to comparable formulas when discretising using the forward Euler method.</p> <p>The GRAND paper derives the following, where \(\tau\) is a temporal step, \(\mathbf{x}\) is the diffusion equation, and \(\mathbf{A}\) is the attention matrix <d-cite key="chamberlainGRANDGraphNeural2021a"></d-cite>:</p> \[\mathbf{x}^{(k+1)}=(\mathbf{I} + \tau \mathbf{A}(\mathbf{x}^{(k)}))\mathbf{x}^{(k)}\] <p>which, when modified, results in:</p> \[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)} + \tau \mathbf{x}^{(k)} \mathbf{A}(\mathbf{x}^{(k)})\] <p>The PDE-GCN defines manifold operators discretized onto graphs. The update is defined as the following, where \(\mathbf{G}\) is the gradient operator, \(\mathbf{K}\) is a \(1 \times 1\) trainable convolution kernel, \(\sigma\) is the activation function, \(\tau\) is the temporal step, and \(\mathbf{x}\) is the diffusion equation <d-cite key="eliasofPdegcnNovelArchitectures2021"></d-cite>:</p> \[\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}-\tau \mathbf{G}^{T} \mathbf{K}^{T}_{k} \sigma (\mathbf{K}_{k} \mathbf{G} \mathbf{x}^{(k)})\] <p>The structure of these latter two models shares many similarities, though where GRAND naturally results in a graph attention network, the PDE-GCN results in a graph convolutional network.</p> <p>The temporal update for the MP-PDE relies on the 1D CNN outputting a temporal bundle, whereas GRAND and PDE-GCN regard their respective layer indexes to be the discretised time steps.</p> <p>These are examples of how spatial and temporal discretisations can result in unique architectures. The PDE-GCN outperforms GRAND on at least two out of three out of the popular Cora, SiteSeer, and PubMed benchmarks. However, the MP-PDE has a different objective altogether; while the PDE-GCN and GRAND output a single graph result (which is fed through a convolutional layer for node classification tasks), the MP-PDE iteratively produces results through time. This iterative requirement also requires that the temporal update must be retrievable and therefore must diverge from Ruthotto et al.’s original interpretation of time steps as layers adopted by the other two models. The MP-PDE instead appears to rely on the neural networks in both node and edge updates to learn spatial derivatives over multiple layers. An interesting experiment would be to apply the other two techniques to the same testing data as PDE-GCN and compare accuracies at a specific point in time (see future directions).</p> <h2 id="conclusion">Conclusion</h2> <h4 id="future-directions">Future Directions</h4> <p>The authors conclude by discussing some future directions.</p> <p>For example, the MP-PDE can be modified for <span style="color:#9444e2;">PDE <em>retrieval</em> (which they call parameter optimization)</span>. There is some precedence for this: Cranmer et al. develop a method which fits a symbolic regression model (eg.: PySR, eureqa) to the learned internal functions of a GNN <d-cite key="cranmerDiscoveringSymbolicModels2020"></d-cite>. Alternatively, the MP-PDE’s capacity for generalization means that biasing the model with a prior to determine coefficients could be as simple as training on an example instance of the predicted equation, fitting this model on real world data (much like a finetuning process), and extracting the \(\theta_{PDE}\) parameters.</p> <p>The one-step loss which is the basis of the <span style="color:#9444e2;">adversarial-style loss</span> is also used in reinforcement learning, which frequently uses deep autoregressive models. Other formulations which borrow from reinforcement learning (where distribution shifts are quite common) and other fields could prove successful as well. Transformer-based natural language processing are now capable of capturing extremely long sequence dependencies and generating coherent long-form text. Since <a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">Transformers are GNNs</a> which use attention to aggregate neighborhoods, this may be a viable avenue to explore.</p> <p><span style="color:#9444e2;">Adaptive time stepping</span> is another avenue which could make the model more efficient and accurate by taking large steps over stable/predictable solution regions and smaller steps over changing/unpredictable solution regions. The choice of a CNN for the decoder works well over regular inputs and outputs, but other options like attention-based architectures could potentially weigh the outputted node embeddings such that the model might learn different time steps. Some care would have to be taken with temporal bundling in this case, since the resulting vectors would be potentially irregular in time.</p> <p>In addition, while the GRAND architecture is designed for a single output, adapting it to suit an iterative solver may prove fruitful since the attention mechanism would encode spatial awareness. The motivation for this choice is that a sparse attention matrix might be able to provide a more global solution.</p> <h4 id="ongoing-challenges">Ongoing Challenges</h4> <p>While there are numerous diverse branches of development, key challenges remain:</p> <ul> <li>(Unified and) appropriate evaluation metrics <ul> <li>Currently, mean squared error (or root mean squared error) is implemented as the choice of loss in not only MP-PDE, but most named networks herein. However, it is unclear whether this is the best measure of correctness to solving a PDE since the specific values of the solution evaluated at the discretised points will depend on the discretisation method. An interesting further study would be to use the MP-PDE and test it on data generated from multiple numerical solvers. Additionally, Brandstetter et al. identify a metric called survival time which defines the length of time before the predicted solution diverges past a specified error threshold. Such metrics are important from a user’s perspective when choosing between architectures, but there has yet to be a unified set of metrics in literature and so we lack convenient benchmarking.</li> </ul> </li> <li>Understanding choices in network architecture <ul> <li>Given an end goal of using neural PDE solvers in practical settings, a major barrier for not only MP-PDE but for GRAND and PDE-GCN as well are the difficulties in choosing network parameters. While the proposed MP-PDE sheds light on certain choices like the message passing function and encoder-processor-decoder structure, it does not address some pragmatic decisions. For example, the 6 message passing layers in the MP-PDE appears relatively arbitrary which is a complaint shared in many machine learning methods. Because of the resulting upfront work in optimising the chosen model to determine what works for a new problem setting, the time cost of implementing it can be prohibitively high in comparison to the relative convenience of the many numerical solvers. One avenue of research to address this concern is neural architecture searching, where the design of neural architectures is discovered rather than manually specified. However, there is still a long way to go as many automated searches require significant compute to test the parameter space adequately.</li> </ul> </li> <li>The chicken and the egg <ul> <li>As impressive as many novel neural methods may be, it remains that training data comes from classical methods. One of the largest open questions (which also drives the need for generalisation) is how we can design neural solvers which require as little data as possible. Transfer learning, curriculum learning, and techniques to encourage generalisation (as seen with the MP-PDE) are all steps toward addressing this problem, but no significant success has been seen from any one in particular.</li> </ul> </li> </ul> <h4 id="remarks">Remarks</h4> <p>In their paper “Message Passing Neural PDE Solver”, Brandstetter at al. present a well-motivated neural solver based on the principle of message passing. The key contributions are the end-to-end network capable of one-shot generalization, and the mitigation of error accumulation in autoregressive models via temporal bundling and the pushforward trick. Note that the latter are self-contained can be applied to other architectures (as in the FNO-PF), providing a valuable tool to improve autoregressive models.</p>]]></content><author><name>Yolanne Lee</name></author><summary type="html"><![CDATA[Recent developments in the field of neural partial differential equation (PDE) solvers have placed a strong emphasis on neural operators. However, the paper Message Passing Neural PDE Solver by Brandstetter et al. published in ICLR 2022 revisits autoregressive models and designs a message passing graph neural network that is comparable with or outperforms both the state-of-the-art Fourier Neural Operator and traditional classical PDE solvers in its generalization capabilities and performance. This blog post delves into the key contributions of this work, exploring the strategies used to address the common problem of instability in autoregressive models and the design choices of the message passing graph neural network architecture.]]></summary></entry><entry><title type="html">Practical Applications of Bsuite For Reinforcement Learning</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/bsuite-applications/" rel="alternate" type="text/html" title="Practical Applications of Bsuite For Reinforcement Learning"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/bsuite-applications</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/bsuite-applications/"><![CDATA[<h2 id="0-introduction">0. Introduction</h2> <p>For the past few decades, the field of AI has appeared similar to the Wild West. There have been rapid achievements <d-cite key="krizhevsky_imagenet_2012"></d-cite><d-cite key="hessel_rainbow_2018"></d-cite> and epic showdowns <d-cite key="brown_superhuman_2019"></d-cite><d-cite key="silver_mastering_2016"></d-cite><d-cite key="vinyals_sc2_2019"></d-cite> happening in the frontier of AI research. The subfield of reinforcement learning has been no exception, where progress in the frontier has generated sensational applied feats while leaving theoretical understanding in the dust <d-cite key="osband_behaviour_2020"></d-cite>. As in many other AI subfields, there remain prevailing questions such as, <em>“Which model should I initially select for the given task?”</em>, <em>“How can I tune hyperparameters to increase performance?”</em>, and <em>“What is the best way to improve my already working model?”</em>. In this blog post, we help tame the frontier of reinforcement learning research by providing insights and quantitative answers to such questions through diagnostic, methodical, and reproducible reinforcement learning techniques. In particular, we focus on DeepMind’s <em>Behaviour Suite for Reinforcement Learning</em> (bsuite) codebase and showcase explicit examples of how it can aid reinforcement learning researchers in the development process and help provide a bridge between theoretical and applied reinforcement learning understanding.</p> <p>This introduction section provides the necessary background and motivation to understand the importance of our contribution. The background section describes how deep learning provides a blueprint for bridging theory to practice, and then discusses traditional reinforcement learning benchmarks. The bsuite summary section provides a high-level overview of the core capabilities tested by bsuite, its motivation, an example environment, and a comparison against traditional benchmark environments. In the motivation section, we present arguments for increasing the wealth and diversity of documented bsuite examples, with references to the paper and reviewer comments. The contribution statement presents the four distinct contributions of our work that help extend the bsuite publication. Finally, the experiment summary section describes our setup and rationale for the experimental illustrations in sections 1-5. The information in this introduction section is primarily distilled from the original bsuite publication <d-cite key="osband_behaviour_2020"></d-cite>.</p> <h3 id="background">Background</h3> <p>The current state of reinforcement learning (RL) theory notably lags progress in practice, especially in challenging problems. There are examples of deep reinforcement learning (DRL) agents learning to play Go from scratch at the professional level <d-cite key="silver_mastering_2016"></d-cite>, learning to navigate diverse video games from raw pixels <d-cite key="mnih_human-level_2015"></d-cite>, and learning to manipulate objects with robotic hands <d-cite key="andrychowicz_learning_2020"></d-cite>. While these algorithms have some foundational roots in theory, including gradient descent <d-cite key="bottou_large-scale_2010"></d-cite>, TD learning <d-cite key="sutton_learning_1988"></d-cite>, and Q-learning <d-cite key="watkins_q-learning_1992"></d-cite>, the authors of bsuite acknowledge that, “The current theory of deep reinforcement learning is still in its infancy” <d-cite key="osband_behaviour_2020"></d-cite>. A strong theory is prized since it can help provide insight and direction for improving known algorithms, while hinting at future research directions.</p> <p>Fortunately, deep learning (DL) provides a blueprint of the interaction between theoretical and practical improvements. During the ‘neural network winter’, DL techniques were disregarded in favor of more theoretically sound convex loss methods <d-cite key="cortes_support-vector_1995"></d-cite>, even though the main ideas and successful demonstrations existed many years previously <d-cite key="rosenblatt_perceptron_1958"></d-cite>. It was only until DL techniques achieved superior scores on benchmark problems, mainly for image recognition <d-cite key="krizhevsky_imagenet_2012"></d-cite>, that DL earned the research spotlight. Consequently, a renewed interest in DL theory followed shortly after <d-cite key="kawaguchi_deep_2016"></d-cite><d-cite key="bartlett_spectrally-normalized_2017"></d-cite><d-cite key="belkin_reconciling_2019"></d-cite>, bolstered by the considerable wealth of applied research. Due to the lack of theory in DRL and the proximity of the DL and DRL research fields, <span class="emph">one enticing avenue to accelerate progress in reinforcement learning research is to follow the blueprint laid out by deep learning research and create well-defined and vetted benchmarks for the understanding of reinforcement learning algorithms</span>.</p> <p>To this end, the trend of RL benchmarks has seen an increase in overall complexity. The earliest such benchmarks were simple MDPs that served as basic testbeds with fairly obvious solutions, such as <em>Cartpole</em> <d-cite key="barto_neuronlike_1983"></d-cite> and <em>MountainCar</em> <d-cite key="moore_efficient_1990"></d-cite>. Other benchmarks proved to be more diagnostic by targeting certain capabilities such as <em>RiverSwim</em> <d-cite key="strehl_analysis_2008"></d-cite> for exploration and <em>Taxi</em> <d-cite key="dietterich_hierarchical_2000"></d-cite> for temporal abstraction. Modern benchmarks such as the <em>ATARI Learning Environment</em> <d-cite key="bellemare_arcade_2013"></d-cite> and board games such as <em>Chess</em>, <em>Go</em>, and <em>Shogi</em> are more complex and prove difficult for humans, with even the best humans unable to achieve perfect play. The corresponding achievements were highly publicized <d-cite key="silver_mastering_2016"></d-cite><d-cite key="mnih_human-level_2015"></d-cite> due to the superhuman performance of the agents, with the agents taking actions that were sometimes not even considered by their human counterparts. Consequently, the pursuit of superhuman performance on complex benchmarks has recently been a strong driver of progress in the field <d-cite key="vinyals_sc2_2019"></d-cite><d-cite key="silver_general_2018"></d-cite><d-cite key="perolat_mastering_2022"></d-cite><d-cite key="ecoffet_first_2021"></d-cite><d-cite key="bakhtin_diplomacy_2022"></d-cite>.</p> <h3 id="summary-of-bsuite">Summary of bsuite</h3> <p>The open-source <em>Behaviour Suite for Reinforcement Learning</em> (bsuite) benchmark <d-cite key="osband_behaviour_2020"></d-cite> goes against the grain of the current benchmark trend of increasing complexity. It acts as a complement to existing benchmarks by creating 23 environments with minimal confounding factors to test 7 behavioral core capabilities of RL agents, as follows: <strong>basic</strong>, <strong>exploration</strong>, <strong>memory</strong>, <strong>generalization</strong>, <strong>noise</strong>, <strong>scale</strong>, and <strong>credit assignment</strong>. Current benchmarks often contain most of these capabilities within a single environment, whereas bsuite tailors its environments to target one or a few of these capabilities. Each bsuite environment is scalable and has 16 to 22 levels of difficulty, providing a more precise analysis of the corresponding capabilities than a simple, and possibly misleading <d-cite key="agarwal_deep_2021"></d-cite>, ranking of algorithm performance. Furthermore, algorithms have fixed evaluation regimes based on the number of seeds and episodes allowed during training, which rewards algorithms that exhibit the capabilities rather than those that focus on sheer compute power. The targeted and scalable nature of bsuite can provide insights such as eliciting bottlenecks and revealing scaling properties that are opaque in traditional benchmarks. With respect to the benchmarks described in the preceding paragraph, bsuite is most similar to the diagnostic benchmarks of <em>RiverSwim</em> <d-cite key="strehl_analysis_2008"></d-cite> for and <em>Taxi</em> <d-cite key="dietterich_hierarchical_2000"></d-cite> due to its purpose as a stepping stone for tackling more challenging benchmarks.</p> <p>The bsuite evaluation of an agent yields a radar chart (Fig. 1) that displays the agent’s score from 0 to 1 on all seven capabilities, usually based on regret, that yields a quick quantitative comparison between agents. Scores near 0 indicate poor performance, often akin to an agent acting randomly, while scores near 1 indicate mastery of all environment difficulties. A central premise of bsuite is that <span class="emph">if an agent achieves high scores on certain environments, then it is much more likely to exhibit the associated core capabilities due to the targeted nature of the environments. Therefore, the agent will more likely perform better on a challenging environment that contains many of the capabilities than one with lower scores on bsuite</span>. This premise is corroborated by recent research that shows how insights on simple environments can still hold true on more complex environments <d-cite key="ceron_revisiting_2021"></d-cite>. However, we urge practitioners to exercise caution when adopting bsuite into the development process, as the insights on more simple bsuite environments are not guaranteed to extend to more complex environments in a straightforward manner.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar01-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar01.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 1. Example radar chart of DQN on all 7 bsuite core capabilities. </div> <p>An example environment is <em>deep sea</em> that targets exploration power. As shown in Figure 2, <em>deep sea</em> is an $N \times N$ grid with starting state at cell $(1, 1)$ and treasure at $(N, N)$, with $N$ ranging from 10 to 100. The agent has two actions, move downward left and downward right; the goal is to reach the treasure and receive a reward of $1$ by always moving downward right. A reward of $0$ is given to the agent for moving downward left at a timestep, while a penalizing reward of $-0.01/N$ is given for moving downward right. The evaluation protocol of <em>deep sea</em> only allows for $10K$ episodes of $N-1$ time steps each, which prevents an algorithm with unlimited time from casually exploring the entire state space and stumbling upon the treasure. Note that superhuman performance is nonexistent in <em>deep sea</em> (and more precisely in the entire bsuite gamut) since a human can spot the optimal policy nearly instantaneously. Surprisingly, we will show later that baseline DRL agents fail miserably at this task.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/diagram02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/diagram02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/diagram02-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/diagram02.png" class="img-fluid asdf" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2. Illustration of deep sea environment taken from <d-cite key="osband_behaviour_2020"></d-cite>. </div> </div> <p>The <strong>challenge</strong> of <em>deep sea</em> is the necessity of exploration in an environment that presents an irreversible, suboptimal greedy action (moving downward left) at every time step. This environment <strong>targets</strong> exploration power by ensuring that a successful agent must deliberately choose to explore the state space by neglecting the greedy action. The <strong>simplistic</strong> implementation removes confounding goals, such as learning to see from pixels while completing other tasks <d-cite key="mnih_human-level_2015"></d-cite>. Furthermore, this environment provides a granular exploration score through <strong>scaling</strong> the environment size by $N$ and determining when an agent starts to fail. Finally, the implementation of the environment yields <strong>fast</strong> computation, allowing multiple, quick runs with minimal overhead and compute cost. These 5 aforementioned key qualities are encompassed by all bsuite environments, and we contrast such environments against traditional benchmark environments in the below table.</p> <table> <thead> <tr> <th>Key Quality</th> <th>Traditional Benchmark Environment</th> <th>bsuite Environment</th> </tr> </thead> <tbody> <tr> <td><strong>Targeted</strong></td> <td>Performance on environment subtly related to many or all core capabilities.</td> <td>Performance on environment directly related with one or few core capabilities.</td> </tr> <tr> <td><strong>Simple</strong></td> <td>Exhibits many confounding factors related to performance.</td> <td>Removes confounding factors related to performance.</td> </tr> <tr> <td><strong>Challenging</strong></td> <td>Requires competency in many core capabilities but not necessarily past normal range in any capability.</td> <td>Pushes agents beyond normal range in one or few core capabilities.</td> </tr> <tr> <td><strong>Scalable</strong></td> <td>Discerns agent’s power through comparing against other agents and human performance.</td> <td>Discerns agent’s competency of core capabilities through increasingly more difficult environments.</td> </tr> <tr> <td><strong>Fast</strong></td> <td>Long episodes with computationally-intensive observations.</td> <td>Relatively small episode and experiment lengths with low observation complexity.</td> </tr> </tbody> </table> <h3 id="motivation">Motivation</h3> <p>The authors of bsuite stated, “Our aim is that these experiments can help provide a bridge between theory and practice, with benefits to both sides” <d-cite key="osband_behaviour_2020"></d-cite>. As discussed in the background section, establishing clear benchmarks can yield applied progress, which in turn can accelerate theoretical progress. The use of bsuite in this manner seems highly fruitful since its environments are targeted, which allows for hypothesis testing and eventual formalization into provable guarantees. As such, <span class="emph">it is instrumental that the applied aspect of bsuite is emphasized through the adoption and diverse application of reinforcement learning practitioners</span>.</p> <p>The applied examples in the published paper are rather meagre: there are two examples of algorithm comparison on two specific environments and three example comparisons of algorithms, optimizers, and ensemble sizes across the entire bsuite gamut in the appendix. The two examples on the specific environments showcase how bsuite can be used for directed algorithm improvement, but the experiments in the appendices only discuss the general notion of algorithm comparison using bsuite scores. In addition to the examples, the authors supply some comments throughout the paper that provide hints regarding the applied usage of bsuite. Looking at the <a href="https://openreview.net/forum?id=rygf-kSYwH">paper reviews</a>, <a href="https://openreview.net/forum?id=rygf-kSYwH&amp;noteId=rkxk2BR3YH">reviewer #1</a> mentioned how there was no explicit conclusion from the evaluation, and <a href="https://openreview.net/forum?id=rygf-kSYwH&amp;noteId=rJxjmH6otS">reviewer #3</a> mentioned that examples of diagnostic use and concrete examples would help support the paper. Furthermore, <a href="https://openreview.net/forum?id=rygf-kSYwH&amp;noteId=SJgEVpbAFr">reviewer #2</a> encouraged publication of bsuite at a top venue to see traction within with the RL research community, and the <a href="https://openreview.net/forum?id=rygf-kSYwH&amp;noteId=7x_6G9OVWG">program chairs</a> mentioned how success or failure can rely on community acceptance. Considering that bsuite received a spotlight presentation at ICLR 2020 and has amassed over 100 citations in the relatively small field of RL reproducibility during the past few years, bsuite has all intellectual merit and some community momentum to reach the level of a timeless benchmark in RL research. <span class="emph">To elevate bsuite to the status of a timeless reinforcement learning benchmark and to help bridge the theoretical and applied sides of reinforcement learning, we believe that it is necessary to develop and document concrete bsuite examples that help answer difficult and prevailing questions throughout the reinforcement learning development process</span>.</p> <h3 id="contribution-statement">Contribution Statement</h3> <p>This blog post extends the work of bsuite by showcasing 12 example use cases with experimental illustration that directly address specific questions in the reinforcement learning development process to (i) help bridge the gap between theory and practice, (ii) promote community acceptance, (iii) aid applied practitioners, and (iv) highlight potential research directions in reproducible reinforcement learning.</p> <h3 id="experiment-summary">Experiment Summary</h3> <p>We separate our examples into 5 categories of <strong>initial model selection</strong>, <strong>preprocessing choice</strong>, <strong>hyperparameter tuning</strong>, <strong>testing and debugging</strong>, and <strong>model improvement</strong>. This blog post follows a similar structure to the paper <em>Deep Reinforcement Learning that Matters</em> <d-cite key="henderson_deep_2018"></d-cite> by posing and answering a question in each category, and then providing a few illustrative examples with conclusions. Most examples use Stable-Baselines3 (SB3) <d-cite key="raffin_stable-baselines3_2022"></d-cite> for training DRL agents due to its clarity and simplicity, and the examples focus on DRL due to its pervasiveness in the applied RL community. We provide code and instructions for each experiment in our <a href="https://github.com/LorenJAnderson/bsuite-applications.git">GitHub codebase</a>, along with hyperparameters and implementation details. Since the focus of this blog post is the discussion of diverse example use cases, not architectural considerations or implementation details, we refer the reader to the <a href="https://openreview.net/pdf?id=rygf-kSYwH#page=13">paper appendix</a> and the <a href="https://colab.research.google.com/github/deepmind/bsuite/blob/master/bsuite/analysis/results.ipynb">colab analysis tutorial</a> for more information about the environments and to the <a href="https://colab.research.google.com/drive/1rU20zJ281sZuMD1DHbsODFr1DbASL0RH">colab intro tutorial</a> and our own codebase for instructions and examples regarding the implementation of bsuite.</p> <p>Although running a bsuite environment is orders of magnitude faster than most benchmark environments, the number of individual bsuite environments and the number of our examples required us to create a subset of bsuite, which we will refer to as <em>mini-bsuite</em> or <em>msuite</em> in this work. We designed msuite to mirror the general scaling pattern of each bsuite environment and the diversity of core capabilities in bsuite; a complete description of msuite can be found in our GitHub codebase. Running experiments on a subset of bsuite highlights its flexibility, and we will show, still elicits quality insights. Since we use a subset of bsuite for our experiments, our radar charts will look different from those in the original bsuite paper. We generally keep the more challenging environments and consequently produce lower scores, especially in the generalization category.</p> <p>We stress that the below examples are not meant to amaze the reader or exhibit state-of-the-art research. <span class="epmh">The main products of this work are the practicality and diversity of ideas in the examples</span>, while the experiments are primarily for basic validation and illustrative purposes. Moreover, these experiments use modest compute power and showcase the effectiveness of bsuite in the low-compute regime. Each example has tangible benefits such as saving development time, shortening compute time, increasing performance, and lessening frustration of the practitioner, among others. To maintain any sense of brevity in this post, we now begin discussion of the examples.</p> <h2 id="1-initial-model-selection">1. Initial Model Selection</h2> <p>The reinforcement learning development cycle typically begins with an environment to solve. A natural question usually follows: “<em>Which underlying RL model should I choose to best tackle this environment, given my resources</em>?”. Resources can range from the hardware (e.g. model size on the GPU), to temporal constraints, to availability of off-the-shelf algorithms <d-cite key="liang_rllib_2018"></d-cite><d-cite key="raffin_stable-baselines3_2022"></d-cite>, to programming efficiency of the practitioner. Initially selecting an effective model can save a great amount of development time due to the potentially greater performance baseline of the agent. In this section, we illustrate how bsuite can be used to effectively answer the question of initial model selection.</p> <h3 id="comparing-baseline-algorithms">Comparing Baseline Algorithms</h3> <p>Perhaps the first choice in the RL development cycle is choosing the algorithm. A considerable amount of RL research is focused on the corresponding algorithms, which presents many possibilities for the researcher. The No Free Lunch Theorem <d-cite key="wolpert_no_1997"></d-cite> tailored to reinforcement learning would state that no algorithm will prove better than any other unless the characteristics of the underlying environment are known. Using bsuite provides a quantitative assessment of algorithm performance on capabilities that are prevalent in many or even most reinforcement learning environments of interest.</p> <p>Example: Figure 3 shows the performance of the Stable-Baselines3 (SB3) implementations of DQN, A2C, and PPO on msuite with our default hyperparameters. Recent research <d-cite key="andrychowicz_what_2020"></d-cite> suggests that PPO is the most commonly used RL algorithm, and it was a successor to DQN and A2C. The results indeed show that PPO is superior on msuite in most categories, providing credibility for its use as the premiere baseline DRL algorithm.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar11-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar11.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3. Comparison of SB3 default DQN, A2C, and PPO baseline algorithms. </div> </div> <h3 id="comparing-off-the-shelf-implementations">Comparing Off-the-Shelf Implementations</h3> <p>Due to the vast number of reinforcement learning paradigms (e.g. model-based, hierarchical), there are many off-the-shelf (OTS) libraries that provide a select number of thoroughly tested reinforcement learning algorithms. Often, temporal resources or coding capabilities do not allow for practitioners to implement every algorithm by hand. Fortunately, running an algorithm on bsuite can provide a quick glance of an OTS algorithm’s abilities at low cost to the practitioner.</p> <p>Example: Figure 4 compares our default DQN implementation against the example DQN implementation in the bsuite codebase. There is a significant difference between the performance of each implementation on msuite, with the bsuite implementation displaying its superiority. Note that the hyperparameters of bsuite DQN were most likely chosen with the evaluation on bsuite in mind, which could explain its increased performance.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar12-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar12.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 4. Comparison of SB3 DQN and bsuite DQN. </div> </div> <h3 id="gauging-hardware-necessities">Gauging Hardware Necessities</h3> <p>Even after an initial algorithm is selected, hardware limitations such as network size and data storage can prevent the agent from being deployed. Using bsuite provides a low-cost comparison among possible hardware choices that can be used to argue for their necessity. This is especially important for small development teams since there can likely be a major disparity between their own hardware resources and those discussed in corresponding research publications.</p> <p>Example: Figure 5 compares the default DQN implementation when varying replay buffer sizes, from $1\mathrm{e}{2}$ to $1\mathrm{e}{5}$, with the default having size $1\mathrm{e}{4}$. The original DQN implementation used a replay buffer of size $1\mathrm{e}{6}$, which is too large for the RAM constraints of many personal computers. The results show that increasing the buffer size to at least $1\mathrm{e}{4}$ yields significant returns on msuite. Note that since the experiment lengths (total time steps for all episodes) of msuite were sometimes less than $1\mathrm{e}{5}$, the largest buffer size of $1\mathrm{e}{5}$ did not always discard experiences from very old episodes, which most likely decreased its performance in comparison to a buffer size of $1\mathrm{e}{4}$.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar13-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar13-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar13-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar13.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 5. Comparison of DQN with varying buffer sizes. </div> </div> <h3 id="future-work">Future Work</h3> <p>Due to the diversity of OTS libraries, one possible research direction in reproducible RL is to test algorithms from different OTS libraries using the same hyperparameters on bsuite and create a directory of bsuite radar charts. This provides practitioners a comparison with their own implementation or a starting point when selecting an OTS library and algorithm. Another direction is to test various aspects related to hardware constraints and attempt to show the tradeoff between constraints and performance on bsuite and other benchmarks. This would especially help practitioners with low compute resources to budget resource use on multiple projects.</p> <h2 id="2-preprocessing-choice">2. Preprocessing Choice</h2> <p>Most benchmark environments present complexities such as high-dimensional observations, unscaled rewards, unnecessary actions, and partially-observable Markov Decision Process (POMDP) dynamics. Some of these difficulties can be curbed using environment preprocessing techniques. While certain environments such as <em>ATARI</em> have formalized standards for preprocessing, there are some aspects such as frame skipping that are considered part of the underlying algorithm, and therefore, a choice of the practitioner <d-cite key="machado_revisiting_2018"></d-cite>. A natural question to ask is, “<em>What environment preprocessing techniques will best help my agent attain its goal in this environment</em>?”. In this section, we show how bsuite can provide insight to the choice of preprocessing, with benefits of increased performance and shortened training time.</p> <h3 id="verification-of-preprocessing">Verification of Preprocessing</h3> <p>Preprocessing techniques usually targeted to ease some aspect of the agent’s training. For example, removing unnecessary actions (e.g. in a joystick action space) prevents the agent from having to learn which actions are useless. While a new preprocessing technique can provide improvements, there is always the chance that it fails to make a substantial improvement, or worse yet, generally decreases performance. Invoking bsuite can help provide verification that the preprocessing provided the planned improvement.</p> <p>Example: Figure 6 shows the performance of the default DQN agent versus an agent that received normalized rewards from the environment. Normalizing the rewards increases the speed of training a neural network, since the parameters are usually initialized to expect target values in a range from $-1$ to $1$. Our results show that the normalization preprocessing indeed increases the capability of navigating varying reward scales while not suffering drastically in any other capability.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar21-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar21-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar21-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar21.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 6. Comparison of DQN with and without reward normalization. </div> </div> <h3 id="better-model-versus-preprocessing">Better Model versus Preprocessing</h3> <p>Instead of choosing to preprocess the environment, a more sophisticated algorithm may better achieve the preprocessing goals. For example, many improvements on the original DQN algorithm have been directed towards accomplishing goals such as improving stability, reducing overestimation, and bolstering exploration. Comparing preprocessing against an algorithmic improvement provides a quantitative reason for deciding between the two options, especially since development time of many common preprocessing wrappers is quite short.</p> <p>Example: Figure 7 shows the results of PPO with a recurrent network versus PPO having its observation as the last 4 stacked frames from the environment. Frame stacking is common on some <em>ATARI</em> environments by converting the POMDP dynamics to an MDP, which is necessary to determine velocity of any element on the screen. An improvement to DQN, Deep Recurrent Q-networks <d-cite key="hausknecht_deep_2017"></d-cite> uses a recurrent LSTM to aid in memory and achieve the same effects of frame stacking. The msuite results show that memory is considerably improved with PPO RNN and therefore may be worth the extra development time.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar22-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar22-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar22-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar22.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 7. Comparison of PPO with frame stacking and PPO with RNN. </div> </div> <h3 id="future-work-1">Future Work</h3> <p>One research direction is to document common preprocessing techniques and determine their scores on bsuite. This would provide practitioners a summary of directed strengths for each preprocessing technique while possibly uncovering unexpected behavior. Another direction is to determine the extent to which preprocessing techniques aided previous results in the literature, which could illuminate strengths or weaknesses in the corresponding algorithms.</p> <h2 id="3-hyperparameter-tuning">3. Hyperparameter Tuning</h2> <p>After selecting a model and determining any preprocessing of the environment, an agent must eventually be trained on the environment to gauge its performance. During the training process, initial choices of hyperparameters can heavily influence the agent’s performance <d-cite key="andrychowicz_what_2020"></d-cite>, including how to explore and how quickly the model should learn from past experience. The corresponding question to ask is, “<em>How can I choose hyperparameters to yield the best performance, given a model?</em>” In this section, we show how bsuite can be used to tune hyperparameters, thereby increasing performance and shortening compute time.</p> <h3 id="unintuitive-hyperparameters">Unintuitive Hyperparameters</h3> <p>Some hyperparameters such as exploration percentage and batch size are more concrete, while others such as discounting factor and learning rate are a little less intuitive. Determining a starting value of an unintuitive hyperparameter can be challenging and require a few trials before honing in on a successful value. Instead of having to run experiments on a costly environment, using bsuite can provide a thoughtful initial guess of the value with minimal compute.</p> <p>Example: Figure 8 shows the results of running PPO with various entropy bonus coefficients across msuite (default is $0.01$). The entropy bonus affects the action distribution of the agent, and the value of $1\mathrm{e}{-2}$ presented in the original paper <d-cite key="schulman_proximal_2017"></d-cite> is fairly unintuitive. The results show that the value of $1\mathrm{e}{-2}$ is indeed superior on msuite by a small margin. Since SB3 has the entropy bonus initialized to 0, this example also shows how hyperparameter tuning with msuite can improve performance even on OTS implementations.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar31-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar31-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar31-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar31.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 8. Comparison of default PPO with varying entropy bonuses. </div> </div> <h3 id="promising-ranges-of-hyperparameters">Promising Ranges of Hyperparameters</h3> <p>Instead of determining a single value of a hyperparameter, gauging an acceptable range may be required. Since hyperparameters can have confounding effects, knowing approximate soft boundaries of hyperparameters at which agents start to fail basic tasks can provide useful information during a more general hyperparameter tuning process. For example, smaller learning rates generally take longer for algorithm convergence, and a practitioner may want to know a promising range of learning rates if the computing budget is flexible. The scaling nature of bsuite presents knowledge of the extent to which different hyperparameter choices affect performance, greatly aiding in ascertaining a promising hyperparameter range.</p> <p>Example: Figure 9 shows the results of default DQN with varying learning rates on msuite (default $7\mathrm{e}{-4}$). The results suggest that learning rates above $1\mathrm{e}{-2}$ start to yield diminishing returns. Since some experiment lengths in msuite only run for $10K$ episodes, the lowest learning rate of $1\mathrm{e}{-6}$ may never converge in time even with high-quality training data, necessitating a modification to msuite to learn a lower bound.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar32-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar32-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar32-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar32.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 9. Comparison of default DQN with varying learning rates. </div> </div> <h3 id="pace-of-annealing-hyperparameters">Pace of Annealing Hyperparameters</h3> <p>While some hyperparameters stay fixed, others must change throughout the course of training. Typically, these include hyperparameters that control the exploration vs. exploitation dilemma, such as entropy bonus and epsilon-greedy exploration. These hyperparameters are often dependent on the entire experiment; for example, SB3 anneals epsilon-greedy exploration for a fixed fraction of the experiment. Therefore, entire experiments, some consisting of millions of episodes, need to be run to determine successful values of these hyperparameters. Using bsuite can provide a quick confirmation that the annealing of these parameters happens at an acceptable rate.</p> <p>Example: Figure 10 shows the performance of DQN with various epsilon-greedy exploration annealing lengths, based on a fixed fraction of the entire experiment (default $0.1$). The annealing fraction of $0.1$ performs best on msuite, which is the same choice of parameter in the original DQN paper. Furthermore, performance decreases with greater annealing lengths. Since bsuite environments are generally scored with regret, we acknowledge that the longer annealing lengths may have better relative performance if bsuite were scored with a training versus testing split.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar33-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar33-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar33-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar33.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 10. Comparison of default DQN with varying epsilon annealing lengths. </div> </div> <h3 id="future-work-2">Future Work</h3> <p>The three experiments above can be extended by documenting the effect of varying hyperparameters on performance, especially in OTS implementations. This would help practitioners understand the effects of certain hyperparameters on the bsuite core capabilities, allowing for a better initial hyperparameter choice when certain capabilities are necessary for the environment at hand. Another research direction is to determine if integrating a fast hyperparameter tuner on general environments such as bsuite into a hyperparameter tuner for single, complex environments would increase the speed of tuning on the fixed environment. Since the bsuite core capabilities are necessary in many complex environments, initially determining competency on bsuite would act as a first pass of the tuning algorithm.</p> <h2 id="4-testing-and-debugging">4. Testing and Debugging</h2> <p>Known to every RL practitioner, testing and debugging during the development cycle is nearly unavoidable. It is common to encounter silent bugs in RL code, where the program runs but the agent fails to learn because of an implementation error. Examples include incorrect preprocessing, incorrect hyperparameters, or missing algorithm additions. Quick unit tests can be invaluable for the RL practitioner, as shown in successor work to bsuite <d-cite key="rajan_mdp_2021"></d-cite>. A corresponding question to ask during the testing and debugging phase is, “<em>What tests can I perform to verify that my agent is running as intended?</em>” In this section, we show how bsuite can be used as a sanity check for the implementation, saving compute time and lessening the frustration of the practitioner. In an effort to refrain from contrived examples, the two examples below highlight real-life scenarios where using bsuite could have saved the authors of this blog post hours of frustration in their own work.</p> <h3 id="incorrect-hyperparameter">Incorrect Hyperparameter</h3> <p>As discussed in the previous section, hyperparameters are of major importance to the performance of a RL algorithm. A missing or incorrect hyperparameter will not necessarily prevent a program from running, but most such bugs will severely degrade performance. Using bsuite can quickly expose poor performance of an algorithm at a low cost to the practitioner.</p> <p>Example: Figure 11 shows the default PPO implementation against a PPO implementation with an erroneous learning rate of $1\mathrm{e}{3}$. Many hyperparameters such as total training steps and maximum buffer size are usually coded using scientific notation since they are so large; consequently, it is easy to forget the ‘minus sign’ when coding the learning rate and instead code the learning rate as $1\mathrm{e}{3}$. The results on msuite show that performance has degraded severely from an OTS implementation, and more investigation into the code is required. One of the authors of this blog post would have saved roughly a day of training a PPO agent in their own work had they realized this exact mistake.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar41-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar41-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar41-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar41.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 11. Comparison of default PPO with miscoded PPO. </div> </div> <h3 id="off-the-shelf-algorithm-testing">Off-the-Shelf Algorithm Testing</h3> <p>While the previous example used an OTS algorithm for comparison to illuminate silent bugs, it may be the case that the OTS algorithm itself could have a silent bug. Whether due to an incorrect library being used or a misunderstanding of the OTS algorithm, any silent bug in an OTS algorithm can be difficult to detect due to the codebase being written by another practitioner. Again, bsuite can be used to diagnose poor performance and elucidate a coding problem.</p> <p>Example: Figure 12 shows the results of the SB3 DQN with our default experimental hyperparameters and with the default SB3 hyperparameters on msuite. A core difference between the hyperparameters is the burn rate: the default SB3 hyperparameters perform $10K$ steps before learning takes place (e.g. backprop), while our default experimental hyperparameters start the learning after $1K$ steps. Since many of the easier msuite environments only last $10K$ time steps, failure to learn anything during that time severely degrades performance, as shown. Noticing the default value of this hyperparameter in SB3 would have saved the authors roughly 10 hours of training time.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar42-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar42-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar42-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar42.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 12. Comparison of DQN with small and large burn-in. </div> </div> <h3 id="future-work-3">Future Work</h3> <p>The training time for a complete run of bsuite can take an hour for even the most basic algorithms. Considering that a few of the easiest bsuite environments could have shown poor performance in the above examples within mere minutes, one research avenue is to create a fast debugging system for reinforcement learning algorithms. In the spirit of bsuite, it should implement targeted experiments to provide actionable solutions for eliminating silent bugs. Such work would primarily act as a public good, but it could also help bridge the gap between RL theory and practice if it embodies the targeted nature of bsuite.</p> <h2 id="5-model-improvement">5. Model Improvement</h2> <p>A natural milestone in the RL development cycle is getting an algorithm running bug-free with notable signs of learning. A common follow-up question to ask is, “<em>How can I improve my model to yield better performance?</em>”. The practitioner may consider choosing an entirely new model and repeating some of the above steps; a more enticing option is usually to improve the existing model by reusing its core structure and only making minor additions or modifications, an approach taken in the development of the baseline RAINBOW DQN algorithm <d-cite key="hessel_rainbow_2018"></d-cite>. In this section, we discuss how bsuite can be used to provide targeted improvements of existing models and increase performance while mitigating compute time.</p> <h3 id="increasing-network-complexity">Increasing Network Complexity</h3> <p>In DRL, the neural network usually encodes the policy, and its architecture directly affects the agent’s learning capacity. The more complicated CNN architecture was a driver for the first superhuman performance of a DRL algorithm on the <em>ATARI</em> suite due to its ability to distill image data into higher-level features. Using bsuite can provide a quick verification if an architectural improvement produces its intended effect.</p> <p>Example: Figure 13 shows the results of PPO against PPO with a recurrent neural network. As mentioned in a previous example, RNNs aid memory and were originally incorporated into DRL as a way to deal with POMDP dynamics. The results on msuite display the substantial increase in memory capability while sacrificing on credit assignment. This example highlights how bsuite can provide warnings of possible unexpected decreases in certain capabilities, which must be monitored closely by the practitioner.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar51-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar51-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar51-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar51.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 13. Comparison of default PPO with PPO RNN. </div> </div> <h3 id="off-the-shelf-improvements">Off-the-Shelf Improvements</h3> <p>While previous examples discussed comparison, verification, and debugging OTS implementations, many OTS libraries provide support for well-known algorithm improvements. For example, some DQN implementations have boolean values to signify the use of noisy networks, double Q-learning, and more. Using bsuite provides the necessary targeted analysis to help determine if certain improvements are fruitful for the environment at hand.</p> <p>Example: Figure 14 shows the results of our default DQN compared against the SB3 QRDQN algorithm with default hyperparameters and the SBE QRDQN algorithm with hyperparameters matching our default DQN implementation. The QRDQN algorithm is an improvement over DQN that aims to capture the distribution over returns instead of a point estimate of the expected return. This implementation is more complex but allows for a precise estimate that aids in stability. The results show that this improvement was rather negligible on msuite, and unless credit assignment is the major concern in the environment at hand, a different improvement may prove more useful.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar52-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar52-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-bsuite-applications/radar52-1400.webp"/> <img src="/2023/assets/img/2023-05-01-bsuite-applications/radar52.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 14. Comparison of DQN with QRDQN variants. </div> </div> <h3 id="future-work-4">Future Work</h3> <p>Since bsuite provides quantitative results, one avenue of research is to create a recommender system that uses information from previous bsuite analyses to recommend improvements in DRL algorithms. The practitioner would need to provide as input the most important capabilities that an environment is believed to exhibit, and bsuite would tailor recommendations towards those capabilities. Such a recommender system could save compute time, increase performance, and ultimately expose the practitioner to new and exciting algorithmic possibilities.</p> <h2 id="6-conclusion">6. Conclusion</h2> <p>Traditional RL benchmarks contain many confounding variables, which makes analysis of agent performance rather opaque. In contrast, bsuite provides targeted environments that help gauge agent prowess in one or few core capabilities. The goal of bsuite is to help bridge the gap between practical theory and practical algorithms, yet there currently is no database or list of example use cases for the practitioner. Our work extends bsuite by providing concrete examples of its use, with a few examples in each of five categories. We supply at least one possible avenue of related future work or research in reproducible RL for each category. In its current state, bsuite is poised to be a standard RL benchmark for years to come due to its acceptance in a top-tier venue, well-structured codebase, multiple tutorials, and over 100 citations in the past few years in a relatively small field. We aim to help propel bsuite, and more generally methodical and reproducible RL research, into the mainstream through our explicit use cases and examples. With a diverse set of examples to choose from, we intend for applied RL practitioners to understand more use cases of bsuite, apply and document the use of bsuite in their experiments, and ultimately help bridge the gap between practical theory and practical algorithms.</p> <h3 id="green-computing-statement">Green Computing Statement</h3> <p>The use of bsuite can provide directed improvements in algorithms, from high-level model selection and improvement to lower-level debugging, testing, and hyperparameter tuning. Due to the current climate crisis, we feel that thoroughly-tested and accessible ideas that can reduce computational cost should be promoted to a wide audience of researchers.</p> <h3 id="inclusive-computing-statement">Inclusive Computing Statement</h3> <p>Many of the ideas in bsuite and this blog post are most helpful in regimes with low compute resources because of the targeted nature of these works. Due to the increasing gap between compute power of various research teams, we feel that thoroughly-tested and accessible ideas that can benefit teams with meagre compute power should be promoted to a wide audience of researchers.</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>We thank the reviewers for their helpful comments. We also thank the authors of bsuite for their outstanding work.</p>]]></content><author><name>Loren Anderson</name></author><summary type="html"><![CDATA[In 2019, researchers at DeepMind published a suite of reinforcement learning environments called Behavior Suite for Reinforcement Learning, or bsuite. Each environment is designed to directly test a core capability of a general reinforcement learning agent, such as its ability to generalize from past experience or handle delayed rewards. In this blog post, we extend their work by providing specific examples of how bsuite can address common challenges faced by reinforcement learning practitioners during the development process.]]></summary></entry><entry><title type="html">Strategies for Classification Layer Initialization in Model-Agnostic Meta-Learning</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/classification-layer-initialization-in-maml/" rel="alternate" type="text/html" title="Strategies for Classification Layer Initialization in Model-Agnostic Meta-Learning"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/classification-layer-initialization-in-maml</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/classification-layer-initialization-in-maml/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In a previous study, Raghu et al. [2020] <d-cite key="DBLP:conf/iclr/RaghuRBV20"></d-cite> found that in model-agnostic meta-learning (MAML) for few-shot classification, the majority of changes observed in the network during the inner loop fine-tuning process occurred in the linear classification head. It is commonly believed that during this phase, the linear head remaps encoded features to the classes of the new task. In traditional MAML, the weights of the final linear layer are meta-learned in the usual way. However, there are some issues with this approach:</p> <p>First, it is difficult to imagine that a single set of optimal classification head weights can be learned. This becomes apparent when considering class label permutations: two different tasks may have the same classes but in a different order. As a result, the weights that perform well for the first task will likely not be effective for the second task. This is reflected in the fact that MAML’s performance can vary by up to 15% depending on the class label ordering during testing <d-cite key="DBLP:conf/iclr/YeC22"></d-cite>.</p> <p>Second, more challenging datasets are being proposed as few-shot learning benchmarks, such as Meta-Dataset <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite>. These datasets have varying numbers of classes per task, making it impossible to learn a single set of weights for the classification layer.</p> <p>Therefore, it seems logical to consider how to initialize the final classification layer before fine-tuning on a new task. Random initialization may not be optimal, as it can introduce unnecessary noise <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>.</p> <p>This blog post will discuss different approaches to the last layer initialization that claim to outperform the original MAML method.</p> <h2 id="what-is-meta-learning">What is Meta-Learning?</h2> <p>Before diving into the topic, let’s look at the general idea of meta-learning. In supervised machine learning, tasks are learned using a large number of labeled examples. However, acquiring a sufficient amount of labeled data can be labor extensive. Also, this approach to machine learning evidently deviates from the human learning process; a child is certainly able to learn what a specific object is, using only a few examples, and not hundreds or thousands. This is where meta-learning comes in. Its goal can be described as acquiring the ability to learn new tasks from only a few examples <d-cite key="9428530"></d-cite>.</p> <p>There is not one fixed framework for meta-learning; however, a common approach is based on the principle that the conditions in which a model is trained and evaluated must match <d-cite key="vinyals2016matching"></d-cite>.<br/> Let’s look at this in more detail for the case of few-shot classification, which can be solved with meta-learning. Here, the meta-learning goal can be verbalized as “learning to learn new classes from few examples” <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite>. When evaluating a meta-learner, one needs a training set \(\mathcal{D^{tr}} = ((\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), (\mathbf{x}_3, y_3), ...)\), consisting of labeled examples for unseen classes. Those are used by the meta-learner to adapt to the novel task. How well the meta-learner performs can then be evaluated on labeled examples from the same classes: \(\mathcal{D^{test}} = ((\mathbf{x}_{1}^{\ast}, y_{1}^{\ast}), (\mathbf{x}_{2}^{\ast}, y_{2}^{\ast}), (\mathbf{x}_{3}^{\ast}, y_{3}^{\ast}), ...)\). The combination of such a training and test set is referred to as an episode or a task: $\mathcal{T} = (\mathcal{D^{tr}}, \mathcal{D^{test}})$.</p> <p>To match the conditions for training and evaluation, one would split all available classes with their examples into a dataset for meta-training \(\mathcal{C}_{train}\) and a dataset for meta-testing \(\mathcal{C}_{test}\). Tasks are then drawn from those datasets for either training or testing purposes.<br/> A possible approach for using a task in the training phase could be: Fine-tune the meta-learner using \(\mathcal{D^{tr}}\), evaluate its performance on \(\mathcal{D^{test}}\), and finally update the model based on this evaluation error.</p> <h2 id="quick-recap-on-maml">Quick recap on MAML</h2> <p>Model-Agnostic Meta-Learning (MAML) <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite> is a well-established algorithm in the field of optimization-based meta-learning. Its goal is to find parameters $\theta$ for a parametric model $f_{\theta}$ that can be efficiently adapted to perform an unseen task from the same task distribution, using only a few training examples. The pre-training of $\theta$ is done using two nested loops (bi-level optimization), with meta-training occurring in the outer loop and task-specific fine-tuning in the inner loop. The task-specific fine-tuning is typically done using a few steps of gradient descent:</p> \[\theta_{i}' = \theta - \alpha\nabla_{\theta}\mathcal{L_{\mathcal{T_{i}}}}(\theta, \mathcal{D^{tr}})\] <p>where $\alpha$ is the inner loop learning rate, $\mathcal{L_{\mathcal{T_{i}}}}$ is a task’s loss function, and $\mathcal{D^{tr}}$ is a task’s training set. The task includes a test set as well: $\mathcal{T_{i}} = (\mathcal{D_{i}^{tr}}, \mathcal{D_{i}^{test}})$.</p> <p>In the outer loop, the meta parameter $\theta$ is updated by backpropagating through the inner loop to reduce errors made on the tasks’ test set using the fine-tuned parameters:</p> \[\theta' = \theta - \eta\nabla_{\theta} \sum_{\mathcal{T_{i}} \sim p(\mathcal{T})}^{} \mathcal{L_{\mathcal{T_{i}}}}(\theta_{i}', \mathcal{D^{test}}).\] <p>Here, $\eta$ is the meta-learning rate. The differentiation through the inner loop involves calculating second-order derivatives, which mainly distinguishes MAML from simply optimizing for a $\theta$ that minimizes the average task loss.</p> <p>It is worth noting that in practical scenarios, this second-order differentiation is computationally expensive, and approximation methods such as first-order MAML (FOMAML) <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite> or Reptile <d-cite key="DBLP:journals/corr/abs-1803-02999"></d-cite> are often used. In FOMAML, the outer loop update is simply: \(\theta' = \theta - \eta\nabla_{\theta'} \sum_{\mathcal{T_{i}} \sim p(\mathcal{T})}^{}\mathcal{L_{\mathcal{T_{i}}}}(\theta_{i}', \mathcal{D^{test}})\), which avoids differentiating through the inner loop.</p> <p>Before proceeding, let’s prepare ourselves for the next sections by looking at the notation we can use when discussing MAML in the few-shot classification regime: The model’s output prediction can be described as $\hat{y} = f_{\theta}(\mathbf{x}) = \underset{c\in[N]}{\mathrm{argmax}} h_{\mathbf{w}} (g_{\phi}(\mathbf{x}), c)$, where we divide our model $f_{\theta}(\mathbf{x})$ (which takes an input $\mathbf{x}$) into a feature extractor $g_{\phi}(\mathbf{x})$ and a classifier $h_\mathbf{w}(\mathbf{r}, c)$, which is parameterized by classification head weight vectors ${\mathbf{w}}_{c=1}^N$. $\mathbf{r}$ denotes an input’s representation, and $c$ is the index of the class we want the output prediction for.</p> <p>Finally, $\theta = {\mathbf{w_1}, \mathbf{w_1}, …, \mathbf{w_N}, \phi}$, and we are consistent with our previous notation.</p> <h2 id="learning-a-single-initialization-vector">Learning a single initialization vector</h2> <p>The first two variants of MAML - we look at - approach the initialization task by initializing the classification head weight vectors identically for all classes. In the paper</p> <p></p> <p><span>   ▶  </span>Han-Jia Ye &amp; Wei-Lun Chao (ICLR, 2022) How to train your MAML to excel in few-shot classification <d-cite key="DBLP:conf/iclr/YeC22"></d-cite>,</p> <p></p> <p>an approach called <strong>UnicornMAML</strong> is presented. It is explicitly motivated by the effect that different class-label assignments can have. Ye &amp; Chao [2022] <d-cite key="DBLP:conf/iclr/YeC22"></d-cite> report that during testing, vanilla MAML can perform very differently for <ins>tasks with the same set of classes</ins>, which are just <ins>differently ordered</ins>. Namely, they report that classification accuracy can vary up to 15% in the one-shot setting and up to 8% in the five-shot setting. This makes MAML’s performance quite unstable. <br/><br/></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/perm_final-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/perm_final-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/perm_final-1400.webp"/> <img src="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/perm_final.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <em>Fig.1 Example of MAML and a class label permutation <d-cite key="DBLP:conf/iclr/YeC22"></d-cite>. We can see the randomness introduced, as $\mathbf{w_1}$ is supposed to interpret the input features as "unicorn" for the first task and as "bee" for the second. For both tasks, the class outputted as a prediction should be the same, as in human perception, both tasks are identical. This, however, is obviously not the case.</em> </p> <p>The solution proposed is fairly simple: Instead of meta-learning $N$ weight vectors for the final layer, only a <ins>single vector</ins> $\mathbf{w}$ is meta-learned and used to initialize all $ \{ \mathbf{w} \}_{c=1}^N $ before the fine-tuning stage.</p> <p>This forces the model to make random predictions before the inner loop, as $\hat{y_c}= h_{\mathbf{w}} (g_{\phi} (\mathbf{x}), c)$ will be the same for all $c \in [1,…,N ]$.</p> <p>After the inner loop, the updated parameters have been computed as usual: \(\theta' = \\{\mathbf{w_1}', \mathbf{w_2}', ..., \mathbf{w_N}', \phi'\\}\). The gradient for updating the single classification head meta weight vector $\mathbf{w}$, is just the aggregation of the gradients w.r.t. all the single $\mathbf{w_c}$:</p> \[\nabla_{\mathbf{w}} \mathcal{L_{\mathcal{T_i}}} (\mathcal{D^{test}}, \theta_i) = \sum_{c \in [N]} \nabla_{\mathbf{w_c}} \mathcal{L_{\mathcal{T_i}}} (\theta_i, \mathcal{D^{test}})\] <p>This collapses the models meta-parameters to $ \theta = \{\mathbf{w}, \phi\} $. <br/><br/></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/unicorn_maml_final-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/unicorn_maml_final-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/unicorn_maml_final-1400.webp"/> <img src="/2023/assets/img/2023-05-01-classification-layer-initialization-in-maml/unicorn_maml_final.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <em>Fig.2 Overview of UnicornMAML <d-cite key="DBLP:conf/iclr/YeC22"></d-cite>. We can see that class label permutations don't matter anymore, as before fine-tuning, the probability of predicting each class is the same.</em> </p> <p>This tweak to vanilla MAML makes UnicornMAML permutation invariant, as models fine-tuned on tasks including the same categories - just differently ordered - will now yield the same output predictions. Also, the method could be used with more challenging datasets where the number of classes varies without any further adaptation: It doesn’t matter how many classification head weight vectors are initialized by the single meta-classification head weight vector.</p> <p>Furthermore, the uniform initialization in Unicorn-MAML addresses the problem of memorization overfitting <d-cite key="DBLP:conf/iclr/YinTZLF20"></d-cite>. The phenomenon describes a scenario where a single model can learn all the training tasks only from the test data in the outer loop. This leads to a model that learns to perform the training tasks but also to a model that doesn’t do any fine-tuning and thus fails to generalize to unseen tasks.</p> <p>Yin et al. [2020] <d-cite key="DBLP:conf/iclr/YinTZLF20"></d-cite> illustrate memorization overfitting using a simple example: Imagine a 3D pose prediction problem, where each task consists of 2D pictures of a certain object. The objects are rotated by some angle from an (unknown) canonical pose in every picture. Each picture is labeled by the angle by which the object is rotated from the object’s canonical pose.</p> <p>In a memorization overfitting scenario, a model learns and memorizes the canonical pose of all the objects shown during training. This way, the model no longer needs to adapt during fine-tuning in the meta-training phase. For correctly dealing with the test examples during training, it could just recognize which object it is looking at and calculate the angle from the remembered canonical pose.<br/> This becomes a problem when unseen objects are shown to the model during meta-testing. Here, it would be critical to infer the canonical pose from the training examples to infer the rotation angle for the test examples correctly. This, however, was not learned by the model in this example.</p> <p>When initializing the classification head identically for all classes, the model is forced to adapt during fine-tuning, as otherwise, it would predict only at the chance level. This prevents memorization overfitting.</p> <p>Ye &amp; Chao [2022] <d-cite key="DBLP:conf/iclr/YeC22"></d-cite> benchmark UnicornMAML on MiniImageNet and TieredImageNet. In the five-shot setting, the approach is claimed to outperform ProtoNet, ProtoMAML, MetaOptNet, MTL+E3BM, RFS-Distill, DeepEMD, MATE+MetaOpt DSN-MR and FEAT. In the one-shot setting, UnicornMAML is reported to perform averagely compared with the other methods.</p> <p>Let’s finally think of how to interpret UnicornMAML: When meta-learning only a single classification head vector, one could say that rather than learning a mapping from features to classes, the weight vector instead learns a prioritization of those features that seem to be more relevant across tasks.</p> <h2 id="zero-initialization">Zero initialization</h2> <p>The second approach for initializing weights identically for all classes is proposed in the paper</p> <p></p> <p><span>   ▶  </span>Chia-Hsiang Kao et al. (ICLR, 2022) MAML is a Noisy Contrastive Learner in Classification <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>.</p> <p></p> <p>Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> modify the original MAML by setting the whole classification head to zero before each inner loop. They refer to this MAML-tweak as the <strong>zeroing trick</strong>.</p> <p>An overview of MAML with the zeroing trick is displayed below:</p> <div class="l-page"> <iframe src="/2023/assets/html/2023-05-01-classification-layer-initialization-in-maml/algorithm.html" frameborder="0" scrolling="no" width="100%" height="400px"></iframe> </div> <p align="center"> <em>Fig.3 MAML with the zeroing trick applied <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>.</em> </p> <p>Through applying the zero initialization, three of the problems addressed by UnicornMAML are solved as well:</p> <ul> <li>MAML, with the zeroing trick applied, leads to random predictions before fine-tuning. This happens as zeroing the whole classification head is also a form of identical weight initialization for all classes. Thus, the zeroing trick solves the problem caused by class label ordering permutations during testing.</li> <li>Through the random predictions before fine-tuning, memorization overfitting is prevented as well.</li> <li>The zeroing trick makes MAML applicable for datasets with a varying number of classes per task.</li> </ul> <p>Interestingly, the motivation for applying the zeroing trick, stated by Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>, is entirely different. In general, Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> want to unveil in what sense MAML encourages its models to learn general-purpose feature representations. They show that under some assumptions, there is a supervised contrastive learning (SCL) objective underlying MAML.</p> <p>In SCL, the label information is leveraged by pulling embeddings belonging to the same class closer together while increasing the embedding distances of samples from different classes <d-cite key="DBLP:conf/nips/KhoslaTWSTIMLK20"></d-cite>. This is achieved by contrasting examples within a batch to each other. If two examples share the same label, the SCL loss is designed to increase their embeddings’ similarity. If the label differs, it enforces the examples embedding similarity to decrease. The SCL loss contains an explicit similarity measure, which distinguishes it from supervised learning.</p> <p>More specifically, Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> show that, in MAML without the zeroing trick, the outer-loop update for the encoder follows a noisy SCL loss under the following assumptions:</p> <ol> <li>The encoder weights are frozen in the inner loop (EFIL assumption)</li> <li>There is only a single inner loop update step.<d-footnote>Note that FOMAML technically follows a noisy SCL loss without this assumption. However, when applying the zeroing trick, this assumption is needed again for stating that the encoder update is following an SCL loss</d-footnote></li> </ol> <p>A noisy SCL loss means that cases can occur where the loss forces the model to maximize similarities between embeddings from samples of different classes. The outer-loop encoder loss in this setting contains an “interference term” which causes the model to pull together embeddings from different tasks or to pull embeddings into a random direction, with the randomness being introduced by random initialization of the classification head. Those two phenomena are termed <em>cross-task interference</em> and <em>initialization interference</em>. Noise and interference in the loss vanish when applying the zeroing trick, and the outer-loop encoder loss turns into a proper SCL loss. Meaning that minimizing this loss forces embeddings of the same class/task together while pushing embeddings from the same task and different classes apart.</p> <p>Those findings are derived using a general formulation of MAML, with a cross-entropy loss, and the details are available in the paper <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>. Also, a slightly simpler example is stated to give an intuition of MAML’s SCL properties. We will briefly summarize it in the following to share this intuition with you.</p> <p>In experiments on the mini-ImageNet and Omniglot datasets, a decent increase in performance is reported for MAML with the zeroing trick compared to vanilla MAML.</p> <h3 id="mamls-scl-intuition">MAML’s SCL Intuition</h3> <p>To get an intuition of how MAML relates to SCL, let’s look at the following setup: an N-way one-shot classification task using MAML with Mean Squared Error (MSE) between the one-hot encoded class label and the prediction of the model. Furthermore, the EFIL assumption is made, the zeroing trick is applied, only a single inner loop update step is used, and only a single task is sampled per batch.</p> <p>In this setting, the classification heads inner-loop update for a single datapoint looks like this:</p> \[\mathbf{w}' = \mathbf{w} - \alpha (-g_{\phi} (\mathbf{x}_{1}^{tr}) \mathbf{t}_{1}^{tr\top})\] <p>$\mathbf{t}_1^{tr}$ refers to the one-hot encoded class label belonging to $\mathbf{x}_1^{tr}$. In words, the features extracted for training example $\mathbf{x}_1^{tr}$ are added to column $\mathbf{w}_c$, with $c$ being the index of 1 in $\mathbf{t}_1^{tr}$. For multiple examples, the features of all training examples labeled with class $c$ are added to the $c^{th}$ column of $\mathbf{w}$.</p> <p>Now, for calculating the model’s output in the outer loop, the model computes the dot products of the columns \(\\{\mathbf{w'} \\}_{c=1}^N\) and the encoded test examples \(g_{\phi}(\mathbf{x}_1^{test})\). To match the one-hot encoded label as well as possible, the dot product has to be large when \(\mathbf{t}_1^{test}\) = \(1\) at index \(c\), and small otherwise. We can see that the loss enforces embedding similarity for features from the same classes while enforcing dissimilarity for embeddings from different classes, which fits the SCL objective.</p> <h2 id="initialization-using-prototypes">Initialization using prototypes</h2> <p>A more sophisticated approach for last-layer initialization in MAML is introduced in the paper</p> <p></p> <p><span>   ▶  </span>Eleni Triantafillou et al. (ICLR, 2020) Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite> .</p> <p></p> <p>As one might guess from the name, <strong>Proto-MAML</strong> makes use of Prototypical Networks (PNs) for enhancing MAML. Unlike the two initialization strategies presented above, Proto-MAML does not force the classification head weights to be initialized identically for all classes before fine-tuning. Instead, it calculates class-specific initialization vectors based on the training examples. This solves some of the problems mentioned earlier (see <a href="#conclusion--discussion">Conclusion &amp; Discussion</a>), but also it adds another type of logic to the classification layer.</p> <p>Let’s revise how PNs work when used for few-shot learning for understanding Proto-MAML afterward:</p> <p>Class prototypes \(\mathbf{c}_{c}\) are computed by averaging over train example embeddings of each class, created by a feature extractor \(g_{\phi}(\mathbf{x})\). For classifying a test example, a softmax over the distances (e.g., squared Euclidean distance) between class prototypes \(\mathbf{c}_{c}\) and example embeddings \(g_{\phi}(\mathbf{x}^{test})\) is used, to generate probabilities for each class.</p> <p>When using the squared Euclidean distance, the model’s output logits are expressed as:</p> \[\begin{align*} &amp;- \vert \vert g_{\phi}(\mathbf{x}) - \mathbf{c}_c \vert \vert^2 \\ =&amp; −g_{\phi}(\mathbf{\mathbf{x}})^{\top} g_{\phi}(\mathbf{x}) + 2 \mathbf{c}_{c}^{\top} g_{\phi}(\mathbf{x}) − \mathbf{c}_{c}^{\top} \mathbf{c}_{c} \\ =&amp; 2 \mathbf{c}_{c}^{\top} g_{\phi}(\mathbf{x}) − \vert \vert \mathbf{c}_{c} \vert \vert^2 + constant. \end{align*}\] <p>Note that the “test” superscripts on $\mathbf{x}$ are left out for clarity. \(−g_{\phi}(\mathbf{x})^{\top} g_{\phi}(\mathbf{x})\) is disregarded here, as it’s the same for all logits, and thus doesn’t affect the output probabilities. When inspecting the left-over equation, we can see that it now has the shape of a linear classifier. More specifically, a linear classifier with weight vectors \(\mathbf{w}_c = 2 \mathbf{c}_c^{\top}\) and biases \(b_c = \vert \vert \mathbf{c}_{c} \vert \vert^2\).</p> <p>Returning to Proto-MAML, Triantafillou et al. [2020] <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite> adapt vanilla MAML by initializing the classification head using the prototype weights and biases, as just discussed. The initialization happens before the inner loop for each task, and the prototypes are computed by MAML’s own feature extractor. Afterward, the fine-tuning works as usual. Finally, when updating $\theta$ in the outer loop, the gradients flow also through the initialization of \(\mathbf{w}_c\) and \(b_c\), which is easy as they fully depend on \(g_{\phi}(\mathbf{x})\).</p> <p>Note that because of computational reasons, Triantafillou et al. [2020] <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite> refer to Proto-MAML as (FO-)Proto-MAML.</p> <p>With Proto-MAML, one gets a task-specific, data-dependent initialization in a simple fashion, which seems super nice. For computing the model’s output logits after classification head initialization, dot products between class prototypes and embedded examples are computed, which again seems very reasonable.</p> <p>One could argue that in the one-shot scenario, Proto-MAML doesn’t learn that much in the inner loop beside the initialization itself. This happens as the dot product between an embedded training example and one class prototype (which equals the embedded training example itself for one class) will be disproportionately high. For a k-shot example, this effect might be less, but still, there is always one training example embedding within the prototype to compare. Following this thought, the training samples would rather provide a useful initialization of the final layer than a lot of parameter adaptation.</p> <p>Proto-MAML is claimed to outperform the approaches, K-nearest neighbours, Finetune, MatchingNet, ProtoNet, fo-MAML and RelationNet on most sub-datasets of MetaDataset <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite>, like ILSVRC-2012 or Omniglot.</p> <h2 id="what-else-is-there">What else is there?</h2> <p>Before proceeding to <a href="#conclusion--discussion">Conclusion &amp; Discussion</a>, here are some pointers to methods that did not perfectly fit the topic but which are closely related:</p> <p>The first method worth mentioning is called Latent Embedding Optimization (LEO) <d-cite key="DBLP:conf/iclr/RusuRSVPOH19"></d-cite>. The authors encode the training data in a low dimensional subspace, from which model parameters $\theta$ can be generated. In the example presented, $\theta$ consists only of $\mathbf{w}$, so for the first inner-loop iteration, this would perfectly fit our initialization topic. The low-dimensional code is generated using a feed-forward encoder, as well as a relation network. Using the relation network allows LEO to consider relations between the training examples of different classes. Very similar classes, for example, might require different decision boundaries than more distinct classes, hence the intuition.</p> <p>LEO deviates from the initialization scheme, however, as optimization is done in the low dimensional subspace and not on the model’s parameters directly. It is stated that optimizing in a lower dimensional subspace helps in low-data regimes.</p> <p>Another related method is called MetaOptNet <d-cite key="DBLP:conf/cvpr/LeeMRS19"></d-cite>. In this approach, convex base learners, like support vector machines, are used as the classification head. Those can be optimized till convergence, which solves, e.g., the problem of varying performance due to random class label orderings.</p> <h2 id="conclusion-and-discussion">Conclusion and Discussion</h2> <p>To conclude, we’ve seen that a variety of problems can be tackled by using initialization strategies for MAML’s linear classification head, including:</p> <ul> <li>Varying performance due to random class label orderings</li> <li>Ability of MAML to work on datasets where the number of classes per task varies</li> <li>Memorization overfitting</li> <li>Cross-task interference</li> <li>and Initialization interference.</li> </ul> <p>Furthermore, for all the approaches presented, a decent gain in performance is reported in comparison to vanilla MAML. It seems, therefore, very reasonable to spend some time thinking about the last layer initialization.</p> <p>Looking at the problems mentioned and variants discussed in more detail, we can state that all the different variants make MAML <strong>permutation invariant with regard to class label orderings</strong>. UnicornMAML and the zeroing trick solve it by uniform initialization of $\mathbf{w}$. In Proto-MAML, the initialization adapts to the class label assignments, so it’s permutation invariant as well.</p> <p>Also, all variants are compatible with <strong>datasets where the number of classes per task varies</strong>. In UnicornMAML, an arbitrary number of classification head vectors can be initialized with the single meta-learned classification head weight vector. When zero-initializing the classification head, the number of classes per task does not matter as well. In Proto-MAML, prototypes can be computed for an arbitrary number of classes, so again, the algorithm works on such a dataset without further adaption.</p> <p>Next, UnicornMAML and the zeroing trick solve <strong>memorization overfitting</strong>, again by initializing $\mathbf{w}$ identically for all classes. Proto-MAML solves memorization overfitting as well, as the task-specific initialization of $\mathbf{w}$ itself can be interpreted as fine-tuning.</p> <p><strong>Cross-task interference</strong> and <strong>initialization interference</strong> are solved by the zeroing trick. For the other methods, this is harder to say, as the derivations made by Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> are quite a case specific. Intuitively, Proto-MAML should solve cross-task interference, as the classification head is reinitialized after each task. Initialization interference is not solved by either ProtoMAML or UnicornMAML, as random initialization before the beginning of meta-training remains.</p> <p>Note that in discussion with a reviewer, Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> state that the main results they show are achieved by models which had the zeroing trick implemented but which didn’t follow the EFIL assumption. They argue that using only the zeroing trick still enhances supervised contrastiveness. This kind of puts their whole theory into perspective, as without the EFIL assumption, MAML with the zeroing trick is neither an SCL algorithm nor a noisy SCL algorithm. Still, noticeable performance gains are reported though.</p> <p>The question arises whether the whole theoretical background is needed or whether the zeroing tricks benefit is mainly the identical initialization for all classes, like in UnicornMAML. It would be nice to see how the single learned initialization vector in UnicornMAML turns out to be shaped and how it compares to the zeroing trick. While the zeroing trick reduces cross-task noise and initialization noise, a single initialization vector can weight some features as more important than others for the final classification decision across tasks.</p> <p>In contrast to the uniform initialization approaches, we have seen Proto-MAML, where class-specific classification head vectors are computed for initialization based on the training data.</p> <p>Finally, Ye &amp; Chao [2022] <d-cite key="DBLP:conf/iclr/YeC22"></d-cite> compare the performance between Proto-MAML and UnicornMAML on MiniImageNet and TieredImageNet. UnicornMAML performs slightly better here in the one- and five-shot settings. Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> report that MAML with the zeroing-trick outperforms unmodified MAML on the mini-ImageNet and Omniglot datasets. They do not provide a benchmark score, however.</p>]]></content><author><name>Nys Tjade Siegel</name></author><summary type="html"><![CDATA[["This blog post discusses different strategies for initializing the classification layers parameters before fine-tuning on a new task in Model-Agnostic Meta-Learning. Each of the strategies in question has emerged from a different problem", "and it will be analyzed whether one approach can solve the problems addressed by the other approaches."]]]></summary></entry><entry><title type="html">Data Poisoning is Hitting a Wall</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/facial-poisoning/" rel="alternate" type="text/html" title="Data Poisoning is Hitting a Wall"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/facial-poisoning</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/facial-poisoning/"><![CDATA[<h2 id="overview-and-motivation">Overview and Motivation</h2> <p>To illustrate the data poisoning process, and to tie in with the paper below, let’s describe data poisoning against the backdrop of the facial recognition problem.</p> <p>Facial recognition systems have been known to pose a severe threat to society. With unprecedented advancements in AI research, it is evident that this threat will be around for a while. There has been a steady increase in vendors offering facial recognition services for downstream applications — ranging from customer onboarding tools to criminal identification for police forces. The systems provided by these vendors are usually trained on images of users’ faces scraped from the Web. Ethical and moral concerns aside, this poses a considerable risk to the privacy of individuals.</p> <h4 id="what-is-data-poisoning">What is Data Poisoning?</h4> <p>Keeping this in mind, a growing body of work has emerged that allows users to fight back using principles from adversarial machine learning. Primary among these is the technique of data poisoning - where users can perturb pictures that they post online so that models that train on these become <em>poisoned</em>. In other words, once a model has been introduced to a perturbed image of a user, it misidentifies any future instances of that person.</p> <p>Services like <em>Fawkes</em> popularized this approach by offering a service promising “strong protection against unauthorized {facial recognition} models.” Users could pass their images through Fawkes and receive poisoned photos - virtually identical to the naked eye, which were then posted to social media, alleviating any worries that they might be used to identify them in the future. It quickly gained popularity, was covered by the New York Times <d-footnote>[This tool could protect your photos from Facial Recognition](https://www.nytimes.com/2020/08/03/technology/fawkes-tool-protects-photos-from-facial-recognition.html)</d-footnote> and received over 500,000 downloads. Following Fawkes’ success, similar systems were proposed in academic and commercial settings.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/facial_poisoning-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/facial_poisoning-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/facial_poisoning-1400.webp"/> <img src="/2023/assets/img/2023-05-01-facial-poisoning/facial_poisoning.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <p>The authors of the paper, however, look at these systems from a different perspective. They argue that services like Fawkes (and poisoning strategies in general) cannot protect users’ privacy when it comes to facial recognition systems. In fact, it usually exacerbates the situation by providing them with a false sense of security. For instance, there might have previously been a privacy-focused user who would have refrained from uploading their photos to the Internet. However, they might do so now under the false belief that their poisoned photos would work towards protecting their privacy. Thus, these users are now <em>less private</em> than they were before.</p> <h2 id="why-doesnt-data-poisoning-work">Why doesn’t data poisoning work?</h2> <p>While data poisoning may have uses in other fields, such as healthcare, this post shows that it would not protect against facial recognition models. The main reason for this is due to a fundamental asymmetry between the users and the model trainers. Let us take the scenario described in the above figure. A user commits to an attack and uploads a perturbed image of themselves to the Web. This image eventually gets scraped by the model as part of its data collection strategy. In this case, the model trainer, or the vendors offering facial recognition services, now benefit from acting second. This provides them with two significant advantages:</p> <ul> <li> <p>Since image poisoning systems cater to large user bases, these systems are usually made publicly accessible. This allows the model trainers to become aware of the technique used, which, in turn, helps them apply techniques to resist the poisoning attacks. This strategy of using alternate training techniques is known as an <strong>adaptive defense</strong>.</p> </li> <li> <p>As current poisoning attacks are designed to prevent <em>existing</em> facial recognition tools from working, there is no reason to assume that future models will also be poisoned. So, trainers can simply wait a while and use newer models to keep identifying users, which would be invulnerable to poisoning attacks. This technique can (aptly) be named an <strong>oblivious defense</strong>.</p> </li> </ul> <p>Observant readers might equate this setting of continually evolving attack and defense tactics to an <em>arms race</em>. However, since a perturbation applied to an image cannot be changed once scraped by the model, a successful attack has to remain effective against <em>all</em> future models, even those trained adaptively against the attack. A better alternative to this would be pushing for legislation that restricts the use of privacy-invasive facial recognition systems.</p> <h2 id="high-level-idea">High Level Idea</h2> <p>We now look at the conclusions put forward in the excellent paper written by Radiya-Dixit <em>et al</em>.</p> <ol> <li>An adaptive model trainer with black-box access to facial recognition systems like Fawkes can train a robust model that resists poisoning attacks and correctly identifies all users with high accuracy.</li> <li>An adaptive model trainer can also repurpose this model to <em>detect</em> perturbed pictures with near-perfect accuracy.</li> <li>Image poisoning systems have already been broken by newer facial recognition that appeared less than a year after the attacks were introduced and employed superior training strategies.</li> <li>It is possible to increase the robustness of a model (against poisoning attacks) without degrading its accuracy in identifying ‘clean’ images.</li> </ol> <p>Let us take a closer look and deconstruct how the authors arrived at these conclusions.</p> <h2 id="experiments">Experiments</h2> <p>For clarity, before we arrive at the individual conclusions, we look at the setup used by the authors to carry out their experiments.</p> <p>The authors evaluate three distinct poisoning attacks: <strong>Fawkes v0.3</strong>, <strong>Fawkes v1.0</strong><d-cite key="shan2020fawkes"></d-cite>, and a separate attack published at ICLR 2021 called <strong>LowKey</strong><d-cite key="cherepanova2021lowkey"></d-cite>. All of these function on the same underlying principle of data poisoning. Their goal is to force the facial recognition model to associate an image with spurious features absent in unperturbed images.</p> <p>The experiments are performed with the <em>FaceScrub</em> dataset<d-cite key="ng2014data"></d-cite>, which contains over 50,000 pictures of 530 celebrities. A sample run of an experimental procedure can be described as follows: A user, in this case, one of the celebrities in the <em>FaceScrub</em> dataset, perturbs all of their images with <em>Fawkes</em> or <em>LowKey</em> in their strongest settings. These images then end up as the training data used by the model trainer. The model trainer uses the standard approach for training their facial recognition system by employing a pre-trained feature extractor to convert pictures into embeddings. Given a test image, the model tries to find a training example that minimizes the distance between them in the embedding space and returns the identity associated with the training example.</p> <p>The authors use various models as feature extractors from <em>FaceNet</em><d-cite key="schroff2015facenet"></d-cite> to OpenAI’s <em>CLIP</em><d-cite key="radford2021learning"></d-cite>. This is an important step that helps quantify the effectiveness of the <strong>oblivious defense</strong> strategy. ***</p> <h4 id="adaptive-defenses-break-facial-poisoning-attacks">Adaptive defenses break facial poisoning attacks</h4> <p>This section describes how the model trainer can adaptively train a generic feature extractor that can resist poisoning attacks.</p> <p>The model trainer begins by collecting a public dataset of unperturbed images. In this case, that would be a canonical dataset of celebrities that are a part of the <em>FaceScrub</em> dataset. With black-box access to the poisoning tool, the trainer calls it to obtain perturbed samples of the same images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/adaptive-attack.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/adaptive-attack.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/adaptive-attack.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-facial-poisoning/adaptive-attack.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With access to both unperturbed images and their corresponding poisoned counterparts, the trainer can teach a model to produce similar embeddings for both sets of pictures, encouraging the model to adaptively learn robust features. This is done hoping that this robustness would eventually generalize to perturbations’ applied to other images.</p> <p>While the above strategy works in theory, it requires direct intervention from model trainers by using the ‘clean’ images provided by them. This would not scale well, especially for large-scale facial recognition systems that look at millions of photographs. However, this attack could also occur without the trainers’ explicit involvement. There is a high possibility that some users already have unperturbed images of themselves on the Web; either they forgot to perturb some pictures, or they were uploaded by someone else. Feature extractors trained on these pictures would then be encouraged to learn robust features.</p> <p><strong>Results:</strong> All three attacks were evaluated against a non-robust <em>WebFace</em> model to establish a baseline. They were found to have a misclassification rate of 55-77% for users who poisoned their pictures online. This compares starkly to a rate of 8% for unprotected users. However, when trained adaptively, the misclassification rate for all users - protected or unprotected - dropped to 5-8%, and all poisoning attacks were rendered ineffective. ***</p> <h4 id="attack-detection">Attack Detection</h4> <p>Since the model trainers have black-box access to the facial poisoning tools (<em>Fawkes</em> and <em>LowKey</em>), they can also turn the tables and build a detector to determine whether a specific image has been perturbed. Such a detector can dynamically filter out perturbed photos, allowing the model to retain only unperturbed pictures of a user. Moreover, detecting an attack could be a privacy concern (for instance, law enforcement might actively target users whose attack attempts are detected).</p> <p>To verify this, the authors were able to fine-tune a standard pre-trained <em>ImageNet</em> model to distinguish between perturbed and clean images of 25 random celebrities in the dataset. The model detected the poisoned images with near-perfect precision (99.8%) and recall (99.8%). ***</p> <h4 id="time-is-all-you-need">Time is all you need</h4> <p>Rather than creating poisoned counterparts to clean images and adaptively training a model, trainers have a much simpler alternative. They can simply wait for better facial recognition systems to be developed and then retroactively apply such a system to pictures they scraped in the past. <em><strong>Simply put, facial poisoning attacks cannot withstand the test of time</strong></em>.</p> <p>To bypass this <em>oblivious</em> defense strategy, an attack must not only be able to fool all present models but also be effective against future iterations without changing its perturbation. Asymetrically (to the benefit of the model trainer) newer techniques need not be robust to all attacks; instead, they just have to resist the specific method used in previous pictures.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/oblivious-attack.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/oblivious-attack.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-facial-poisoning/oblivious-attack.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-facial-poisoning/oblivious-attack.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To confirm this, the paper included a study where <em>Fawkes</em> was pitted against various feature extractors ordered chronologically. While the original <em>Fawkes v0.3</em> was utterly ineffective against any model apart from <em>WebFace</em>, the updated v1.0 could transfer its attack to other extractors like <em>VGGFace</em>, <em>FaceNet</em>, and <em>ArcFace</em>. However, while <em>Fawkes v1.0</em> provided a perfect (100%) error rate on the <em>Celeb1M</em> model (the one it was trained to target), it failed miserably against more recent extractors like <em>MagFace</em><d-cite key="meng2021magface"></d-cite> or <em>CLIP</em>. A similar trend was also observed when using <em>LowKey</em>. While it fared better than <em>Fawkes</em> and could transfer its attack to MagSafe, LowKey failed to break the fine-tuned <em>CLIP</em> model trained by the authors.</p> <p>To provide more credence to their findings, the authors also illustrated how users who downloaded an older model (<em>Fawkes v0.3</em>, for example) could not ‘regain’ their privacy by switching to an updated attack. For brevity, this post does not go into the specifics, but we encourage interested readers to look at the paper and additional supplementary material. ***</p> <h4 id="robustness-shouldnt-come-at-the-cost-of-accuracy">Robustness shouldn’t come at the cost of accuracy</h4> <p>A potential caveat for the <em>adaptive</em> and <em>oblivious</em> defenses is that increased robustness may come at the cost of decreased accuracy. For example, the CLIP model is much more robust than all the other feature extractors, but its clean accuracy falls slightly below the best models. In most cases, a trainer might be hesitant to deploy a <em>CLIP</em>-based model if only a small minority of users try to attack the system.</p> <p>Keeping this in mind, the authors demonstrated two approaches that allow model trainers to incorporate the best qualities of both worlds:</p> <p><strong>Top2:</strong> This approach involved having a human in the loop. The authors propose that the system simply run the image through both models and return two candidate labels. To further streamline the process, the system could pass the image to the robust model only when the more accurate model cannot get a result. Humans could visually inspect these images to check for inconsistencies or determine if they were poisoned.</p> <p><strong>Confidence Thresholding:</strong> To automate the above process, the system could begin by passing the image through the most accurate model and checking the prediction’s confidence. This can be quantitatively defined as the distance between the target picture and its nearest neighbor in the embedding space. If the system finds the confidence below a certain threshold, the image is passed through the robust model instead.</p> <p>The paper demonstrates a facial recognition system that uses <em>MagFace</em> for an accurate model and combines that with a more robust model like the fine-tuned <em>CLIP</em> or an adaptively trained model. In both cases, the clean accuracy of the system matches or exceeds that of <em>MagFace</em>, while retaining high robustness to attacks.</p> <hr/> <h3 id="conclusion">Conclusion</h3> <p>The main takeaway from this post is that data poisoning is no longer an effective method to protect users from facial recognition systems. The original premise for developing poisoning attacks was to facilitate an ‘arms race,’ where better attacks could counteract improved defenses. However, the people who deploy facial recognition models would always have the upper hand.</p> <p>The paper shows that facial recognition models can be trained to detect and overcome poisoning attacks by simply having black-box access to a public-facing tool or just waiting for newer models and retroactively using them. To compete even against the latter category of systems, users would have to presume that minimal changes will be made to facial recognition models in the upcoming years. Given the state and pace of research in the field, that seems highly unlikely. ***</p> <h3 id="outlook">Outlook</h3> <p>This blog post provides a better understanding of the techniques used to neutralize the effects of data poisoning from the ICLR 2022 paper <em>Data Poisoning Won’t Save You from Facial Recognition.</em> We hope that this has been of help to researchers and practitioners in the fields of adversarial ML.</p> <p>We now look to provide some clarifications and how we think this work would fit in the current age of machine learning.</p> <p><strong>The work is a net positive</strong> This paper takes a gloomy stance on the current state of protection against facial recognition models. By stating that model trainers would always have the upper hand in the race by simply switching to a more advanced framework, the authors quash any possibility of a technological solution. Instead, they argue that a legislative approach might hold the key to solving the problem. Looking at the discussion between the authors and the reviewers before the acceptance of the paper <d-footnote>[ICLR OpenReview](https://openreview.net/forum?id=B5XahNLmna)</d-footnote>, it was clear that the reviewers were reluctant to accept the finality of the solution - a sentiment we’re sure would be shared by many others. However, if nothing else, this paper warns users against the futility of using commercial products like Fawkes to protect their identities. In alleviating the false sense of security provided by data poisoning attacks, this paper - and, by extension, this post - serves as a net positive for users’ privacy.</p> <p><strong>Is legislation the answer?</strong> With artificial intelligence embedding itself into society at an unprecedented rate, it is clear that a complete overhaul of legislative frameworks is urgently required. As AI becomes more mainstream, privacy-invasive systems could graduate from storing information to using them for financial incentives. While we have seen this happen with users’ browsing data, the repercussions of using biometrics would be much more severe. In fact, there have already been cases where facial recognition has been used by companies on users without their prior explicit consent. <d-footnote> [Madison Square Garden has put lawyers who represent people suing it on an 'exclusion list' to keep them out of concerts and sporting events](https://www.nytimes.com/2022/12/22/nyregion/madison-square-garden-facial-recognition.html)</d-footnote></p> <p>While we agree with the authors for a push towards proper legislation, given the rate of progress, we believe the community can do more. Legislation is a process that moves slowly and usually needs uniform implementation. Literature on the subject has shown that each country has its own views on the emerging landscape of AI <d-footnote>[How different countries view artificial intelligence](https://www.brookings.edu/research/how-different-countries-view-artificial-intelligence/)</d-footnote> and bases its rules on those views. These may or may not always work. We believe a temporary stopgap in the form of a technological solution would be helpful, while a legislative solution holds maximum promise in the long run.</p> <hr/> <h3 id="tldr">TL;DR</h3> <p>This post broadly explores the ineffectiveness of data poisoning strategies against facial recognition models. It shows that commercial solutions like Fawkes and LowKey, which allow users to perturb their photos before posting them to social media, offer no protection to the users once their pictures are scraped.</p> <p>It reveals that an ‘oblivious’ model trainer can simply wait long enough for future developments to nullify the effects of the perturbation. Or, since the people developing the facial recognition systems also have access to poisoning tools, they can simply develop strategies to detect and adapt to the perturbations.</p> <p>Finally, given that there are no technical solutions to the problem, the best approach would be to push for legislation to counteract privacy-invasive facial recognition systems.</p> <hr/>]]></content><author><name>Rajat Sahay</name></author><summary type="html"><![CDATA[In this post, we look at the paper 'Data Poisoning Won't Save You From Facial Recognition', discuss the impact of the work, and additionally look at how this work fares in the current state of adversarial machine learning. Being a blog post as opposed to a traditional paper, we try to avoid inundating the reader with mathematical equations and complex terminologies. Instead, we aim to put forth this work's primary concept and implications, along with our observations, in a clear, concise manner. Don't want to go through the entire post? Check out the TL;DR at the end for a quick summary.]]></summary></entry><entry><title type="html">A Hitchhiker’s Guide to Momentum</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/hitchhikers-momentum/" rel="alternate" type="text/html" title="A Hitchhiker’s Guide to Momentum"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/hitchhikers-momentum</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/hitchhikers-momentum/"><![CDATA[<blockquote> <p>Dedicated to the memory of Boris Polyak <a href="https://memorialsource.com/memorial/polyak">(May 4, 1935 - February 3, 2023)</a>, inventor of this method and pioneer of the field of optimization.</p> </blockquote> <div style="display: none"> $$ \def\argmin{\mathop{\mathrm{arg\,min}}} \def\xx{\pmb{x}} \def\HH{\pmb{H}} \def\bb{\pmb{b}} \def\EE{ \mathbb{E} } \def\RR{ \mathbb{R} } \def\lmax{L} \def\lmin{\mu} \def\defas{\stackrel{\text{def}}{=}} \definecolor{colormomentum}{RGB}{27, 158, 119} \definecolor{colorstepsize}{RGB}{217, 95, 2} \def\mom{ {\color{colormomentum}{m}} } \def\step{ {\color{colorstepsize}h} } $$ </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h2> <p>Gradient descent with momentum,<d-cite key="polyak1964some"></d-cite> also known as heavy ball or momentum for short, is an optimization method designed to solve unconstrained minimization problems of the form \begin{equation} \argmin_{\xx \in \RR^d} \, f(\xx)\,, \end{equation} where the objective function \(f\) is differentiable and we have access to its gradient \(\nabla f\). In this method the update is a sum of two terms. The first term is the difference between the current and the previous iterate \((\xx_{t} - \xx_{t-1})\), also known as <em>momentum term</em>. The second term is the gradient \(\nabla f(\xx_t)\) of the objective function.</p> <p class="framed"> <b class="underline">Gradient Descent with Momentum</b><br/> <b>Input</b>: starting guess \(\xx_0\), step-size \(\step &gt; 0\) and momentum parameter \(\mom \in (0, 1)\).<br/> \(\xx_1 = \xx_0 - \dfrac{\step}{\mom+1} \nabla f(\xx_0)\) <br/> <b>For</b> \(t=1, 2, \ldots\) compute \begin{equation}\label{eq:momentum_update} \xx_{t+1} = \xx_t + \mom(\xx_{t} - \xx_{t-1}) - \step\nabla f(\xx_t) \end{equation} </p> <p>Despite its simplicity, gradient descent with momentum exhibits unexpectedly rich dynamics that we’ll explore on this post.</p> <h3 id="history-and-related-work">History and related work</h3> <p>The origins of momentum can be traced back to Frankel’s method in the 1950s for solving linear system of equations.<d-cite key="frankel1950convergence"></d-cite> It was later generalized by Boris Polyak to non-quadratic objectives<d-cite key="polyak1964some"></d-cite>. While the quadratic case is by now well understood, the general strongly convex case has instead had some fascinating developments in the last years. In the convex (but not strongly convex) case, <a href="https://arxiv.org/pdf/1412.7457.pdf">Ghadimi et al.</a><d-cite key="ghadimi2015global"></d-cite> showed the global convergence of the method in 2015. One year later, Lessard, Recht and Packard<d-cite key="lessard2016analysis"></d-cite> surprised the community with an example of a on-dimensional Lipschitz gradient and strongly convex function where the heavy-ball method (with a specific choice of parameters) doesn’t converge nor diverge, but cycles instead.</p> <p>A paper that also explores the dynamics of momentum is Gabriel Goh’s excellent <a href="https://distill.pub/2017/momentum/">Why Momentum Really Works</a>.<d-cite key="goh2017momentum"></d-cite> There are subtle but important differences between both analysis. The landscape described in the section <a href="https://distill.pub/2017/momentum/#momentum2D">“The Dynamics of Momentum”</a> describe the improvement along the direction <em>of a single eigenvector</em>. This partial view produces some misleading conclusions. For example, along the direction of a single eigenvector, the largest improvement is achieved with zero momentum and a step-size of 1 over the associated eigenvalue. This conclusion however doesn’t hold in higher dimensions, where as we will see, the momentum term that yields the fastest convergence is non-zero.</p> <p>A <strong>stochastic variant</strong> of this method, where the gradient is replaced by a stochastic estimate, is one of the most popular methods for deep learning. This has led in recent years to a flurry of research –and improved understanding – of this stochastic variant. Although we won’t be analyzing the stochastic variant, due to its importance, let us briefly mention some recent works.</p> <p>One of the first works to highlight the importance of momentum for training deep neural networks is the 2013 paper by <a href="https://arxiv.org/abs/1712.07628">Sutskever et al</a>.<d-cite key="sutskever2013importance"></d-cite> Some recent progress in the field has been possible by viewing the stochastic variant as an averaging method.<d-cite key="flammarion2015averaging"></d-cite> This has led to the development of improved last-iterate convergence rates <d-cite key="taylor2019stochastic"></d-cite> <d-cite key="tao2021the"></d-cite> and a better understanding of it’s behavior in the non-convex setting.<d-cite key="defazio2020momentum"></d-cite> Another fruitful line of work has been to consider the <i>overparametrized</i> (or interpolation) setting, where the variance of the updates vanishes at the optimum. In this regime, different momentum-like methods have been shown to enjoy a faster worst-case convergence rate than SGD.<d-cite key="Liu2020Accelerating"></d-cite> <d-cite key="vaswani2019fast"></d-cite></p> <h2 id="how-fast-is-momentum">How Fast is Momentum?</h2> <p>Momentum is <em>fast</em>. So fast that it’s often the default choice of machine learning practitioners. But can we quantify this more precisely?</p> <p>Throughout the post we’ll assume that our objective function \(f\) is a quadratic objective of the form \begin{equation}\label{eq:opt} f(\xx) \defas \frac{1}{2}(\xx - \xx^\star) \HH (\xx - \xx^\star)~, \end{equation} where \(\HH\) is a symmetric positive definite matrix and \(\xx^\star\) is the minimizer of the objective. We’ll assume that the eigenvalues of \(\HH\) are in the interval \([\mu, L]\), where \(\mu\) is strictly positive by the PSD assumption.</p> <p>The measure we’ll use to quantify the speed of convergence is the rate of convergence. This is the worst-case relative improvement in the iterate suboptimality at iteration \(t\), defined as \begin{equation}\label{eq:convergence_rate} r_t \defas \sup_{\xx_0, \text{eigs}(\HH) \in [\mu, L]} \frac{\|\xx_{t} - \xx^\star\|}{\|\xx_{0} - \xx^\star\|}\,. \end{equation} This is a worst-case measure because of all problem instances, we take worst possible initialization \(\xx_0\) and matrix \(\HH\) with eigenvalues in the interval \([\mu, L]\).</p> <p>This is a measure of how much progress is made (in the worst-case) at iteration \(t\). The smaller the value of \(r_t\), the faster the algorithm converges. Since all algorithms that we consider converge exponentially fast, for large enough \(t\) the error is of the order of \(\mathcal{O}{(\text{constant}^t)}\). Hence the most informative quantity is the value of \(\text{constant}\) in this expression. We’ll call this quantity the <i>asymptotic rate of convergence</i>, and denote it: \begin{equation} r_{\infty} \defas \limsup_{t \to \infty} \sqrt[t]{r_t}\,. \end{equation} This is the quantity we’ll be discussing throughout the post and what we’ll use to compare the speed of momentum for different values of its hyperparameters.</p> <h3 id="a-connection-between-optimization-methods-and-polynomials">A connection between optimization methods and polynomials</h3> <p>To compute easily the asymptotic rate of convergence for all admissible values of step-size and momentum, we’ll use a connection between optimization of quadratic functions and the theory of orthogonal polynomials. This theory was extensively used in the early days of numerical analysis <d-cite key="Rutishauser1959"></d-cite> and provides an elegant and simple way to compute asymptotic rates (and non-asymptotic ones, although not the topic of this blog post) from known results in the theory of orthogonal polynomials. We favor this technique for its simplicity and elegance, although ones that also be used with identical results. Other techniques include the linear operator technique used by Polyak,<d-cite key="polyak1964some"></d-cite> the estimate sequences technique pioneered by Nesterov<d-cite key="nesterov1983method"></d-cite> or the use of Lyapunov functions.<d-cite key="JMLR:v22:20-195"></d-cite></p> <p>The main result that will allow us to make the link between optimization and orthogonal polynomials is the following result. It’s origins seem unclear, although a proof can be found in the 1959 monograph of Rutishauser.<d-cite key="Rutishauser1959"></d-cite></p> <p class="lemma"> Consider the following polynomial \(P_t\) of degree \(t\), defined recursively as: \begin{equation} \begin{split} &amp;P_{t+1}(\lambda) = (1 + \mom - \step \lambda ) P_{t}(\lambda) - \mom P_{t-1}(\lambda)\\ &amp;P_1(\lambda) = 1 - \frac{\step}{1 + \mom} \lambda\,, ~ P_0(\lambda) = 1\,,~ \end{split}\label{eq:def_residual_polynomial2} \end{equation} Then we can write the suboptimality at iteration \(t\) as \begin{equation} \xx_t - \xx^\star = P_t(\HH) \left( \xx_0 - \xx^\star \right) \,, \end{equation} where \(P_t(\HH)\) is the matrix obtained from evaluating the (originally real-valued) polynomial \(P_t\) at the matrix \(\HH\). </p> <p>This last identity will allow us to easily compute convergence rates. In particular, plugging it into the definition of the convergence rate \eqref{eq:convergence_rate} we get that the rate is determined by the absolute value of the residual polynomial over the \([\mu, L]\) interval: \begin{align} r_t &amp;= \sup_{\xx_0, \text{eigs}(\HH) \in [\mu, L]} \frac{\|P_t(\HH) \left( \xx_0 - \xx^\star \right)\|}{\|\xx_{0} - \xx^\star\|} \\ &amp; = \sup_{\text{eigs}(\HH) \in [\mu, L]} \|P_t(\HH)\| \\ &amp; = \sup_{\lambda \in [\mu, L]} \lvert P_t(\lambda) \rvert\,. \end{align} We’ve now reduced the problem of computing the convergence rate to the problem of computing the absolute value of a polynomial over a given interval. This is a problem that has been extensively studied in the theory of orthogonal polynomials. In particular, we’ll use known bounds on Chebyshev polynomials of the first and second kind, as the residual polynomial of momentum can be written as a convex combination of these two polynomials. This fact is proven in the next result, which is a generalization of equation (II.29) in (Rutishauser 1959).<d-cite key="Rutishauser1959"></d-cite></p> <p class="lemma"> The residual polynomial of momentum can be written in terms of Chebyshev polynomials of the first and second kind as \begin{align} P_t(\lambda) = \mom^{t/2} \left( {\small\frac{2\mom}{1+\mom}}\, T_t(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\,U_t(\sigma(\lambda))\right)\,. \end{align} where \(\sigma(\lambda) = {\small\dfrac{1}{2\sqrt{\mom}}}(1 + \mom - \step\,\lambda)\,\) is a linear function that we'll refer to as the <span class="underline">link function</span> and \(T_t\) and \(U_t\) are the Chebyshev polynomials of the first and second kind respectively. </p> <div class="wrap-collapsible-XXX"> <input id="collapsible3" class="toggle" type="checkbox"/> <label for="collapsible3" class="lbl-toggle" tabindex="0"><b>Show proof</b></label><div class="collapsible-content"><div class="content-inner"><div class="proof" id="proof-variance"> <p> Let's denote by \(\widetilde{P}_t\) the right hand side of the above equation, that is, \begin{equation} \widetilde{P}_{t}(\lambda) \defas \mom^{t/2} \left( {\small\frac{2 \mom}{1 + \mom}}\, T_t(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\, U_t(\sigma(\lambda))\right)\,. \end{equation} Our goal is to show that \(P_t = \widetilde{P}_t\) for all \(t\). </p> <p> For \(t=1\), \(T_1(\lambda) = \lambda\) and \(U_1(\lambda) = 2\lambda\), so we have \begin{align} \widetilde{P}_1(\lambda) &amp;= \sqrt{\mom} \left(\tfrac{2 \mom}{1 + \mom} \sigma(\lambda) + \tfrac{1 - \mom}{1 + \mom} 2 \sigma(\lambda)\right)\\ &amp;= \frac{2 \sqrt{\mom}}{1 + \mom} \sigma(\lambda) = 1 - \frac{\step}{1 + \mom} \lambda\,, \end{align} which corresponds to the definition of \(P_1\) in \eqref{eq:def_residual_polynomial2}. </p> <p> Assume it's true for any iteration up to \(t\), we will show it's true for \(t+1\). Using the three-term recurrence of Chebyshev polynomials we have \begin{align} &amp;\widetilde{P}_{t+1}(\lambda) = \mom^{(t+1)/2} \left( {\small\frac{2 \mom}{1 + \mom}}\, T_{t+1}(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\, U_{t+1}(\sigma(\lambda))\right) \\ &amp;= \mom^{(t+1)/2} \Big( {\small\frac{2 \mom}{1 + \mom}}\, (2 \sigma(\lambda) T_{t}(\sigma(\lambda)) - T_{t-1}(\sigma(\lambda))) \nonumber\\ &amp;\qquad\qquad + {\small\frac{1 - \mom}{1 + \mom}}\, (2 \sigma(\lambda) U_{t}(\sigma(\lambda)) - U_{t-1}(\sigma(\lambda)))\Big)\\ &amp;= 2 \sigma(\lambda) \sqrt{\mom} P_t(\lambda) - \mom P_{t-1}(\lambda)\\ &amp;= (1 + \mom - \step \lambda) P_t(\lambda) - \mom P_{t-1}(\lambda) \end{align} where the third identity follows from grouping polynomials of same degree and the induction hypothesis. The last expression is the recursive definition of \(P_{t+1}\) in \eqref{eq:def_residual_polynomial2}, which proves the desired \(\widetilde{P}_{t+1} = {P}_{t+1}\). </p> </div></div></div></div> <h3 id="tools-of-the-trade-the-two-faces-of-chebyshev-polynomials">Tools of the trade: the two faces of Chebyshev polynomials</h3> <p>A key feature that we’ll use extensively about Chebyshev polynomials is that they behave very differently inside and outside the interval \([-1, 1]\). Inside this interval (shaded blue region) the magnitude of these polynomials stays close to zero, while outside it explodes:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/two_phases_chebyshev.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/two_phases_chebyshev.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/two_phases_chebyshev.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/two_phases_chebyshev.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s make this observation more precise.</p> <p><strong>Inside</strong> the \([-1, 1]\) interval, Chebyshev polynomials admit the <a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials#Trigonometric_definition">trigonometric definitions</a> \(T_t(\cos(\theta)) = \cos(t \theta)\) and \(U_{t}(\cos(\theta)) = \sin((t+1)\theta) / \sin(\theta)\) and so they have an oscillatory behavior with values bounded in absolute value by 1 and \(t+1\) respectively.</p> <p><strong>Outside</strong> of this interval the Chebyshev polynomials of the first kind admit the <a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials#Explicit_expressions">explicit form</a> for \(|\xi| \ge 1\): \begin{align} T_t(\xi) &amp;= \dfrac{1}{2} \Big(\xi-\sqrt{\xi^2-1} \Big)^t + \dfrac{1}{2} \Big(\xi+\sqrt{\xi^2-1} \Big)^t \\ U_t(\xi) &amp;= \frac{(\xi + \sqrt{\xi^2 - 1})^{t+1} - (\xi - \sqrt{\xi^2 - 1})^{t+1}}{2 \sqrt{\xi^2 - 1}}\,. \end{align} We’re interested in convergence rates, so we’ll look into \(t\)-th root asymptotics of the quantities.<d-footnote>With little extra effort, it would be possible to derive non-asymptotic convergence rates, although I won't pursue this analysis here.</d-footnote> Luckily, these asymptotics are the same for both polynomials<d-footnote>Although we won't use it here, this \(t\)-th root asymptotic holds for (almost) all orthogonal polynomials, not just Chebyshev polynomials. See for instance reference below</d-footnote> <d-cite key="stahl1990nth"></d-cite> and taking limits we have that \begin{equation} \lim_{t \to \infty} \sqrt[t]{|T_t(\xi)|} = \lim_{t \to \infty} \sqrt[t]{|U_t(\xi)|} = |\xi| + \sqrt{\xi^2 - 1}\,. \end{equation}</p> <h2 id="the-robust-region">The Robust Region</h2> <p>Let’s start first by considering the case in which the image of \(\sigma\) is in the \([-1, 1]\) interval. This is the most favorable case. In this case, the Chebyshev polynomials are bounded in absolute value by 1 and \(t+1\) respectively. Since the Chebsyshev polynomials are evaluated at \(\sigma(\cdot)\), this implies that \(\lvert \sigma(\lambda)\rvert \leq 1\). We’ll call the set of step-size and momentum parameters for which the previous inequality is verified the <em>robust region</em>.</p> <p>Let’s visualize this region in a map. Since \(\sigma\) is a linear function, its extremal values are reached at the edges: \begin{equation} \max_{\lambda \in [\lmin, \lmax]} |\sigma(\lambda)| = \max{|\sigma(\lmin)|, |\sigma(\lmax)|}\,. \end{equation} Using \(\lmin \leq \lmax\) and that \(\sigma(\lambda)\) is decreasing in \(\lambda\), we can simplify the condition \(\lvert \sigma(\lambda)\rvert \leq 1\) to \(\sigma(\lmin) \leq 1\) and \(\sigma(L) \geq -1\), which in terms of the step-size and momentum correspond to: \begin{equation}\label{eq:robust_region} \frac{(1 - \sqrt{\mom})^2}{\lmin} \leq \step \leq \frac{(1 + \sqrt{\mom})^2}{L} \,. \end{equation} These two conditions provide the upper and lower bound of the robust region.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_robust_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_robust_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_robust_region-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_robust_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="asymptotic-rate">Asymptotic rate</h3> <p>Let \(\sigma(\lambda) = \cos(\theta)\) for some \(\theta\), which is always possible since \(\sigma(\lambda) \in [-1, 1]\). In this regime, Chebyshev polynomials verify the identities \(T_t(\cos(\theta)) = \cos(t \theta)\) and \(U_t(\cos(\theta)) = \sin((t+1)\theta)/\sin(\theta)\) , which replacing in the definition of the residual polynomial gives \begin{equation} P_t(\sigma^{-1}(\cos(\theta))) = \mom^{t/2} \left[ {\small\frac{2\mom}{1+\mom}}\, \cos(t\theta) + {\small\frac{1 - \mom}{1 + \mom}}\,\frac{\sin((t+1)\theta)}{\sin(\theta)}\right]\,. \end{equation}</p> <p>Since the expression inside the square brackets is bounded in absolute value by \(t+2\), taking \(t\)-th root and then limits we have \(\limsup_{t \to \infty} \sqrt[t]{\lvert P_t(\sigma^{-1}(\cos(\theta)))\rvert} = \sqrt{\mom}\) for <i>any</i> \(\theta\). This gives our first asymptotic rate:</p> <p class="framed" style="text-align: center"> The asymptotic rate in the robust region is \(r_{\infty} = \sqrt{\mom}\). </p> <p>This is nothing short of magical. It would seem natural –and this will be the case in other regions– that the speed of convergence should depend on both the step-size and the momentum parameter. Yet, this result implies that it’s not the case in the robust region. In this region, the convergence <i>only</i> depends on the momentum parameter $\mom$. Amazing.<d-footnote>This insensitivity to step-size has been leveraged by Zhang et al. 2018 to develop a momentum tuner </d-footnote> <d-cite key="zhang2017yellowfin"></d-cite></p> <p>This also illustrates why we call this the <i>robust</i> region. In its interior, perturbing the step-size in a way that we stay within the region has no effect on the convergence rate. The next figure displays the asymptotic rate (darker is faster) in the robust region.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_robust_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_robust_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_robust_region-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_robust_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="the-lazy-region">The Lazy Region</h2> <p>Let’s consider now what happens outside of the robust region. In this case, the convergence will depend on the largest of \(\{\lvert\sigma(\lmin)\rvert, \lvert\sigma(L)\rvert\}\). We’ll consider first the case in which the maximum is \(\lvert\sigma(\lmin)\rvert\) and leave the other one for next section.</p> <p>This region is determined by the inequalities \(\lvert\sigma(\lmin)\rvert &gt; 1\) and \(\lvert\sigma(\lmin)\rvert \geq \lvert\sigma(L)\rvert\). Using the definition of \(\sigma\) and solving for \(\step\) gives the equivalent conditions \begin{equation} \step \leq \frac{2(1 + \mom)}{L + \lmin} \quad \text{ and }\quad \step \leq \frac{(1 - \sqrt{\mom})^2}{\lmin}\,. \end{equation} Note the second inequality is the same one as for the robust region \eqref{eq:robust_region} but with the inequality sign reversed, and so the region will be on the oposite side of that curve. We’ll call this the <i>lazy region</i>, as in increasing the momentum will take us out of it and into the robust region.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_lazy_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_lazy_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_lazy_region-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_lazy_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="asymptotic-rate-1">Asymptotic rate</h3> <p>As we saw earlier, outside of the \([-1, 1]\) interval both Chebyshev have simple \(t\)-th root asymptotics. Using this and that both kinds of Chebyshev polynomials agree in sign outside of the \([-1, 1]\) interval we can compute the asymptotic rate as \begin{align} \lim_{t \to \infty} \sqrt[t]{r_t} &amp;= \sqrt{\mom} \lim_{t \to \infty} \sqrt[t]{ {\small\frac{2\mom}{\mom+1}}\, T_t(\sigma(\lmin)) + {\small\frac{1 - \mom}{1 + \mom}}\,U_t(\sigma(\lmin))} \\ &amp;= \sqrt{\mom}\left(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1} \right) \\ \end{align} This gives the asymptotic rate for this region</p> <p class="framed" style="text-align: center"> In the lazy region the asymptotic rate is \(r_{\infty} = \sqrt{\mom}\left(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1} \right)\). </p> <p>Unlike in the robust region, this rate depends on both the step-size and the momentum parameter, which enters in the rate through the link function \(\sigma\). This can be observed in the color plot of the asymptotic rate</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_lazy_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_lazy_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_lazy_region-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_lazy_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="knifes-edge">Knife’s Edge</h2> <p>The robust and lazy region occupy most (but not all!) of the region for which momentum converges. There’s a small region that sits between the lazy and robust regions and the region where momentum diverges. We call this region the <i>Knife’s edge</i></p> <p>For parameters not in the robust or lazy region, we have that \(|\sigma(L)| &gt; 1\) and \(|\sigma(L)| &gt; |\sigma(\lmin)|\). Using the asymptotics of Chebyshev polynomials as we did in the previous section, we have that the asymptotic rate is \(\sqrt{\mom}\left(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1} \right)\). The method will only converge when this asymptotic rate is below 1. Enforcing this results in \(\step \lt 2 (1 + \mom) / L\). Combining this condition with the one of not being in the robust or lazy region gives the characterization: \begin{equation} \step \lt \frac{2 (1 + \mom)}{L} \quad \text{ and } \quad \step \geq \max\Big\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 + \sqrt{\mom})^2}{L}\Big\}\,. \end{equation}</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_knife_edge-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_knife_edge-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_knife_edge-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/sketch_knife_edge.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="asymptotic-rate-2">Asymptotic rate</h3> <p>The asymptotic rate can be computed using the same technique as in the lazy region. The resulting rate is the same as in that region but with \(\sigma(L)\) replacing \(\sigma(\lmin)\):</p> <p class="framed" style="text-align: center"> In the Knife's edge region the asymptotic rate is \(\sqrt{\mom}\left(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1} \right)\). </p> <p>Pictorially, this corresponds to</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_knife_edge-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_knife_edge-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_knife_edge-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_knife_edge.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="putting-it-all-together">Putting it All Together</h2> <p>This is the end of our journey. We’ve visited all the regions on which momentum converges.<d-footnote>There's a small convergent region with <i>negative</i> momentum parameter that we haven't visited. Although not typically used for minimization, negative momentum has found applications in smooth games <a href="https://arxiv.org/abs/1807.04740">(Gidel et al., 2020)</a>.</d-footnote> The only thing left to do is to combine all the asymptotic rates we’ve gathered along the way.</p> <p class="theorem"> The asymptotic rate \(\limsup_{t \to \infty} \sqrt[t]{r_t}\) of momentum is \begin{alignat}{2} &amp;\sqrt{\mom} &amp;&amp;\text{ if }\step \in \big[\frac{(1 - \sqrt{\mom})^2}{\lmin}, \frac{(1+\sqrt{\mom})^2}{L}\big]\\ &amp;\sqrt{\mom}(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1}) &amp;&amp;\text{ if } \step \in \big[0, \min\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 - \sqrt{\mom})^2}{\lmin}\}\big]\\ &amp;\sqrt{\mom}(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1})&amp;&amp;\text{ if } \step \in \big[\max\big\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 + \sqrt{\mom})^2}{L}\big\}, \tfrac{2 (1 + \mom) }{L} \big)\\ &amp;\geq 1 \text{ (divergence)} &amp;&amp; \text{ otherwise.} \end{alignat} </p> <p>Plotting the asymptotic rates for all regions we can see that Polyak momentum (the method with momentum $\mom = \left(\frac{\sqrt{L} - \sqrt{\lmin}}{\sqrt{L} + \sqrt{\lmin}}\right)^2$ and step-size $\step = \left(\frac{2}{\sqrt{L} + \sqrt{\lmin}}\right)^2$ which is asymptotically optimal among the momentum methods with constant coefficients) is at the intersection of the three regions.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum-1400.webp"/> <img src="/2023/assets/img/2023-05-01-hitchhikers-momentum/rate_convergence_momentum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="reproducibility">Reproducibility</h2> <p>All plots in this post were generated using the following Jupyer notebook: <a href="/2023/assets/html/2023-05-01-hitchhikers-momentum/hitchhikers-momentum.html">[HTML]</a> <a href="/2023/assets/html/2023-05-01-hitchhikers-momentum/hitchhikers-momentum.ipynb">[IPYNB]</a></p>]]></content><author><name>Fabian Pedregosa</name></author><summary type="html"><![CDATA[Polyak momentum is one of the most iconic methods in optimization. Despite it's simplicity, it features rich dynamics that depend both on the step-size and momentum parameter. In this blog post we identify the different regions of the parameter space and discuss their convergence properties using the theory of Chebyshev polynomials.]]></summary></entry><entry><title type="html">How does the inductive bias influence the generalization capability of neural networks?</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/" rel="alternate" type="text/html" title="How does the inductive bias influence the generalization capability of neural networks?"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/"><![CDATA[<p>Deep neural networks are a commonly used machine learning technique that has proven to be effective for many different use cases. However, their ability to generalize from training data is not well understood. In this blog post, we will explore the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>, which aims to shed light on the question of why neural networks are able to generalize, and how inductive biases influence their generalization capabilities.</p> <h2 id="overfitting-puzzle">Overfitting Puzzle</h2> <p>One open question in the field of machine learning is the <strong>overfitting puzzle</strong>, which describes the paradox that neural networks are often used in an overparameterized state (i.e., with more parameters than training examples), yet they are still able to generalize well to new, unseen data. This contradicts <strong>classical learning theory</strong>, which states that a model with too many parameters will simply memorize the training data and perform poorly on new data. This is based on the <a href="https://machinelearningcompass.com/model_optimization/bias_and_variance/"><strong>bias-variance tradeoff</strong></a> which is commonly illustrated in this way <d-cite key="fortmann2012understanding"></d-cite>:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The tradeoff consists of finding the optimal model complexity between two extremes: If there are too few parameters, the model may have high bias and underfit the data, resulting in poor performance on both the training and test data. On the other hand, if there are too many parameters, the model may have high variance and overfit the training data, resulting in a good performance on the training data but a poor performance on the test data.</p> <p>Therefore, it is important to carefully balance the number of parameters and the amount of data available to achieve the best possible generalization performance for a given learning task.</p> <p>Neural networks, particularly deep networks, are typically used in the overparameterized regime, where the number of parameters exceeds the number of training examples. In these cases, common generalization bounds do not apply <d-cite key="DBLP:journals/corr/abs-1801-00173"></d-cite>. According to classical learning theory, the generalization behavior of a learning system should depend on the number of training examples (n), and the complexity of the model should be balanced with its fit to the data <d-cite key="DBLP:journals/corr/abs-1801-00173"></d-cite>. Otherwise, the algorithm would overfit. However, neural networks have shown that this is not always the case, as they can perform well even in cases of extreme overparameterization (e.g., a 5-layer CNN with 80 million parameters <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>). This is a very interesting finding as it shows that the classical learning theory may not hold true for neural networks.</p> <p>To better understand this phenomenon, Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> examined the role of <strong>inductive bias</strong> in neural networks and its influence on the generalization capability of these networks. Inductive bias, or learning bias, refers to the assumptions a network makes about the nature of the target function and is determined by the network’s architecture. Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted experiments with different types of fully connected networks (FCN) and convolutional neural networks (CNN) to investigate which biases are effective for these network architectures.</p> <h2 id="experiments">Experiments</h2> <p>In the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>, the authors use <strong>empirical studies</strong> to better understand the <em>overfitting puzzle</em> and how inductive bias affects the behavior of overparameterized neural networks. The authors specifically aim to investigate the role of inductive bias under <strong>different architectural choices</strong> by comparing fully connected and convolutional neural networks.</p> <p>The task used in the study is to learn an identity map through a single data point, which is an artificial setup that demonstrates the most extreme case of overparameterization. The goal of the study is to determine whether a network tends towards memorization (learning a constant function) or generalization (learning the identity function).</p> <p>To enable the <strong>identity task</strong> <d-cite key="DBLP:conf/eccv/HeZRS16"></d-cite> for linear models, the authors ensure that hidden dimensions are not smaller than the input and set the weights to the identity matrix in every layer. For convolutional layers, only the center of the kernel is used, and all other values are set to zero, simulating a 1 x 1 convolution which acts as a local identity function. For deeper models that use the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> activation function, it is necessary to encode and recover negative values, as they are discarded by the ReLU function. This can be achieved by using hidden dimensions that are twice the size of the input and storing negative and positive values separately.</p> <p>All networks are trained using standard gradient descent to minimize the mean squared error.</p> <p>The study uses the <strong><a href="https://paperswithcode.com/dataset/mnist">MNIST dataset</a></strong> and tests the networks on various types of data, including a linear combination of two digits, random digits from the MNIST test set, random images from the Fashion MNIST dataset, and algorithmically generated image patterns.</p> <p>So let us look at some of the results:</p> <div class="l-page"> <iframe src="/2023/assets/html/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_3.html" frameborder="0" scrolling="no" width="100%" height="450px"></iframe> </div> <p>The first column of the figure above shows the single data point that was used to train the network on, and all following columns show the test data with its specific results. The rows represent the different implementations of the respective networks (FCN, CNN).</p> <h3 id="fully-connected-networks-fcn">Fully connected networks (FCN)</h3> <p>For fully connected networks, the outputs differ depending on the depth of the network and the type of testing data. Shallower networks seem to incorporate random white noise into the output, while deeper networks tend to learn the constant function. The similarity of the test data to the training example also affects the behavior of the model. When the test data is from the MNIST digit sets, all network architectures perform quite well. However, for test data that is more dissimilar to the training data, the output tends to include more random white noise. The authors prove this finding with a <em>theorem</em> for 1-layer FCNs. The formula shows the prediction results for a test data point $x$:</p> \[f(x) = \Pi_{\parallel}(x) + R \Pi_{\perp}(x)\] <p>The test data point is decomposed into components that are parallel $\Pi_{\parallel}$ and perpendicular $\Pi_{\perp}$ to the training example. $R$ is a random matrix, independent of the training data. If the test data is highly correlated to the training data, the prediction resembles the training output. If the test data is dissimilar to the training data, $\Pi_{\perp}(x)$ dominates $\Pi_{\parallel}(x)$, the output is randomly projected by $R$ and persists of white noise.</p> <p>This behavior can be confirmed by visualizing the results of the 1-layer FCN:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The inductive bias does not lead to either good generalization or memorization. Instead, the predictions become more random as the test data becomes less similar to the training data.</p> <p>Deeper networks tend to learn the constant function, resulting in a strong inductive bias towards the training output regardless of the specific input. This behavior is similar to that of a deep ReLU network, as shown in the figure comparing deep FCN and deep ReLU networks.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conclude that more complex network architectures are more prone to memorization. This finding aligns with statistical learning theory, as a more complex architecture has more parameters and, therefore, more overparameterization.</p> <h3 id="convolutional-neural-networks-cnn">Convolutional neural networks (CNN)</h3> <p>For convolutional neural networks, the inductive bias was analyzed using the ReLU activation function and testing networks with different depths. The hidden layers of the CNN consist of 5 × 5 convolution filters organized into 128 channels. The networks have two constraints to match the structure of the identity target function.</p> <p>If you choose the button ‘CNN’ in the first figure, it shows the resulting visualizations. It can be seen that shallow networks are able to learn the identity function, while intermediate-depth networks function as edge detectors, and deep networks learn the constant function. Whether the model learns the identity or the constant function, both outcomes reflect inductive biases since no specific structure was given by the task.</p> <p>A better understanding of the evolution of the output can be obtained by examining the status of the prediction in the hidden layers of the CNN. Since CNNs, unlike FCNs, preserve the spatial relations between neurons in the intermediate layers, these layers can be visualized. The figure below shows the results for a randomly initialized 20-layer CNN compared to different depths of trained CNNs.”</p> <div class="l-page"> <iframe src="/2023/assets/html/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/CNNs_intermedLayers.html" frameborder="0" scrolling="no" width="100%" height="450px"></iframe> </div> <p>Random convolution gradually smooths out the input data, and after around eight layers, the shapes are lost. When the networks are trained, the results differ. The 7-layer CNN performs well and ends up with an identity function of the input images, while the results of the 14-layer CNN are more blurry. For the 20-layer trained CNN, it initially behaves similarly to the randomly initialized CNN by wiping out the input data, but it preserves the shapes for a longer period. In the last three layers, it renders the constant function of the training data and outputs 7 for any input.</p> <p>These results align with the findings of Radhakrishnan et al. [2018] <d-cite key="radhakrishnan2019memorization"></d-cite> in ‘Memorization in overparametrized autoencoders’, which used a similar empirical framework on fully-connected autoencoders. They found that deep neural networks learn locally contractive maps around the training examples, leading to learning the constant function.</p> <p>As for FCNs, the experiments show that the similarity of the test data to the training data point increases task success. Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted further experiments with different <strong>feature channel numbers and dimensions</strong>. They found that increasing the hidden dimensions/adding channels is much less prone to overfitting than adding depth. This should be considered when designing new models: if the goal is to increase the number of parameters of an existing model (perhaps to improve optimization dynamics or prepare for more training data), it is better to try increasing the hidden dimension before tuning the depth, unless the nature of the data changes.</p> <p>Another factor that influences inductive bias is **model initialization++. For networks with few channels, the difference between random initialization and the converged network is extreme <d-cite key="DBLP:conf/iclr/FrankleC19"></d-cite>. This can be explained as follows: in the regime of random initialization with only a few channels, the initialization does not have enough flexibility to compensate for incorrect choices. As a result, the networks are more likely to converge to non-optimal extrema. Having more channels helps to smooth out this problem, as more parameters can compensate for ‘unlucky’ cases.</p> <h2 id="general-findings">General findings</h2> <p>The first figure in this post shows that CNNs have better generalization capability than FCNs. However, it is important to note that the experiments primarily aim to compare different neural networks <strong>within their architecture type</strong>, so a comparison between FCNs and CNNs cannot be considered fair. CNNs have natural advantages due to sparser networks and structural biases, such as local receptive fields and parameter sharing, that are consistent with the identity task. Additionally, CNNs have more parameters, as seen in the underlying figure: a 6-layer FCN contains 3.6M parameters, while a 5-layer CNN (with 5x5 filters of 1024 channels) has 78M parameters. These differences should be taken into account when evaluating the results of the experiments.</p> <div class="l-page" style="width: 704px; margin: auto;"> <iframe src="/2023/assets/html/2023-05-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/plot.html" frameborder="0" scrolling="no" width="100%" height="480px"></iframe> </div> <p>To conclude, CNNs generalize better than FCNs, even though they have more parameters. This is consistent with the observed phenomenon that neural networks do not follow the statistical learning theory.</p> <p>The experiments described above lead to the following main findings of the paper:</p> <ul> <li>The number of parameters does not strongly correlate with generalization performance, but the structural bias of the model does.</li> </ul> <p>For example, when equally overparameterized,</p> <ul> <li>training a very deep model is prone to memorization, while</li> <li>adding more feature channels/dimensions is much less likely to cause overfitting.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>After reading this blog post, we hope that the concept of the overfitting puzzle is understood and it is revealed how the generalization capability of neural networks contrasts with classical learning theory. We also made the significance of the study conducted by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> clear, as they provide more insights into the inductive bias. The artificial setup used in the study is a smart way to approach this topic and allows for an intuitive interpretation of the results. The authors found that CNNs tend to <em>generalize</em> by actually learning the concept of identity, while FCNs are prone to memorization. Within these networks, it can be said that the simpler the network architecture is, the better the task results. Another observation is that deep CNNs exhibit extreme memorization. It would have been interesting to analyze the inductive bias for other types of data (e.g., sequence data like speech) and compare whether the stated theorems also hold in those cases.</p> <p>In summary, Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted interesting studies that have helped the machine learning community to gain a deeper understanding of inductive bias. Their results provide concrete guidance for practitioners that can help design models for new tasks.</p>]]></content><author><name>Charlotte Barth</name></author><summary type="html"><![CDATA[["The blog post discusses how memorization and generalization are affected by extreme overparameterization. Therefore", "it explains the overfitting puzzle in machine learning and how the inductive bias can help to understand the generalization capability of neural networks."]]]></summary></entry><entry><title type="html">How much meta-learning is in image-to-image translation?</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/how-much-meta-learning-is-in-image-to-image-translation/" rel="alternate" type="text/html" title="How much meta-learning is in image-to-image translation?"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/how-much-meta-learning-is-in-image-to-image-translation</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/how-much-meta-learning-is-in-image-to-image-translation/"><![CDATA[<p>At the last ICLR conference, Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> presented work showing that CNNs do not transfer information between classes of a classification task.</p> <ul> <li>Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn [ICLR, 2022] Do Deep Networks Transfer Invariances Across Classes?<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite></li> </ul> <p>Here is a quick summary of their findings: If we train a Convolutional Neural Net (CNN) to classify animals on a set of randomly brightened and darkened images of cats and dogs, it will learn to ignore the scene’s brightness. We say that the CNN learned that classification is <strong>invariant</strong> to the <strong>nuisance transformation</strong> of randomly changing the brightness of an image. We now add a set of leopards to the training data, but fewer examples of them (they are hard to photograph) than we have cats and dogs. However, we keep using the same random transformations. The training set thus becomes <strong>class-imbalanced</strong>.</p> <p>We might expect a sophisticated learner to look at the entire dataset, recognize the random brightness modifications across all species of animal and henceforth ignore brightness when making predictions. If this applied to our experiment, the CNN would be similarly good at ignoring lighting variations on all animals. Furthermore, we would expect the CNN to become more competent at ignoring lighting variations in proportion to <strong>the total amount of images</strong>, irrespective of which animal they depict.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> show that a CNN does not behave like this: When using a CNN on a <strong>class-imbalanced</strong> classification task with random nuisance transformations, the CNNs invariance to the transformation is proportional to the size of the training set <strong>for each class</strong>. This finding suggests CNNs don’t <strong>transfer invariance</strong> between classes when learning such a classification task.</p> <p>However, there is a solution: Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use an Image-to-Image translation architecture called MUNIT<d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> to learn the transformations and generate additional data from which the CNN can learn the invariance separately for each class. Thus, the invariance to nuisance transformations is transferred <strong>generatively</strong>. They call this method <strong>Generative Invariance Transfer (GIT)</strong>.</p> <p><strong>So why is this an interesting result?</strong></p> <p>In the field of machine learning many have dreamed for a long time<d-cite key="schmidhuber:1987:srl"></d-cite><d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite> of a learner that, having learned a number of tasks can adapt to new tasks with little to no extra training - a learner that has learned to learn, a meta-learner. Yet, specialized meta-learners <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite><d-cite key="NIPS2017_cb8da676"></d-cite><d-cite key="NIPS2016_90e13578"></d-cite><d-cite key="sung2018learning"></d-cite> struggled to outperform baseline methods<d-cite key="DBLP:journals/corr/abs-2104-02638"></d-cite><d-cite key="DBLP:journals/corr/abs-1904-04232"></d-cite>, arguably due to high computational requirements<d-cite key="nichol2018first"></d-cite> and few large scale datasets<d-cite key="triantafillou2019meta"></d-cite>. We believe this to be caused by a too-narrow conception of what constitutes meta-learning. We argue that:</p> <ul> <li>In contradiction to recent definitions of meta-learning, the experiment described above is a meta-learning experiment.</li> <li>MUNIT is related to contemporary meta-learning methods and a meta-learner.</li> <li>These two findings point to a too-narrow conception of meta-learning in the recent literature. A wider conception based on mutual information could lead to interesting future work.</li> </ul> <p>Before we proceed to the main post, let’s clarify some definitions. If you are already familiar with the subject, you may skip this part. If you have only a vague notion of contemporary meta-learning you will be able to follow the article anyway. However, if you want to know more, <a href="https://interactive-maml.github.io/">here</a> is a gentle introduction to MAML, one of the most popular methods.</p> <details> <summary><b> Definition: Class-Imbalanced Classification</b></summary> <br/> <p> In many real-world classification datasets, the number of examples for each class varies. <b>Class-imbalanced classification</b> refers to classification on datasets where the frequencies of class labels vary significantly. </p> <p> It is generally more difficult for a neural network to learn to classify classes with fewer examples <d-cite key="5128907"></d-cite><d-cite key="10.1117/12.2228523"></d-cite>. However, it is often important to perform well on all classes, regardless of their frequency in the dataset. If we train a model to classify a dataset of different skin tumors, most examples may be benign. Still, it is crucial to identify the rare, malignant ones. Experiment design, including training and evaluation methods must therefore be adjusted when using class-imbalanced data. (see Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 3.1) </p> <br/> </details> <details> <summary><b> Definition: Nuisance Transformation &amp; Transformation Invariance</b></summary> <br/> <p> Transformations are alterations of data. In the context of image classification, <b>nuisance transformations</b> are alterations that do not affect the class labels of the data. A model is said to be invariant to a <b>nuisance transformation</b> if it can successfully ignore the transformation when predicting a class label. </p> We can formally define a <b>nuisance transformation</b> <p> $$T(\cdot |x)$$ </p> <p> as a distribution over transformation functions. An example of a <b>nuisance transformation</b> might be a distribution over rotation matrices of different angles, or lighting transformations with different exposure values. By definition, <b>nuisance transformations</b> have no impact on class labels $y$, only on data $x$. A perfectly <b>transformation-invariant</b> classifier would thus completely ignore them, i.e., </p> <p> $$ \hat{P}_w(y = j|x) = \hat{P}_w(y = j|x'), \; x' \sim T(\cdot |x). $$ </p> <p> (see Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 3.1) </p> </details> <h2 id="a-closer-look-at-the-experiment">A closer look at the experiment</h2> <p>Let’s take a more detailed look at the experiment Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> conducted:</p> <p>Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> take a dataset, e.g., CIFAR-100, then apply a nuisance transformation, for example, random rotation, background intensity, or dilation and erosion. They then remove samples from some classes until the distribution of class sizes follows <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf’s law</a> with parameter 2.0 and a minimum class size of 5. The test set remains balanced, i.e., all test classes have the same number of samples. They then train a CNN model - for example, a ResNet - on this imbalanced and transformed training data.</p> <p>To measure the invariance of the trained model to the applied transformation Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use the empirical <a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> between the predictions on the untransformed test set and the transformed test set of each class.</p> <p>If the learner is invariant to the transformation, the predicted probability distribution over class labels should be identical for the transformed and untransformed images. In that case, the KLD should be zero and greater than zero otherwise. The higher the expected KL-divergence, the more the applied transformation impacts the network’s predictions.</p> <p>The result: eKLD falls with class size. This implies that the CNN does not learn that there are the same nuisance transformations on all images and therefore does not transfer this knowledge to the classes with less training data. A CNN learns invariance <strong>separately for each class</strong> (see also Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 3.2).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="how-is-this-a-meta-learning-experiment">How is this a meta-learning experiment?</h2> <p>You might think this is a cool experiment, but how is it related to meta-learning?</p> <p>And, indeed, in contemporary literature meta-learning is often conceived of as learning multiple tasks. In an much-cited 2022 survey, Hosepdales et al. write:</p> <blockquote> <p>Meta-learning is most commonly understood as learning to learn; the process of improving a learning algorithm over multiple learning episodes. In contrast, conventional ML improves model predictions over multiple data instances. <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite></p> </blockquote> <p>In another popular survey Vanschoren [2018] describes the meta-learning process as follows:</p> <blockquote> <p>First, we need to collect meta-data that describe prior learning tasks and previously learned models. They comprise the exact algorithm configurations used to train the models, including hyperparameter settings, pipeline compositions and/or network architectures, the resulting model evaluations, such as accuracy and training time, the learned model parameters, such as the trained weights of a neural net, as well as measurable properties of the task itself, also known as meta-features.<d-cite key="vanschoren2018meta"></d-cite></p> </blockquote> <p>Francheschi et al. [2018] basically equate meta-learning (ML) with <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyperparameter optimization</a> (HO):</p> <blockquote> <p>[…] both HO and ML essentially boil down to nesting two search problems: at the inner level we seek a good hypothesis (as in standard supervised learning) while at the outer level we seek a good configuration (including a good hypothesis space) where the inner search takes place.<d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite></p> </blockquote> <p>This perspective on meta-learning seems to indicate that “true” meta-learning requires a rigid structure of multiple discrete tasks that is optimized over. However, in the invariance transfer setting we neither have multiple learning episodes, i.e., we learn over multiple data instances, nor any “meta-features”. Also, adding a class to the dataset does not exactly constitute a new “task”, even though knowledge of the nuisance transform is applicable.</p> <p>So is Zhou et al.’s<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiment no meta-learning after all?</p> <p>Let’s look at one of the original papers on meta-learning. In the 1998 book “Learning to learn” Sebastian Thrun &amp; Lorien Pratt define an algorithm as capable of “Learning to learn” if it improves its performance in proportion to the number of tasks it is exposed to:</p> <blockquote> <p>an algorithm is said to learn to learn if its performance at each task improves with experience and with the number of tasks. Put differently, a learning algorithm whose performance does not depend on the number of learning tasks, which hence would not benefit from the presence of other learning tasks, is not said to learn to learn <d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite></p> </blockquote> <p>Now this seems a much looser definition. How might this apply to the experiment just outlined? In the introduction, we thought about how a sophisticated learner might handle a dataset like the one described in the last section. We said that a sophisticated learner would learn that the nuisance transformations are applied uniformly <strong>to all classes</strong>. Therefore, if we added more classes to the dataset, the learner would become <strong>more invariant</strong> to the transformations because we expose it to more examples of them. Since this is part of the classification task <strong>for each class</strong>, the learner should, everything else being equal, become better at classification, especially on classes with few training examples. To see this, we must think of the multi-classification task not as a single task but as multiple mappings from image features to activations that must be learned, as a set of binary classification tasks. Thrun and Pratt continue:</p> <blockquote> <p>For an algorithm to fit this definition, some kind of <em>transfer</em> must occur between multiple tasks that must have a positive impact on expected task-performance <d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite>.</p> </blockquote> <p>This transfer is what Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> tried to measure. There is some meta-information learnable across several tasks, in our case, the transformation distribution across many binary classification tasks. If a learner can learn this meta-information and transfer it to each new task it has “learned to learn”; it is a meta-learner. The goal of Zhou et al.’s<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiment was to see whether this transfer takes place. Thus, arguably, it is a meta-learning experiment.</p> <h2 id="generative-invariance-transfer">Generative Invariance Transfer</h2> <p>Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> don’t stop there. They show that using the MUNIT (Multimodal Unsupervised image-to-image Translation)<d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> architecture, they can learn the nuisance transformations applied to the dataset and generate additional training samples for the classes with few samples, improving transformation invariance there. They call this Generative invariance transfer (GIT). Let’s take a closer look:</p> <p>MUNIT networks are capable of performing image-to-image translation, which means that they can translate an image from one domain, such as pictures of leopards, into another domain, such as pictures of house cats. The translated image should look like a real house cat while still resembling the original leopard image. For instance, if the leopard in the original image has its eyes closed, the translated image should contain a house cat with closed eyes. Eye state is a feature present in both domains, so a good translator should not alter it. On the other hand, a leopard’s fur is yellow and spotted, while a house cat’s fur can be white, black, grey, or brown. To make the translated images indistinguishable from real house cats, the translator must thus replace leopard fur with house cat fur.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>MUNIT networks learn to perform translations by correctly distinguishing the domain-agnostic features (such as eye state) from the domain-specific features (such as the distribution of fur color). They embed an image into two latent spaces: a content space that encodes the domain-agnostic features and a style space that encodes the domain-specific features (see figure above).</p> <p>To transform a leopard into a house cat, we can encode the leopard into a content and a style code, discard the leopard-specific style code, randomly select a cat-specific style code, and assemble a house cat image that looks similar by combining the leopard’s content code with the randomly chosen cat style code (see figure below).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> modify the process of using MUNIT to transfer images between domains. They do not use MUNIT to translate images <strong>between</strong> domains but <strong>within</strong> a domain. The MUNIT network exchanges the style code of an image with another style code of the same domain. For example, if the domain is house cats, the MUNIT network might translate a grey house cat into a black one. The learning task in this single-domain application of MUNIT is to decompose example-agnostic content features from example-specific style features so that the translated images still look like house cats. For example, fur color is a valid style feature for translating within the ‘house cat’ domain because every house cat has a fur color. A translator only switching fur color is hard to detect.</p> <p>However, if the domain included house cats <strong>and apples</strong>, fur color is not a valid style feature. If it was, the translator might translate fur color on an apple and give it black fur, which would look suspiciously out of place. Whatever house cats and apples have in common - maybe their position or size in the frame - would be a valid style feature. We would expect an intra-domain translator on an apples-and-cats dataset to change the position and size of an apple but not to turn it into a cat (not even partially).</p> <p>It turns out that on a dataset with uniformly applied nuisance transformations, the nuisance transformations are valid style features: The result of randomly rotating an apple cannot be discerned as artificial when images of all classes, house cats and apples, were previously randomly rotated.</p> <p>Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> find that when they train a MUNIT network on a dataset with nuisance transformations and class imbalances, the MUNIT network decomposes the class and transformation distributions. The style latent space of the MUNIT network approximates the transformation distribution $T(\cdot |x)$. The content space preserves the remaining features of the image, such as its class. Thus, when translating an image, i.e., exchanging its style code, MUNIT applies a random nuisance transformation while preserving content. Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use this method to generate data for classes with few examples. While the CNN is still unable to transfer invariance to $T(\cdot |x)$ between classes, it can now learn it for each class separately using the data generated by MUNIT, which has acquired knowledge of $T(\cdot |x)$ from the entire dataset (see also Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 4).</p> <p>So MUNIT decomposes the example-specific information, e.g., whether something is an apple or a house cat, from the meta-information, i.e., nuisance transformations applied to the entire dataset. When we add more classes, it has more data and can better learn the transformation distribution $T(\cdot |x)$. Does solving a meta-learning problem make MUNIT a meta-learner? Let’s look at the relationship MUNIT has with contemporary meta-learners</p> <h2 id="how-much-meta-learning-is-in-munit">How much meta-learning is in MUNIT?</h2> <p>To see how well MUNIT fits the definition of meta-learning, let’s see what the same survey papers we consulted earlier consider the structure of a meta-learning algorithm.</p> <h3 id="part-1-the-task-centered-view">Part 1: The task-centered view</h3> <p>Hospedales et al. [2021] <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite> defines a generic meta-learner as follows: An outer training loop with a set of trainable parameters iterates over tasks in a distribution of tasks. Formally a task is comprised of a dataset and a loss function $ \mathcal{T} = \{ \mathcal{D}, \mathcal{L} \} $. In an inner loop, a learning algorithm based on the outer loop’s parameters is instantiated for each task. We train it on a training set (<em>meta-training</em>) and test it on a validation set (<em>meta-validation</em>). We then use loss on this validation set to update the outer loop’s parameters. In this task-centered view of meta-learning, we can express the objective function as</p> <p> $$ \underset{\omega}{\mathrm{min}} \; \mathbb{E}_{\mathcal{T} \sim p(\mathcal{T})} \; \mathcal{L}(\mathcal{D}, \omega), $$ </p> <p>where $ \omega $ is parameters trained exclusively on the meta-level, i.e., the <em>meta-knowledge</em> learnable from the task distribution <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite>.</p> <p>This <em>meta-knowledge</em> is what the meta-learner accumulates and transfers across the tasks. Collecting meta-knowledge allows the meta-learner to improve its expected task performance with the number of tasks. The meta-knowledge in the experiment of Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> is the invariance to the nuisance transformations as the transformations are identical and need to be ignored for images of all classes. By creating additional transformed samples, the MUNIT network makes the meta-knowledge learnable for the CNN.</p> <p>The task-centered view of meta-learning brings us to a related issue: A meta-learner must discern and decompose task-specific knowledge from meta-knowledge. Contemporary meta-learners decompose meta-knowledge through the different objectives of their inner and outer loops and their respective loss terms. They store meta-knowledge in the outer loop’s parameter set $ \omega $ but must not learn task-specific information there. Any unlearned meta-features lead to slower adaptation, negatively impacting performance, <em>meta-underfitting</em>. On the other hand, any learned task-specific features will not generalize to unseen tasks in the distribution, thus also negatively impacting performance, <em>meta-overfitting</em>.</p> <p>We recall that, similarly, MUNIT <d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> decomposes domain-specific style information and domain-agnostic content information. Applied to two domains, leopards and house cats, a MUNIT network will encode the domain-agnostic information, e.g., posture, scale, background, in its content latent space, and the domain-specific information, e.g., how a cat’s hair looks, in its style latent space. If the MUNIT network encoded the domain-agnostic information in the style latent space, the resulting image would not appear to be a good translation since the style information is discarded and replaced. It might turn a closed-eyed leopard into a staring cat. If the MUNIT network encoded the domain-specific transformation in the content latent space, the network would have difficulty translating between domains. A house cat might still have its original leopard fur.</p> <p>Although the single-domain application of MUNIT explicitly learns a single task and scales “over multiple data instances” instead of “multiple learning episodes”<d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite> it is clearly compatible with the task-centered view of meta-learning set forth <em>in the same survey paper</em>. Both meta-learning and multi-domain unsupervised image-to-image translation are thus learning problems that require a separation of the general from the specific.</p> <p>As we shall see, this is even visible when comparing their formalizations as optimization problems.</p> <h3 id="part-2-the-bi-level-programming-view">Part 2: The bi-level programming view</h3> <p>Francheschi et al. [2018] <d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite> show that all contemporary neural-network-based meta-learning approaches can be expressed as bi-level optimization problems. Formally the optimization objective of a general meta-learner can be expressed as:</p> <p> $$ \bbox[5pt, border: 2px solid blue]{ \begin{align*} \omega^{*} = \underset{\omega}{\mathrm{argmin}} \sum_{i=1}^{M} \mathcal{L}^{meta}(\theta^{* \; (i)}(\omega), D^{val}_i), \end{align*} } $$ </p> <p>where $M$ describes the number of tasks in a batch, $\mathcal{L}^{meta}$ is the meta-loss function, and $ D^{val}_i $ is the validation set of the task $ i $. $\omega$ represents the parameters exclusively updated in the outer loop. $ \theta^{* \; (i)} $ represents an inner loop learning a task that we can formally express as a sub-objective constraining the primary objective</p> <p> $$ \bbox[5pt, border: 2px solid red]{ \begin{align*} s.t. \; \theta^{* \; (i)} = \underset{\theta}{\mathrm{argmin}} \; \mathcal{L^{task}}(\theta, \omega, D^{tr}_i), \end{align*} } $$ </p> <p>where $ \theta $ are the model parameters updated in the inner loop, $ \mathcal{L}^{task} $ is the loss function by which they are updated and $ D^{tr}_i $ is the training set of the task $ i $ <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite>.</p> <p>While not adhering to Francheschi et al.’s [2018] notion of a meta-learner as “nesting two search problems”, it turns out that the loss functions of MUNIT can be similarly decomposed:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>MUNIT’s loss function consists of two adversarial (GAN) <d-cite key="DBLP:conf/nips/GoodfellowPMXWOCB14"></d-cite> loss terms (see figure above) with several auxiliary reconstruction loss terms. To keep the notation simple, we combine all reconstruction terms into a joined reconstruction loss $ \mathcal{L}_{recon}(\theta_c, \theta_s) $, where $ \theta_c $ are the parameters of the <em>content</em> encoding/decoding networks and $ \theta_s $ are the parameters of the <em>style</em> encoding/decoding networks. We will only look at one of the two GAN losses in detail since they are symmetric, and one is discarded entirely when MUNIT is used on a single domain in the fashion of Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite>.</p> <p>MUNIT’s GAN loss term is</p> <p> $$ \begin{align*} &amp;\mathcal{L}^{x_{2}}_{GAN}(\theta_d, \theta_c, \theta_s) \\\\ =&amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp; \;\mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], \end{align*} $$ </p> <p>where the $ \theta_d $ represents the parameters of the discriminator network, $p(x_2)$ is the data of the second domain, $ c_1 $ is the content embedding of an image from the first domain to be translated. $ s_2 $ is a random style code of the second domain. $ D_2 $ is the discriminator of the second domain, and $ G_2 $ is its generator. MUNIT’s full objective function is:</p> <p> $$ \begin{align*} \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \underset{\theta_d}{\mathrm{argmax}}&amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp; \; \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], + \; \mathcal{L}^{x_{1}}_{GAN}(\theta_d, \theta_c, \theta_s) \\ +&amp; \;\mathcal{L}_{recon}(\theta_c, \theta_s) \end{align*} $$ </p> <p>(compare <d-cite key="DBLP:conf/eccv/HuangLBK18, DBLP:conf/nips/GoodfellowPMXWOCB14"></d-cite>). We can reformulate this into a bi-level optimization problem by extracting a minimization problem describing the update of the generative networks. We also drop the second GAN loss term as it is not relevant to our analysis.</p> <p> $$ \bbox[5px, border: 2px solid blue]{ \begin{align*} \omega^{*} &amp; = \{ \theta_c^*, \theta_s^* \} \\\\ &amp; = \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d^{*})) \right] \\ &amp; + \mathcal{L}_{recon}(\theta_c, \theta_s), \end{align*} } $$ </p> <p>We then add a single constraint, a subsidiary maximization problem for the discriminator function:</p> <p> $$ \bbox[5px, border: 2px solid red]{ \begin{align*} &amp;s.t. \;\theta_d^{*} \\\\ &amp; = \underset{\theta_d}{\mathrm{argmax}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ &amp; + \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right] \end{align*} } $$ </p> <p>Interestingly, this bi-level view does not only resemble a meta-learning procedure as expressed above, but the bi-level optimization also facilitates a similar effect. Maximizing the discriminator’s performance in the constraint punishes style information encoded as content information. If style information is encoded as content information, the discriminator detects artifacts of the original domain in the translated image. Similarly, a meta-learner prevents <em>meta-overfitting</em> via an outer optimization loop.</p> <p><em>However, MUNIT, while representable as a bi-level optimization problem does not “essentially boil down to nesting two search problems”.<d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite></em> During GAN training, the discriminator’s parameters are updated through the changes in the generator’s parameters, which derive from the discriminator’s parameters, and so forth; The training of the discriminator and generator are dependent processes. Crucially, they depend on each other symmetrically, forming a min-max game. Contemporary meta-learners, meanwhile, are strictly hierarchical, with an outer and inner optimization loop.</p> <h3 id="now-does-munit-meta-learn">Now, does MUNIT meta-learn?</h3> <p>So it appears that while not conforming to any verbal definition of a contemporary meta-learner MUNIT seems to:</p> <p>a) adhere to multiple formalizations made in the very same publications to define meta-learning</p> <p>b) solve a meta-learning problem via GIT when applied to a single domain (if you agree with the conclusion of the previous chapter)</p> <p>We thus conclude:</p> <p>When applied to a single domain MUNIT <em>does</em> meta-learn as it combines information from all classes to extract the transformation distribution. While it does not perform classification explicitly, the class information of an image is encoded in MUNIT’s content space. Since MUNIT is trained in an unsupervised way, it is probably closer to a distance metric than an actual class label. We might thus classify single-domain MUNIT as an unsupervised, generative meta-learner.</p> <h2 id="implications">Implications</h2> <p>That invariance transfer and GIT are meta-learning and that MUNIT is a meta-learner is important. Granted, it is not especially hard to see that invariance transfer is a form of “learning to learn” or that Image-to-Image translation is essentially a mechanism to decompose class-specific form general features.</p> <p>However, because contemporary meta-learning has been narrowly cast as “improving a learning algorithm over multiple learning episodes”<d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite> and “nesting two search problems”<d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite> it is hard to recognize GIT as meta-learning.</p> <p>In these authors opinion this is not GIT’s fault, but a sign that meta-learning has recently been conceived of too narrowly. Zhou et al.’s<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiment is a beautiful illustration of this showing that something as general as a GAN loss term, with appropriate modifications, can be used to meta-learn.</p> <p>A too-narrow conception goes further than obscuring some experiment’s significance though: Meta-learning as a field has recently struggled to compete with less specialized architectures<d-cite key="DBLP:journals/corr/abs-2104-02638"></d-cite><d-cite key="DBLP:journals/corr/abs-1904-04232"></d-cite>. Multi-task datasets are hard to scale <d-cite key="triantafillou2019meta"></d-cite>, as are episode rollouts <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite>. Meanwhile, large-scale architectures have shown impressive zero-shot capabilities<d-cite key="dosovitskiy2021an"></d-cite><d-cite key="pmlr-v139-radford21a"></d-cite>.</p> <p>Zhou et al.’s<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> contributions are therefore important as a challenge to the status quo in meta-learning. MUNIT seems to meta-learn by embedding class (and class-specific features) in one space and transformation-specific features (e.g., how bright/dark) in another. This seems to point to a conception of meta-learning as finding mutual information between sets of examples (not necessarily defined by class or transformation feature but by arbitrary concepts) or hierarchies of such sets. Examining and designing mechanisms by which such behavior can be evoked is an exciting direction for future work.</p> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li> <p>Zhou et al.’s<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiments show that the meta-learning setting can be formulated more broadly than learning an explicit task distribution, suggesting that specialized datasets are not necessary.</p> </li> <li> <p>Using GIT, Zhou et al.<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> show that meta-learning algorithms can come in shapes other than inner and outer training loops. Analysis suggests that countervailing loss terms facilitate the decomposition of meta-features from task-specific features.</p> </li> <li> <p>Our discussion of Zhou et al.’s<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiments suggests, that when thinking about meta-learning, thinking about mutual information between batches of examples (not necessarily aligned with class labels) and how to extract it trumps thinking about distinct tasks.</p> </li> </ol>]]></content><author><name>Maximilian Eißler</name></author><summary type="html"><![CDATA[...in which we find a connection between meta-learning literature and a paper studying how well CNNs deal with nuisance transforms in a class-imbalanced setting. Closer inspection reveals a surprising amount of similarity - from meta-information to loss functions. This implies that the current conception of meta-learning might be too narrow.]]></summary></entry><entry><title type="html">Thinking Like Transformers</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/raspy/" rel="alternate" type="text/html" title="Thinking Like Transformers"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/raspy</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/raspy/"><![CDATA[<h1 id="thinking-like-transformers">Thinking Like Transformers</h1> <ul> <li><a href="https://arxiv.org/pdf/2106.06981.pdf">Paper</a><d-cite key="weiss2021thinking"></d-cite> by Gail Weiss, Yoav Goldberg, Eran Yahav</li> </ul> <p>Transformer models are foundational to AI systems. There are now countless explanations of “how transformers work?” in the sense of the architecture diagram at the heart of transformers.</p> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_5_0.svg" alt="svg"/></p> <p>However this diagram does not provide any intuition into the computational model of this framework. As researchers become interested in how Transformers work, gaining intuition into their mechanisms becomes increasingly useful.</p> <p><a href="https://arxiv.org/pdf/2106.06981.pdf">Thinking like Transformers</a> proposes a computational framework for Transformer-like calculations. The framework uses discrete computation to simulate Transformer computations. The resulting language <a href="https://github.com/tech-srl/RASP">RASP</a> is a programming language where, ideally, every program can compile down to a specific Transformer (indeed, David Lindner and colleagues have recently released a <a href="https://arxiv.org/abs/2301.05062">compiler</a> for a large subset of RASP!).</p> <p>In this blog post, I reimplemented a variant of RASP in Python (RASPy). The language is roughly compatible with the original version, but with some syntactic changes that I thought were fun. With this language, we have a challenging set of puzzles to walk through and understand how it works.</p> <p>Before jumping into the language itself, let’s look at an example of what coding with Transformers looks like. Here is some code that computes the <code class="language-plaintext highlighter-rouge">flip</code>, i.e. reversing an input sequence. The code itself uses two Transformer layers to apply attention and mathematical computations to achieve the result.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">flip</span><span class="p">():</span>
    <span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">flip</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="n">indices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">flip</span>
<span class="nf">flip</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_11_0.svg" alt="svg"/></p> <h2 id="transformers-as-code">Transformers as Code</h2> <p>Our goal is to define a computational formalism that mimics the expressivity of Transformers. We will go through this process by analogy, describing each language construct next to the aspect of the Transformer it represents. (See the full <a href="https://arxiv.org/pdf/2106.06981.pdf">paper</a> for the formal language specification).</p> <p>The core unit of the language is a <em>sequence operation</em> that transforms a sequence to another sequence of the same length. I will refer to these throughout as <em>transforms</em>.</p> <h3 id="inputs">Inputs</h3> <p>In a Transformer, the base layer is the input fed to the model. This input usually contains the raw tokens as well as positional information.</p> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_15_0.svg" alt="svg"/></p> <p>In code, the symbol <code class="language-plaintext highlighter-rouge">tokens</code> represents the simplest transform. It returns the tokens passed to the model. The default input is the sequence “hello”.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_17_0.svg" alt="svg"/></p> <p>If we want to change the input to the transform, we use the input method to pass in an alternative.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_19_0.svg" alt="svg"/></p> <p>As with Transformers, we cannot access the positions of these sequences directly. However, to mimic position embeddings, we have access to a sequence of indices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">indices</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_21_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sop</span> <span class="o">=</span> <span class="n">indices</span>
<span class="n">sop</span><span class="p">.</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">goodbye</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_22_0.svg" alt="svg"/></p> <h3 id="feed-forward-network">Feed Forward Network</h3> <p>After the input layer, we reach the feed-forward network. In a Transformer, this stage can apply mathematical operations to each element of the sequence independently.</p> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_24_0.svg" alt="svg"/></p> <p>In code, we represent this stage by computation on transforms. Mathematical operations are overloaded to represent independent computation on each element of the sequence .</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l</span><span class="sh">"</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_26_0.svg" alt="svg"/></p> <p>The result is a new transform. Once constructed it can be applied to new input.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">*</span> <span class="mi">2</span>  <span class="o">-</span> <span class="mi">1</span>
<span class="n">model</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_28_0.svg" alt="svg"/></p> <p>Operations can combine multiple transforms. For example, functions of <code class="language-plaintext highlighter-rouge">tokens</code> and <code class="language-plaintext highlighter-rouge">indices</code>. The analogy here is that the Transformer activations can keep track of multiple pieces of information simultaneously.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">indices</span>
<span class="n">model</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_30_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l</span><span class="sh">"</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">indices</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_31_0.svg" alt="svg"/></p> <p>We provide a few helper functions to make it easier to write transforms. For example, <code class="language-plaintext highlighter-rouge">where</code> provides an “if” statement like construct</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">where</span><span class="p">((</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">h</span><span class="sh">"</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">l</span><span class="sh">"</span><span class="p">),</span> <span class="n">tokens</span><span class="p">,</span> <span class="sh">"</span><span class="s">q</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_33_0.svg" alt="svg"/></p> <p>And <code class="language-plaintext highlighter-rouge">map</code> lets us define our own operators, for instance a string to int transform. (Users should be careful to only use operations here that could be computed with a simple neural network).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">atoi</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">ord</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nf">ord</span><span class="p">(</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">))</span>
<span class="n">atoi</span><span class="p">.</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">31234</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_35_0.svg" alt="svg"/></p> <p>When chaining these transforms, it is often easier to work with functions. For example the following applies where and then <code>atoi</code> and then adds 2.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">atoi</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">seq</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">ord</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nf">ord</span><span class="p">(</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">))</span> 

<span class="n">op</span> <span class="o">=</span> <span class="p">(</span><span class="nf">atoi</span><span class="p">(</span><span class="nf">where</span><span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">op</span><span class="p">.</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">02-13</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_37_0.svg" alt="svg"/></p> <p>From here on, unless we use a different input sequence, we will assume that the input is ‘hello’ and omit the input display in the illustrations.</p> <h3 id="attention-selectors">Attention Selectors</h3> <p>Things get more interesting when we start to apply attention. This allows routing of information between the different elements of the sequence.</p> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_39_0.svg" alt="svg"/></p> <p>We begin by defining notation for the keys and queries of the model. Keys and queries are effectively transforms that we will broadcast and compare to each other to create <em>selectors</em>, our parallel to attention patterns. We create them directly from transforms. For example, if we want to define a key, we call <code class="language-plaintext highlighter-rouge">key</code> on a transform.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_41_0.svg" alt="svg"/></p> <p>Similarly for <code class="language-plaintext highlighter-rouge">query</code>. (Queries are presented as columns to reflect their relation to the selectors we will create from them.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_43_0.svg" alt="svg"/></p> <p>Scalars can be used as keys or queries. They broadcast out to the length of the underlying sequence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_45_0.svg" alt="svg"/></p> <p>By applying a comparison operation between a key and a query we create a <em>selector</em>, our parallel to an attention matrix - though this one is unweighted.</p> <p>A selector is a binary matrix indicating which input position (column) each output position (row) will attend to in an eventual attention computation. In the comparison creating it, the key values describe the input (column) positions, and the query values describe the output (row) positions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eq</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="n">eq</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_47_0.svg" alt="svg"/></p> <p>Some examples:</p> <ul> <li>A selector that matches each output position to the previous input position.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offset</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">offset</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_49_0.svg" alt="svg"/></p> <ul> <li>A selector that matches each output position to all earlier input positions.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">before</span> <span class="o">=</span> <span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">before</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_51_0.svg" alt="svg"/></p> <ul> <li>A selector that matches each output position to all later input positions.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">after</span> <span class="o">=</span> <span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">after</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_53_0.svg" alt="svg"/></p> <p>Selectors can be merged using boolean operations. For example, this selector focuses each output position on 1) earlier positions that 2) contain the same original input token as its own. We show this by including both pairs of keys and queries in the matrix.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">before</span> <span class="o">&amp;</span> <span class="n">eq</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_55_0.svg" alt="svg"/></p> <h3 id="using-attention">Using Attention</h3> <p>Given an attention selector we can provide a value sequence to aggregate. We represent aggregation by <strong>summing</strong> up over the values that have a true value for their selector.</p> <p>(Note: in the original paper, they use a <strong>mean</strong> aggregation and show a clever construction where mean aggregation is able to represent a sum calculation. RASPy uses sum by default for simplicity and to avoid fractions. In practicce this means that RASPy may underestimate the number of layers needed to convert to a mean based model by a factor of 2.)</p> <p>Attention aggregation gives us the ability to compute functions like histograms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_59_0.svg" alt="svg"/></p> <p>Visually we follow the architecture diagram. Queries are to the left, Keys at the top, Values at the bottom, and the Output is to the right.</p> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_61_0.svg" alt="svg"/></p> <p>Some attention operations may not even use the input tokens. For instance to compute the <code class="language-plaintext highlighter-rouge">length</code> of a sequence, we create a “select all” attention selector and then add 1 from each position.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">length</span> <span class="o">=</span> <span class="n">length</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">length</span><span class="sh">"</span><span class="p">)</span>
<span class="n">length</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_63_0.svg" alt="svg"/></p> <p>Here’s a more complex example, shown step-by-step. (This is the kind of thing they ask in interviews!)</p> <p>Say we want to compute the sum of neighboring values in a sequence, along a sliding window. First we apply the forward cutoff, attending only to positions that are not too far in the past.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">WINDOW</span><span class="o">=</span><span class="mi">3</span>
<span class="n">s1</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="n">WINDOW</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>  
<span class="n">s1</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_65_0.svg" alt="svg"/></p> <p>Then the backward cutoff, attending only to positions up to and including our own.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s2</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>
<span class="n">s2</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_67_0.svg" alt="svg"/></p> <p>Intersect.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sel</span> <span class="o">=</span> <span class="n">s1</span> <span class="o">&amp;</span> <span class="n">s2</span>
<span class="n">sel</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_69_0.svg" alt="svg"/></p> <p>And finally aggregate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sum2</span> <span class="o">=</span> <span class="n">sel</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> 
<span class="n">sum2</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_71_0.svg" alt="svg"/></p> <p>Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum. The cumulative sum has to go into a second layer because it is applied to a transform which uses length, and so it can only be computed after the computation of length is complete.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">before</span> <span class="o">|</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">))).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">cumsum</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">cumsum</span><span class="p">().</span><span class="nf">input</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_73_0.svg" alt="svg"/></p> <h3 id="layers">Layers</h3> <p>The language supports building up more complex transforms. It keeps track of the <em>layers</em> by tracking the operations computed so far.</p> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_76_0.svg" alt="svg"/></p> <p>Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="n">indices</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="nf">input</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_78_0.svg" alt="svg"/></p> <h2 id="coding-with-transformers">Coding with Transformers</h2> <p>Given this library of functions, we can write operations to accomplish surprisingly complex tasks.</p> <p><strong>Can we produce a Transformer that does basic addition of two arbitrary length numbers?</strong></p> <p>i.e. given a string “19492+23919” can we produce the correct output?</p> <p>We will go through these steps, and their solutions, here. If you would rather do them on your own, we provide a version where you can try them yourself!</p> <p>Before we dive in to the main task, we will do some challenges of increasing difficulty to help us build some intuitions.</p> <h3 id="challenge-1-select-a-given-index">Challenge 1: Select a given index</h3> <p>Produce a sequence where all the elements have the value at index i.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">i</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_83_0.svg" alt="svg"/></p> <h3 id="challenge-2-shift">Challenge 2: Shift</h3> <p>Shift all of the tokens in a sequence to the right by i positions. (Here we introduce an optional parameter in the aggregation: the default value to be used when no input positions are selected. If not defined, this value is 0.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shift</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="o">-</span><span class="n">i</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">shift</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">shift</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_85_0.svg" alt="svg"/></p> <h3 id="challenge-3-minimum">Challenge 3: Minimum</h3> <p>Compute the minimum values of the sequence. (This one starts to get harder. Our version uses 2 layers of attention.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">sel1</span> <span class="o">=</span> <span class="n">before</span> <span class="o">&amp;</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span>
    <span class="n">sel2</span> <span class="o">=</span> <span class="nf">key</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nf">query</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">less</span> <span class="o">=</span> <span class="p">(</span><span class="n">sel1</span> <span class="o">|</span> <span class="n">sel2</span><span class="p">).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">less</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">minimum</span><span class="p">()([</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_87_0.svg" alt="svg"/></p> <p>The idea behind our solution is an implicit full ordering of the input positions: we (implicitly) order the positions according to input token value, with input position as tie breaker. Our first act is to have each position attend to all positions before it in the ordering: <code class="language-plaintext highlighter-rouge">sel1</code> focuses on earlier input positions with the same input token value, and <code class="language-plaintext highlighter-rouge">sel2</code> focuses on input positions with lower input token value. We then aggregate a 1 from all positions to get where each position is located in this ordering (i.e., how many other positions precede it). The minimum value is the input value at the first position according to this ordering (i.e., the one which had no other positions precede it).</p> <h3 id="challenge-4-first-index">Challenge 4: First Index</h3> <p>Compute the first index that has token q, assuming the sequence always has length shorter than 100. (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">first</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">minimum</span><span class="p">(</span><span class="nf">where</span><span class="p">(</span><span class="n">seq</span> <span class="o">==</span> <span class="n">q</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">99</span><span class="p">))</span>
<span class="nf">first</span><span class="p">(</span><span class="sh">"</span><span class="s">l</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_90_0.svg" alt="svg"/></p> <h3 id="challenge-5-right-align">Challenge 5: Right Align</h3> <p>Right align a padded sequence e.g. ralign().inputs(‘xyz___’) = ‘—xyz’” (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ralign</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> <span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">sop</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">sop</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">ralign</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">ralign</span><span class="p">()(</span><span class="sh">"</span><span class="s">xyz__</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_92_0.svg" alt="svg"/></p> <h3 id="challenge-6-split">Challenge 6: Split</h3> <p>Split a sequence into two parts at value v and then right align. You can assume there is exactly one appearance of v in the sequence. (3 layers to get and align the first part of the sequence, but only 1 for the second.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">get_first_part</span><span class="p">,</span> <span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">split_point</span> <span class="o">=</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">sop</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">v</span><span class="p">)).</span><span class="nf">value</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">get_first_part</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">ralign</span><span class="p">(</span><span class="n">default</span><span class="p">,</span> 
                   <span class="nf">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&lt;</span> <span class="n">split_point</span><span class="p">,</span> 
                         <span class="n">sop</span><span class="p">,</span> <span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&gt;</span> <span class="n">split_point</span><span class="p">,</span> <span class="n">sop</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">)(</span><span class="sh">"</span><span class="s">xyz+zyr</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_94_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="sh">"</span><span class="s">xyz+zyr</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_95_0.svg" alt="svg"/></p> <h3 id="challenge-6-slide">Challenge 6: Slide</h3> <p>Replace special tokens “&lt;” with the closest non “&lt;” value to their right. (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">slide</span><span class="p">(</span><span class="n">match</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="p">((</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="nf">key</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> <span class="o">==</span> <span class="nf">query</span><span class="p">(</span><span class="bp">True</span><span class="p">))).</span><span class="nf">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">seq</span> <span class="o">=</span>  <span class="nf">where</span><span class="p">(</span><span class="n">match</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">seq</span><span class="p">.</span><span class="nf">name</span><span class="p">(</span><span class="sh">"</span><span class="s">slide</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">slide</span><span class="p">(</span><span class="n">tokens</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">&lt;</span><span class="sh">"</span><span class="p">).</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">xxxh&lt;&lt;&lt;l</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_97_0.svg" alt="svg"/></p> <h3 id="challenge-7-add">Challenge 7: Add</h3> <p>For this one you want to perform addition of two numbers. Here are the steps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">add</span><span class="p">().</span><span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">683+345</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ol> <li>Split into parts (challenge 6). Convert to ints. Add.</li> </ol> <blockquote> <p>“683+345” =&gt; [0, 0, 0, 9, 12, 8]</p> </blockquote> <ol> <li>Compute the carry terms. Three possibilities: definitely receives carry (“1”), definitely doesn’t receive carry (“0”), maybe receives carry (“&lt;”).Because we are only adding two numbers, the only case in which a position might receive a carry is if the position after it sums to 9. In that case, it will receive a carry if and only if the position after <em>that</em> receives a carry.</li> </ol> <blockquote> <p>[0, 0, 0, 9, 12, 8] =&gt; “00&lt;100”</p> </blockquote> <ol> <li>Slide the carry coefficients. A position that might receive a carry will get one if and only if the next position receives a carry - and so on down the chain until the next definite carry/no carry.</li> </ol> <blockquote> <p>“00&lt;100” =&gt; 001100”</p> </blockquote> <ol> <li>Complete the addition.</li> </ol> <p>Each of these is 1 line of code. The full system is 6 layers. (if you are careful you can do it in 5!).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="c1"># 0) Parse and add
</span>    <span class="n">x</span> <span class="o">=</span> <span class="nf">atoi</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">sop</span><span class="p">))</span> \
        <span class="o">+</span> <span class="nf">atoi</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="n">sop</span><span class="p">))</span>
    <span class="c1"># 1) Check for carries 
</span>    <span class="n">gets_carry</span> <span class="o">=</span> <span class="nf">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="nf">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">9</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">,</span> <span class="nf">where</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="mi">9</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">)))</span>
    <span class="c1"># 2) Slide carries to their columns - all in one parallel go!                                         
</span>    <span class="n">gets_carry</span> <span class="o">=</span> <span class="nf">atoi</span><span class="p">(</span><span class="nf">slide</span><span class="p">(</span><span class="n">gets_carry</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">&lt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">gets_carry</span><span class="p">))</span>
    <span class="c1"># 3) Add in carries, and remove overflow from original addition.                                                                                  
</span>    <span class="nf">return </span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">gets_carry</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span>
<span class="nf">add</span><span class="p">()(</span><span class="sh">"</span><span class="s">683+345</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/2023/assets/img/2023-05-01-raspy/Blog_99_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">683</span> <span class="o">+</span> <span class="mi">345</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1028
</code></pre></div></div> <p>Pretty neat stuff. If you are interested more in this topic, be sure to check at the paper:</p> <p><a href="https://arxiv.org/pdf/2106.06981.pdf">Thinking like Transformers</a> and the <a href="https://github.com/tech-srl/RASP">RASP language</a>.</p>]]></content><author><name>Alexander Rush</name></author><summary type="html"><![CDATA[Thinking like Transformers proposes a simple language for coding with attention-like primitives. Using this language, we consider a challenging set of puzzles to gain intuition for how Transformer could implement basic algorithms.]]></summary></entry><entry><title type="html">Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning</title><link href="https://iclr-blogposts.github.io/2023/blog/2023/riit/" rel="alternate" type="text/html" title="Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning"/><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2023/blog/2023/riit</id><content type="html" xml:base="https://iclr-blogposts.github.io/2023/blog/2023/riit/"><![CDATA[<h2 id="background">Background</h2> <h3 id="from-rl-to-marl">From RL to MARL</h3> <p>Since AlphaZero beats humans at Go, RL has become a consistent hot spot in academia and industry. The agent of RL can obtain some rewards by interacting with the environment and taking actions to maximize these cumulative rewards. Actually, almost all the RL problems can be described as <strong>Markov Decision Processes</strong> as illustrated in Figure <a href="#mdp">1</a>.</p> <div id="mdp" class="img-height-200 img-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/mdp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/mdp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/mdp-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/mdp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 1: The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton &amp; Barto (2018)<d-cite key="sutton2018reinforcement"></d-cite>). $R_t, S_t, A_t$ denote the reward, state and action at timestep $t$.</div> <p>Just as its name implies, MARL contains multiple agents trained by RL algorithms in the same environment. Many complex multi-agent systems such as robot swarm control, autonomous vehicle coordination, and sensor networks, can be modeled as MARL tasks. The interaction of these agents would make them work together to achieve a common goal.</p> <div style="display:flex; margin-bottom:-30px; margin-left :150px; margin-right :150px"> <div id="chase" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/chase.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/chase.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/chase.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/chase.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="magent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/magent.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/magent.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/magent.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/magent.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></div> <div style="display:flex; margin-top:-30px; margin-left :50px; margin-right :50px"> <div id="hide" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/hide.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/hide.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/hide.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/hide.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="smac" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/smac.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/smac.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/smac.gif-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/smac.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></div> <div style="margin-bottom: 20px"><div class="caption">Figure 2: Some multi-agent cooperative scenarios [from-left-to-right]. <a href="https://github.com/openai/multiagent-particle-envs"> <br/> (a) Chasing in Multi-Agent Particle Environment (Predator-Prey); </a> <a href="https://github.com/geek-ai/MAgent"> (b) MAgent Environment; </a> <a href="https://openai.com/blog/emergent-tool-use"> <br/> (c) Hide &amp; Seek; </a> <a href="https://github.com/oxwhirl/smac"> (d) StarCraft Multi-Agent Challenge. </a></div></div> <p>In this general setting, agents usually have a limited sight range to observe their surrounding environment. As shown in Figure <a href="#smac_obs">3</a>, the cyan border indicates the sight and shooting range of the agent, which means the agent could only obtain the information of terrain or other agents in that range. This restricted field of view may also result in the difficulty of agents to access to global state information, making its policy updates subject to bias and unsatisfactory performance. In general, these kinds of multi-agent scenarios can be modeled as <strong>Decentralized Partially Observable Markov Decision Processes</strong> (Dec-POMDP)<d-cite key="png2009pomdps"></d-cite>.</p> <p>Even though many RL algorithms<d-cite key="sutton2018reinforcement"></d-cite> and their variants have been successfully extended to the cooperative scenarios in MARL setting, few of their performance is satisfactory. One of the most troublesome issues is <em>Non-Stationarity</em>. Specifically, as a part of the environment, the changing policies of other agents during training would make the observation non-stationary from the perspective of any individual agent<d-cite key="oliehoek2016concise"></d-cite> and significantly slow down the policy optimization of MARL. This situation has forced researchers to seek a method that can exploit global information during training but does not destroy the ability of the agents to only use their respective observations during execution, to find a joint policy $\boldsymbol{\pi} = \langle \pi^{1},…,\pi^{n}\rangle$ to maximize global reward. Naturally, the simplicity and effectiveness of the <strong>Centralized Training with Decentralized Execution</strong> (CTDE) paradigm have attracted the attention of the community, and many MARL algorithms based on CTDE were proposed, making a remarkable contribution to MARL.</p> <p>In the rest of this section, we briefly introduce Dec-POMDP and CTDE to facilitate the understanding of the contents of MARL, the QMIX algorithm and the following text.</p> <div style="float:left; margin-left :150px; margin-right :150px;"><div id="smac_obs" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/smac_agent_obs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/smac_agent_obs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/smac_agent_obs-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/smac_agent_obs.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 3: The partial observation of agents <br/>(Image source: SMAC<d-cite key="samvelyan2019starcraft"></d-cite>). </div></div> <h3 id="decentralized-partially-observable-markov-decision-process">Decentralized Partially Observable Markov Decision Process</h3> <p>A <strong>Decentralized Partially Observable Markov Decision Process</strong> (Dec-POMDP) model, as described in <d-cite key="pmlr-v80-rashid18a"></d-cite><d-cite key="oliehoek2016concise"></d-cite>, is typically used to represent a full cooperative multi-agent task. The model consists of a tuple denoted by $G=(S, U, P, r, Z, O, n, \gamma)$, and involves $n$ agents, where $n$ is an integer between 1 and $n$, inclusive. The true state of the environment, denoted by $s \in S$, describes global information that is relevant to both agents and other auxiliary features. At each timestep $t$, a transition in the environment occurs via a joint action $\mathbf{u} \in \mathbf{U} \equiv U^{n}$, which is composed of an action $u^i \in U$, chosen by each agent. This transition is driven by the state transition function $P\left(s^{\prime} \mid s, \mathbf{u}\right): S \times \mathbf{U} \times S \rightarrow[0,1]$. Additionally, there is a shared global reward function, denoted by $r(s, \mathbf{u}): S \times \mathbf{U} \rightarrow \mathbf{R}$, which is optimized by the whole team. Finally, each agent has a partial observation described by $o^i \in O$, which is derived from the observation function $Z(o^i \mid s, u^i) : S \times U \rightarrow O$. All agents work cooperatively to maximize the shared global reward $R_{t}=\sum_{k=0}^{T} \gamma^{k} r_{t+k}$, which is described by the joint value function \(Q^{\boldsymbol{\pi}}\left(s_{t}, \mathbf{u}_{t}\right) = \mathbb{E}_{s_{t+1: \infty}, \mathbf{u}_{t+1: \infty}}\left[R_{t} \mid s_{t}, \mathbf{u}_{t}\right]\).</p> <h3 id="centralized-training-with-decentralized-execution-and-value-decomposition">Centralized Training with Decentralized Execution and Value Decomposition</h3> <p>To better explore the factors affecting the QMIX algorithm, our focus lies in the <strong>Centralized Training with Decentralized Execution</strong> (CTDE) paradigm of MARL algorithms. These algorithms under this paradigm have access to the true state $s$ and the action-observation histories $\tau^{i}$ of all agents to centrally train policies, but each agent can only rely on its local observation $o^{i}$ for decision-making. Some value-based algorithms implemented under CTDE follow the Individual-Global-Max (<strong>IGM</strong>) principle<d-cite key="pmlr-v97-son19a"></d-cite>, ensuring consistency between the joint action-value function $Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)$ and individual agent-utilities $[Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}$:</p> \[\underset{\mathbf{u}}{\operatorname{argmax}}\ Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right) = (\underset{u^{1}}{\operatorname{argmax}}\ Q_{1} \left(\tau^{1}, u^{1}\right), \ldots, \underset{u^{n}}{\operatorname{argmax}}\ Q_{n} \left(\tau^{n} , u^{n}\right)). \tag{1} \label{eq1}\] <p>One of the most typical ways to efficiently train the joint value function \(Q_{tot} \left(\boldsymbol{\tau}, \mathbf{u}\right)\) is to decompose it into the utility functions \([Q_i\left(\tau^i, u^i\right)] _{i=1} ^{n}\) and maintain updating consistency between them via IGM. The simplest factorization structure, called <em>additivity</em>, has been proposed by VDN<d-cite key="10.5555/3237383.3238080"></d-cite>, which makes VDN simply factorize $Q_{tot}$ into a sum of per-agent utilities \(Q_{tot}^{\mathrm{VDN}} \left(\boldsymbol{\tau}, \boldsymbol{u}\right)=\sum_{i=1}^{n} Q_{i} \left(\tau^{i}, u^{i}\right)\). VDN’s simplicity and equal weighting of each utility in the joint value function makes it ineffective for cooperative tasks, which has motivated the QMIX structure and other more efficient decomposition approaches.</p> <h3 id="notation">Notation</h3> <p>In this subsection, we define the notations used in this post. Specifically, in traditional RL, time steps $t$ are usually represented in the update formula and the value function of RL is considered to be estimated by the pairwise variables at the current time step $t$ and the next time step $t+1$. Since the <em>ID</em> of the agent also needs to be represented in the MARL algorithm, it may cause ambiguity when expressed in the same formula as the time step $t$. For simplicity of expression, variables without $t$ are indicated to be implemented at the current time step, while variables at the next time step are indicated with an apostrophe in the upper right corner in the rest of the context, e.g., $s$ means the current state and $s^{\prime}$ indicates the next time step state, the same approach applies to actions $u$ and observations $o$. All the notations are listed in Table <a href="#table1">1</a>.</p> <p><a name="table1"> </a></p> <div class="caption"> Table 1: All the notations used in this post. </div> <style type="text/css">.tg{border-collapse:collapse;border-spacing:0}.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial,sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal}.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial,sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal}.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}</style> <table class="tg"> <thead> <tr> <th class="tg-c3ow">Notation</th> <th class="tg-c3ow">Description</th> <th class="tg-c3ow">Notation</th> <th class="tg-c3ow">Description</th> </tr> </thead> <tbody> <tr> <td class="tg-c3ow">$s$</td> <td class="tg-c3ow">the current state (at time $t$)</td> <td class="tg-c3ow">$S$</td> <td class="tg-c3ow">the set of all states</td> </tr> <tr> <td class="tg-c3ow">$s^{\prime}$</td> <td class="tg-c3ow">the next state (at time $t+1$)</td> <td class="tg-c3ow">$U$</td> <td class="tg-c3ow">the set of all actions</td> </tr> <tr> <td class="tg-c3ow">$u^{i}$</td> <td class="tg-c3ow">the action of agent $i$</td> <td class="tg-c3ow">$N$</td> <td class="tg-c3ow">the set of all agents</td> </tr> <tr> <td class="tg-c3ow">$\mathbf{u}$</td> <td class="tg-c3ow">the joint actions (at time $t$)</td> <td class="tg-c3ow">$\tau^{i}$</td> <td class="tg-c3ow">the action-observation history of agent $i$</td> </tr> <tr> <td class="tg-c3ow">$o^{i}$</td> <td class="tg-c3ow">the observation of agent $i$</td> <td class="tg-c3ow">$${\tau}$$</td> <td class="tg-c3ow">the joint action-observation histories</td> </tr> <tr> <td class="tg-c3ow">$$o$$</td> <td class="tg-c3ow">the joint observation</td> <td class="tg-c3ow">$r(s, \mathbf{u})$</td> <td class="tg-c3ow">the joint reward supplied by environments</td> </tr> <tr> <td class="tg-c3ow">$Q_{i}(\tau^{i}, u^{i})$</td> <td class="tg-c3ow">the utility function of agent $i$</td> <td class="tg-c3ow">$\gamma$</td> <td class="tg-c3ow">the discount factor</td> </tr> <tr> <td class="tg-c3ow">$Q_{tot}({\tau}, \mathbf{u})$</td> <td class="tg-c3ow">the joint value function </td> <td class="tg-c3ow">$P(s^{\prime} \mid s, \mathbf{u})$</td> <td class="tg-c3ow">the transition function</td> </tr> <tr> <td class="tg-c3ow">$Z(o^{i} \mid s, u^{i})$</td> <td class="tg-c3ow">the observation function</td> <td class="tg-c3ow">$\epsilon$</td> <td class="tg-c3ow">action selection probability of $\epsilon$-greedy</td> </tr> <tr> <td class="tg-c3ow">$N$</td> <td class="tg-c3ow">the set of all agents with $n$ agents</td> <td class="tg-c3ow">$$\theta$$</td> <td class="tg-c3ow">the set of parameters of agents network, with $[\theta^{i}]_{i=1}^{n}$</td> </tr> <tr> <td class="tg-c3ow">$b$</td> <td class="tg-c3ow">sampled batch size for training</td> <td class="tg-c3ow">$\phi$</td> <td class="tg-c3ow">the parameter of mixing network</td> </tr> <tr> <td class="tg-c3ow">$TS$</td> <td class="tg-c3ow">the $T$otal rollout $S$amples</td> <td class="tg-c3ow">$PP$</td> <td class="tg-c3ow">the number of rollout $P$rocesses in $P$arallel</td> </tr> <tr> <td class="tg-c3ow">$SE$</td> <td class="tg-c3ow">the number of $S$amples in each <br/> $E$pisode</td> <td class="tg-c3ow">$PI$</td> <td class="tg-c3ow">the $P$olicy $I$teration number</td> </tr> </tbody> </table> <h2 id="qmix-and-monotonicity-constraint">QMIX and Monotonicity Constraint</h2> <p>To deal with the relationship between the individual agent and the cooperative group, QMIX<d-cite key="pmlr-v80-rashid18a"></d-cite> learns a joint action-value function $Q_{tot}$ and factorizes the joint policy into the individual policy of each agent. In other words, as illustrated in Figure <a href="#frame">4</a>, QMIX integrates all the individual $Q_{i}$ with a mixing network to obtain a centralized value function $Q_{tot}$, which can be more appropriately updated by the global reward.</p> <div id="frame" class="img-height-310 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/qmix_frame-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/qmix_frame-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/qmix_frame-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/qmix_frame.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 4: Framework of QMIX. (Image source: QMIX<d-cite key="pmlr-v80-rashid18a"></d-cite>). On the left is Mixing Network (A Hypernetwork), and on the right is the Agent network.</div> <p>Still, it also can be represented in Eq.(\ref{eq2})</p> \[Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi) = g_{\phi}\left(s, Q_{1}\left(\tau^{1}, u^{1} ; \theta^{1}\right), \ldots, Q_{n}\left(\tau^{n}, u^{n} ; \theta^{n}\right)\right);\] \[with \quad \frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0, \quad \forall i \in N. \tag{2} \label{eq2}\] <p>where $\theta^i$ is the parameter of the agent network $i$, $u^{i}$ denotes the action of agent $i$, and $\phi$ is the trainable parameter of the mixing network. The the mixing network $g_{\phi}(\cdot)$ is responsible to factorize $Q_{tot}$ to each utility $Q_{i}$. The <em>Monotonicity Constraint</em> is also implemented in the mixing network $g_{\phi}(\cdot)$, which inputs the global state $s$ and outputs <em>non-negative</em> wights through a <em>hyper-network</em> as illustrated in the left part of Figure <a href="#frame">4</a>, which will result in \(\frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0\). This delicate design ensures consistency between joint actions and the individual actions of each agent, then guarantees the Individual-Global-Max (IGM) principle. Benefiting from the monotonicity constraint in Eq.(\ref{eq2}), maximizing joint $Q_{tot}$ is precisely the equivalent of maximizing individual $Q_i$, which would also allow the optimal individual action to maintain consistency with optimal joint action. Furthermore, QMIX learns the centralized value function $Q_{tot}$ by sampling a multitude of transitions from the replay buffer and minimizing the mean squared temporal-difference (TD) error loss:</p> \[\mathcal{L}(\theta)= \frac{1}{2} \sum_{i=1}^{b}\left[\left(y_{i}^{}-Q_{tot}(s, u ; \theta, \phi)\right)^{2}\right] \tag{3} \label{eq3}\] <p>where the TD target value \(y=r+\gamma \underset{u^{\prime}}{\operatorname{max}} Q_{tot}(s^{\prime},u^{\prime};\theta^{-},\phi^{-})\), and $\theta^{-}, \phi^{-}$ are the target network parameters copied periodically from the current network and kept constant for a number of iterations. $b$ is the sampled training batch size. Due to the strong constraints in Eq.(\ref{eq2}), QMIX is still criticized for the insufficient expressive capacity of the joint value function<d-cite key="mahajan2019maven"></d-cite>.</p> <h2 id="extension-to-qmix">Extension to QMIX</h2> <h3 id="experimental-design">Experimental Design</h3> <p>To facilitate the study of proper techniques affecting the training effectiveness and sample efficiency of QMIX, we perform a set of experiments designed to provide insight into some methods that have been proven effective in single-agent RL but may be ambiguous in MARL. In particular, we investigate the effects of <strong>Adam optimizer with parallel rollout process; the incremental replay buffer size; the number of parallel rollout processes; $\epsilon$-exploration steps; the implementation of $Q(\lambda)$ in centralized value function; the hidden size of the recurrent network of agents</strong>. And we also dive into the <strong>role of monotonicity constraints in QMIX</strong>. For all experiments, we generally implement PyMARL<d-cite key="samvelyan2019starcraft"></d-cite> framework to implement QMIX. To ensure fairness we run independent 3 to 6 experimental trials for each evaluation, each with a random seed. Unless otherwise mentioned, we use default settings as in PyMARL whenever possible, while incorporating the techniques of interest. To prevent the training process of the algorithm from crashing by chance, we remove the highest and lowest scores when counting the calculated returns and win rates for the test episode. All the results are plotted with the median and shaded the interval, and the final scores were <strong><em>not</em></strong> smoothed for the sake of image aesthetics, and we did so to verify exactly what direct effect the proposed techniques could have on QMIX.</p> <p><strong>StarCraft Multi-Agent Challenge (SMAC)</strong> As a commonly used testing environment, SMAC<d-cite key="samvelyan2019starcraft"></d-cite> sets an example to offer a great opportunity to tackle the cooperative control problems in the multi-agent domain. We focus on the micromanagement challenge in SMAC, which means each agent is controlled by an independent agency that conditions on a limited observation area, and these groups of units are trained to conquer the enemy consisting of built-in AI. According to the quantity and type of enemy, all testing scenarios could be divided into <em>Easy, Hard</em>, and <em>Super-Hard</em> levels. Since QMIX can effectively solve the <em>Easy</em> tasks, we pay attention to some <em>Hard</em> and <em>Super-Hard</em> scenarios that QMIX failed to win, especially in <em>Corridor, 3s5z_vs_3s6z</em>, and <em>6h_vs_8z</em>.</p> <p><strong>Predator-Prey (PP)</strong> is representative of another classical problem called <em>relative overgeneralization</em><d-cite key="wei2018multiagent"></d-cite>. The cooperating predators are trained to chase a faster running prey, and hope to capture this escaping robot with the fewest steps possible. We leverage Predator-Prey-2 (a variant of Predator-Prey) proposed in FACMAC<d-cite key="peng2021facmac"></d-cite>, whose policy of prey is replaced with a hard-coded heuristic policy. The heuristic policy asks the prey to move to the farthest sampled position to the closest predator. If one of the cooperative agents collides with the prey, a team reward of +10 is emitted; otherwise, no reward is given. In the original simple tag environment, each agent can observe the relative positions of the other two agents, the relative position and velocity of the prey, and the relative positions of the landmarks. This means each agent’s private observation provides an almost complete representation of the true state of the environment.</p> <p>To introduce partial observability to the environment, the view radius is added to the agent, which restricts the agents from receiving information about other entities (including all landmarks, the other two agents, and the prey) that are out of range. Specifically, we set the view radius such that the agents can only observe other agents roughly 60% of the time. These environments require greater cooperation between agents.</p> <p><strong>Notes:</strong> Although the code repository of this post is given in the abstract, we give its url here again for greater convenience and still strongly welcome researchers to conduct experiments referring to the proposed methods. Still, in the following subsections, we post their corresponding permalinks for easy understanding.</p> <p>Code Repository: <a href="https://github.com/hijkzzz/pymarl2"> https://github.com/hijkzzz/pymarl2 </a></p> <h3 id="optimizer">Optimizer</h3> <p>As an important part of training neural networks, the selection of an optimizer is very important since it could seriously affect the training effect of the reinforcement learning agent. Without a further illustration, QMIX uses RMSProp<d-cite key="zou2019sufficient"></d-cite> to optimize the neural networks of agents as they prove stable in SMAC. While Adam<d-cite key="kingma2014adam"></d-cite> is famous for the fast convergence benefiting from the momentum in training, which seems to be the first choice for AI researchers. We reckon that the momentum property in Adam would have some advantages in learning the sampled data which is generated by agents interacting with the environment as in MARL. And then, on the other hand, QMIX is criticized for performing sub-optimally and sampling inefficiency when equipped with the A2C framework, which is implemented to promote the training efficiency of the RL algorithm. VMIX<d-cite key="su2021value"></d-cite> argues this limitation is brought about by the value-based inherent Q function, so they extend QMIX to the actor-critic style algorithm to take advantage of the A2C framework. This controversy attracts our attention to evaluate the performance of QMIX using Adam, as well as the parallel sampling paradigm.</p> <p><strong>Permalink:</strong> <a href="https://github.com/hijkzzz/pymarl2/blob/45278a5f8d1e3d006811351ed5fa99d614731e7d/src/learners/nq_learner.py#L37-L40"> Adam optimizer in nq_learner. </a></p> <div id="optimizer" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/optimizer.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/optimizer.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/optimizer.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/optimizer.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 5: The performance of QMIX optimized by Adam and RMSProp.</div> <p><strong>Results</strong> As shown in Figure <a href="#optimizer">5</a>, we run the Adam-supported QMIX with <strong>8 rollout processes</strong>. Different from what was described in VMIX, the performance and efficiency of QMIX could be greatly improved by Adam. We speculate the reason is the momentum property in Adam could fastly fit the newly sampled data from the parallel rollout processes and then enhance the performance, while RMSProp failed.</p> <h3 id="rollout-process-number">Rollout Process Number</h3> <p>Naturally, we come to focus on the benefits of parallel data sampling in QMIX. A2C<d-cite key="pmlr-v48-mniha16"></d-cite> provides an excellent example to reduce training time and improve the training efficiency in single-agent RL. As we implement the algorithms under the paradigm of A2C, there is usually a defined total number of samples and an unspecified number of rollout processes. The total number of samples $TS$ can be calculated as $TS = SE \cdot PP \cdot PI$, where $TS$ is the total sum of sampled data, $SE$ denotes the number of samples in each episode, $PP$ and $PI$ denote the number of rollout processes in parallel and the policy iteration number, respectively. This section aims to perform analysis and spur discussion on the impact of the parallel rollout process on the final performance of QMIX.</p> <p><strong>Permalink:</strong> <a href="https://github.com/hijkzzz/pymarl2/blob/45278a5f8d1e3d006811351ed5fa99d614731e7d/src/config/algs/qmix.yaml#L9-L10"> 1) Rollout process number setting in the configuration file</a>; 2) <a href="https://github.com/hijkzzz/pymarl2/blob/45278a5f8d1e3d006811351ed5fa99d614731e7d/src/runners/parallel_runner.py#L88-L212"> Parallel trajectory sampling code. </a></p> <div id="process_number" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/process_number.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/process_number.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/process_number.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/process_number.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 6: The performance of different rollout process numbers of QMIX. When given the total number of samples, the performance of fewer processes achieves better performance.</div> <p><strong>Results</strong> Still, we use Adam-supported QMIX to evaluate the effect of the number of the rollout process. Since we could choose the <em>Parallel</em> model to sample the interacting data of the agent with the environment in PyMARL, we can theoretically get more <strong>on-policy</strong> data which is close to the updating policy in training. Figure <a href="#process_number">6</a> shows that when $TS$ and $PP$ is given, the performance enhancement of QMIX is not consistent with the increase in rollout process number. The intuitive explanation is when we set the fewer rollout processes, the greater the quantity of policy would iterate<d-cite key="sutton2018reinforcement"></d-cite>. Besides, too fast updated data in parallel may cause the factitious unstable training in policy updating, i.e., it is difficult for agents to learn effective information from rapidly sampled data from the replay buffer. The more times policies are iterated, the more information the agents would learn which lead to an increase in performance. However, it also causes longer training time and loss of stability. We suggest trying the fewer rollout process in the beginning and then balancing between training time and performance.</p> <h3 id="replay-buffer-size">Replay Buffer Size</h3> <p>Replay buffer plays an important role in improving sample efficiency in off-policy single-agent RL. Its capacity would greatly affect the performance and stability of algorithms. Researchers usually set a very large capacity of replay buffer in Deep Q-network (DQN)<d-cite key="mnih2013playing"></d-cite> to stabilize the training. Some research on the effect of replay buffer in single-agent RL has already been carried out in <d-cite key="pmlr-v119-fedus20a"></d-cite>, which poses the distribution of sampled training data should be close as possible to the agents’ policies to be updated. Actually, there are two factors affected when we change the capacity of the replay buffer: (1) the replay capacity (total number of transitions/episodes stored in the buffer); and (2) the replay ratio (the number of gradient updates per environment transition/episode) of old policies. When we increase the capacity of the replay buffer, the aged experiences of old policies would grow as the replay ratio is fixed. Then the distribution of outdated experiences would also be much different from the updating policy, which would bring additional difficulty to the training agents. From the results in <d-cite key="pmlr-v119-fedus20a"></d-cite>, there seems to be an optimal range of choices between replay buffer size and replay ratio of experiences in RL, where we would like to know whether it is consistent with the results in MARL.</p> <p><strong>Permalink:</strong> <a href="https://github.com/hijkzzz/pymarl2/blob/45278a5f8d1e3d006811351ed5fa99d614731e7d/src/config/algs/qmix.yaml#L11"> Replay buffer size setting in the configuration file. </a></p> <div id="replay_buffer" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/buffer_size.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/buffer_size.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/buffer_size.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/buffer_size.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 7: Setting the replay buffer size to 5000 episodes allows for QMIX’s learning to be stable.</div> <p><strong>Results</strong> The results seem not to be consistent with that in single-agent RL. Figure <a href="#replay_buffer">7</a> shows the large replay buffer size of QMIX would cause instability during training. When we increase the buffer size from the default setting in PyMARL, the performance would almost continuously declines. We speculate the reason is the fast-changing distribution of experiences in a larger buffer would make it more difficult to fit sampled data due to the enormous joint action space. Since the samples become obsolete more quickly, these aged policies would also be more different from the updating policy, which brings additional difficulty. On the other hand, we find the same performance decline when we squeeze the buffer. We reckon that a small buffer would accelerate the updating speed of sampling data in a disguised way, which makes it tough to fit the data and learn a good policy. We believe that researchers should be cautious to increase the buffer size in other multi-agent applications.</p> <h3 id="eligibility-traces">Eligibility Traces</h3> <p>The well-known trade-off between bias and variance of bootstrapping paradigm is a classic research topic in RL. Since we implement the Centralized Value Function (CVF) to alleviate the <em>Non-Stationarity</em> multi-agent settings, the estimated accuracy of CVF is critical to MARL and then guides the policies of agents to update. Eligibility traces such as TD($\lambda$)<d-cite key="sutton2018reinforcement"></d-cite>, Peng’s Q($\lambda$)<d-cite key="pmlr-v139-kozuno21a"></d-cite>, and TB($\lambda$)<d-cite key="10.5555/645529.658134"></d-cite> achieve a balance between return-based algorithms (where return refers to the sum of discounted rewards $\sum_{k} \gamma^{k} r_{t+k}$) and bootstrap algorithms (where return refers $r_t + V(s_{t+1})$), then speed up the convergence of agents’ policies. As a pioneer, SMIX<d-cite key="wen2020smix"></d-cite> equipped QMIX with the SARSA($\lambda$) to estimate the accurate CVF and get decent performance. As another example of eligibility trace in Q-learning, we study the estimation of CVF using Peng’s Q$(\lambda)$ for QMIX.</p> <p><strong>Permalink:</strong> <a href="https://github.com/hijkzzz/pymarl2/blob/45278a5f8d1e3d006811351ed5fa99d614731e7d/src/utils/rl_utils.py#L6-L45"> Different eligibility traces code in repository. </a></p> <div id="qlambda1" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/td_lambda.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/td_lambda.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/td_lambda.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/td_lambda.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 8: Q(λ) significantly improves the performance of QMIX, but large values of λ lead to instability in the algorithm. </div> <p><strong>Results</strong> As the same in single-agent RL, the Q-networks without sufficient training usually have a large bias in bootstrapping returns. Figure <a href="#qlambda1">8</a> shows that, with the help of Q$(\lambda)$, the performance of QMIX has generally improved across all scenarios. It means the more accurate estimate of CVF would still provide a better direction of policy updating for each agent. However, the value of $\lambda$ in Peng’s Q$(\lambda)$ is not so radical as in single-agent RL, which would lead to failed convergence due to the large variance. We recommend a small $\lambda$, such as $0.5$, when using $Q(\lambda)$ in MARL.</p> <h3 id="hidden-size">Hidden Size</h3> <p>Searching for an optimal scale and architecture of neural networks is a very tough problem in the field of machine learning. Researchers typically use empirically small networks to train the agents in deep reinforcement learning. Since the role of neural networks is to extract the features of input states and actions, the size of the neural network would also have a great impact on the performance of MARL algorithms. The study in <d-cite key="pmlr-v119-ota20a"></d-cite> has revealed that networks with a complex structure like ResNet<d-cite key="He_2016_CVPR"></d-cite> and DenseNet<d-cite key="Huang_2017_CVPR"></d-cite> can extract more useful information for training, while Ba<d-cite key="ba2014deep"></d-cite> poses that the width of neural networks is probably more important than its depth. The subsequent study on QMIX<d-cite key="rashid2020monotonic"></d-cite> makes preliminary research on the depth of neural networks, which showed a limited improvement in performance. Though, there is little research on the width of neural networks in MARL. Instead of searching for an optimal network architecture here, we just want to make a pilot study on the effect of the hidden size of network width in QMIX.</p> <p><strong>Permalink:</strong> <a href="https://github.com/hijkzzz/pymarl2/blob/45278a5f8d1e3d006811351ed5fa99d614731e7d/src/config/algs/qmix_large.yaml#L25-L30"> Hidden size of neural network setting in the configuration file. </a></p> <div id="hiddensize" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/hidden_size.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/hidden_size.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/hidden_size.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/hidden_size.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 9: Impact of the hidden size of network in QMIX.</div> <p><strong>Results</strong> The study in <d-cite key="ba2014deep"></d-cite> illustrates the ability of infinity-width networks to fit any complex function, which would theoretically provide the performance gain from increasing network width. As shown in Figure <a href="#hiddensize">9</a>, the final performance or the efficiency of policy training would have varying degrees of improvement when we increase the hidden size of the network from 64 to 256 in QMIX, where <strong>QMIX-ALL-Hidden indicates all the sizes of the Recurrent Neural Network (RNN) and the Mixing network would be increased to 256, while QMIX-RNN-Hidden only refers to the size of the RNN part of the network will be changed</strong>. Also, the results reveal the spectacular effect of increasing the network width of RNN, which would allow for about a 20% increase in the Super-Hard scenarios <em>3s5z_vs_3s6z</em>. While the performance improvement is limited in enlarging the mixing network. We speculate that more units in the network are needed to represent the complex temporal context information in RNN, which is not included in the mixing network. We advise researchers to appropriately increase the network width of RNN to achieve better performance.</p> <h3 id="exploration-steps">Exploration Steps</h3> <p>Exploration and exploitation are other classic trade-offs in reinforcement learning. Agents need some directed mechanisms to explore the states that may be of higher value or inexperienced. The most versatile method of exploration in RL is $\epsilon$-greedy action, which makes the agent select random actions with probability $\epsilon$, or select the greedy action with $1 - \epsilon$. The value of $\epsilon$ would drop-down with training and then stays at a small constant. The annealing period of $\epsilon$-greedy determines how fast the drop down will be. This exploration mechanism is usually implemented for each agent to select their action, which has been criticized by MAVEN<d-cite key="mahajan2019maven"></d-cite> for lacking joint exploratory policy over an entire episode. However, we can still get more exploration when $\epsilon$ drops slower, then we evaluate the performance of the annealing period of $\epsilon$-greedy in some Super-Hard scenarios in SMAC.</p> <p><strong>Permalink:</strong> <a href="https://github.com/hijkzzz/pymarl2/blob/45278a5f8d1e3d006811351ed5fa99d614731e7d/src/config/algs/qmix.yaml#L5-L7"> $\epsilon$-greedy exploration steps setting in the configuration file. </a></p> <div id="exploration" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/exploration.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/exploration.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/exploration.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/exploration.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 10: Experinments for the impact of ε anneal period.</div> <p><strong>Results</strong> Apparently, appropriately increasing the annealing period of $\epsilon$-greedy from 100K steps to 500K would get explicit performance gain in those hard explorated scenarios, where QMIX failed with the default setting. However, as shown in Figure <a href="#exploration">10</a>, too large steps like 1000K would also bring additional exploration noise even making the training collapse. The results above confirm the $\epsilon$-greedy mechanism is still the proper and simplest choice in MARL but should be elaboratively tuned for different tasks.</p> <h3 id="integrating-the-techniques">Integrating the Techniques</h3> <p>These techniques mentioned above indeed impact QMIX in hard cooperative scenarios of SMAC, which really catches our attention to exhaust the extreme performance of QMIX. We combine these techniques and finetune all the hyperparameters in QMIX for each scenario of SMAC. As shown in Table <a href="#table2">2</a>, the Finetuned-QMIX would almost conquer all the scenarios in SMAC and exceed the effect of the original QMIX by a large margin in some Hard and Super-Hard scenarios.</p> <p><a name="table2"> </a></p> <div class="caption"> Table 2: Best median test win rate of Finetuned-QMIX and QMIX (batch size=128) in all testing scenarios. </div> <table style="text-align: center; width: 600px; margin: 0 auto; margin-bottom:20px; margin-top:20px"> <thead> <tr> <td><b>Senarios</b></td> <td><b>Difficulty</b></td> <td><b>QMIX</b></td> <td><b>Finetuned-QMIX</b></td> </tr> </thead> <tbody> <tr> <td>10m_vs_11m</td> <td>Easy</td> <td>98%</td> <td><b>100%</b></td> </tr> <tr> <td>8m_vs_9m</td> <td>Hard</td> <td>84%</td> <td><b>100%</b></td> </tr> <tr> <td>5m_vs_6m</td> <td>Hard</td> <td>84%</td> <td><b>90%</b></td> </tr> <tr> <td>3s_vs_5z</td> <td>Hard</td> <td>96%</td> <td><b>100%</b></td> </tr> <tr> <td>bane_vs_bane</td> <td>Hard</td> <td><b>100%</b></td> <td><b>100%</b></td> </tr> <tr> <td>2c_vs_64zg</td> <td>Hard</td> <td><b>100%</b></td> <td><b>100%</b></td> </tr> <tr> <td>corridor</td> <td>Super hard</td> <td>0%</td> <td><b>100%</b></td> </tr> <tr> <td>MMM2</td> <td>Super hard</td> <td>98%</td> <td><b>100%</b></td> </tr> <tr> <td>3s5z_vs_3s6z</td> <td>Super hard</td> <td>3%</td> <td><b>93% (Hidden Size = 256)</b></td> </tr> <tr> <td>27m_vs_3s6z</td> <td>Super hard</td> <td>56%</td> <td><b>100%</b></td> </tr> <tr> <td>6h_vs_8z</td> <td>Super hard</td> <td>0%</td> <td><b>93% (λ = 0.3)</b></td> </tr> </tbody> </table> <h2 id="role-of-monotonicity-constraint">Role of Monotonicity Constraint</h2> <h3 id="amazing-performance-in-policy-based-methods">Amazing Performance in Policy-Based Methods</h3> <div id="qmix_sy" class="img-height-180 image-center img-margin-left-30"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/riit.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/riit.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/riit.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/riit.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 11: Architecture for AC-MIX: <b>|·|</b> denotes <b>absolute value operation</b>, implementing the monotonicity constraint of QMIX. <b>W</b> denotes the non-negative mixing weights. Agent $i$ denotes the agent's network, which can be trained end-to-end by maximizing the $Q_{tot}$.</div> <p>The novelty of QMIX is the IGM consistency between $\text{argmax} Q_{tot}$ and $\text{argmax} \sum_{i}^{n} Q_{i}$, which is implemented in the mixing network. <strong>We still expect to study the role of <em>monotonicity constraint</em> in MARL</strong>. Therefore, we propose an actor-critic style algorithm called Actor-Critic-Mixer (AC-MIX), which has a similar architecture to QMIX. As illustrated in Figure <a href="#qmix_sy">11</a>, we use the monotonic mixing network as a centralized critic, which integrates $Q_{i}$ of each agent, to optimize the decentralized policy networks $π^i_{θ_i}$ in an end-to-end pattern. We still add the Adaptive Entropy $\mathcal{H}(\cdot)$<d-cite key="zhou2020smarts"></d-cite> of each agent in the optimization object of Eq.(\ref{eq4}) to get more exploration, and the detail of the algorithm will be described in Appendix <a href="#A">A</a>.</p> \[\max _{\theta} \mathbb{E}_{t, s_{t}, \tau_{t}^{1}, \ldots, \tau_{t}^{n}}\left[Q_{\theta_{c}}^{\pi}\left(s_{t}, \pi_{\theta_{1}}^{1}\left(\cdot \mid \tau_{t}^{1}\right), \ldots, \pi_{\theta_{n}}^{n}\left(\cdot \mid \tau_{t}^{n}\right)\right) + \mathbb{E}_{i}\left[\mathcal{H}\left(\pi_{\theta_{i}}^{i}\left(\cdot \mid \tau_{t}^{i}\right)\right)\right]\right] \tag{4} \label{eq4}\] <div id="riit_abla" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/monotonicity_riit.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/monotonicity_riit.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/monotonicity_riit.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/monotonicity_riit.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 12: Comparing AC-MIX w./ and w./o. monotonicity constraint (remove absolute value operation) on SMAC and Predator-Prey-2</div> <p>As the monotonicity constraint on the critic of AC-MIX is theoretically no longer required as the critic is not used for greedy action selection. We can evaluate the effects of the monotonicity constraint by removing the absolute value operation in the mixing network. The results in Figure <a href="#riit_abla">12</a> demonstrate the <em>monotonicity constraint</em> significantly improves the performance of AC-MIX. Then to explore the generality of <em>monotonicity constraints</em> in the parallel sampling framework of MARL, we extend the above experiments to VMIX<d-cite key="su2021value"></d-cite>. VMIX adds the monotonicity constraint to the value network of A2C, and learns the policy of each agent by advantage-based policy gradient<d-cite key="sutton2018reinforcement"></d-cite> as illustrated in Figure <a href="#vmix_net">13</a>. Still, the result from Figure <a href="#vmix_abla">14</a> shows that the monotonicity constraint improves the sample efficiency in value networks.</p> <div id="vmix_net" class="img-height-180 image-center img-margin-left-60"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/vmix.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/vmix.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/vmix.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/vmix.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 13. Architecture for VMIX: |·| denotes absolute value operation</div> <div id="vmix_abla" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/monotonicity_vmix.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/monotonicity_vmix.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/monotonicity_vmix.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/monotonicity_vmix.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure 14: Comparing VMIX w./ and w./o. monotonicity constraint (remove absolute value operation) on SMAC</div> <h3 id="what-is-under-the-hood">What is Under the Hood?</h3> <p>Observed from the results of previous experiments, <strong>the <em>monotonicity constraints</em> in the mixing network indeed improve performance and sample efficiency of training</strong>, but on the flip side of the coin, QMIX is still criticized for the insufficient expressive capacity of the centralized critic<d-cite key="mahajan2019maven"></d-cite>, which may cause poor performance. The abnormal question naturally occurred to us: <em>Why the performance of AC-MIX would be better than AC-MIX-nonmonotonic which aims to relax the monotonicity constraint of mixing network</em>?</p> <p>To answer this question we first need to reexamine the <strong>IGM</strong> principle. Since in QMIX, $Q_{tot}$ is decomposed by the mixing network into the sum of the weighted $[Q_i] _{i=1}^{n}$, as shown in Figure <a href="#frame">4</a>, where the weights and bias of mixing network are generated by the <em>Hypernetwork</em>, then the monotonicity in QMIX can be defined simplistically as a constraint on the relationship between \(Q_{tot}\) and each \(Q_{i}\) :</p> \[Q_{tot} = \sum_{i=1}^{N}w_{i}(s_{t}) \cdot Q_{i} + b(s_{t}), \\ w_{i} = \frac{\partial Q_{tot}}{\partial Q_{i}} \geq 0, \forall i \in N. \tag{5} \label{5}\] <p>From the sufficient condition above, the weight $w_{i}$ in <em>Mixing Network</em> would be forced to be greater or equal to zero $w_{i} \geq 0$. To put it another way, it makes the parameter space smaller for searching $w_{i}$ weights to decompose $Q_{tot}$. As illustrated in the schematic diagram <a href="#diagram">15</a>, assume there is only 1 agent in the environment, the parameter searching space will be directly halved and the optimal $w_{1}$ will be found in the region where $w \geq 0$, i.e., the green region. Similarly, when the number of agents is 2 or 3, its parameter searching space for $w_i$ will be restricted to the first quadrant, and the same can be recursively extended to the case of high-dimensional parameter space. <strong>In other words, the search area of exhausting the whole joint state-action space would also be decreased exponentially by $(\frac{1}{2})^{N}$ ($N$ denotes the number of parameter space of $w_{i}$, as well as the number of agents).</strong> Then the optimal solution in the original domain cannot be expressed correctly in the restricted region. Since the essence of learning in MARL is to search for the optimal joint-policy parameterized by weights and bias of agents and mixing network, QMIX could find a satisfying policy more quickly in these <strong>reduced</strong> parameter spaces.</p> <div id="diagram" style="display:flex; margin:20px 0; gap:5px"> <div id="1_agent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/1_agent.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/1_agent.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/1_agent.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/1_agent.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="2_agent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/2_agent.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/2_agent.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/2_agent.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/2_agent.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="3_agent" class="img-height-100"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/3_agent.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/3_agent.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/3_agent.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/3_agent.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></div> <div style="margin-bottom: 20px"> <div class="caption">Figure 15: the weight parameter space diagram of different number of agents in QMIX [from-left-to-right]. (a) weight parameter space of only 1 agent; (b) weight parameter space of 2 agents; (c) weight parameter space of 3 agents.</div></div> <p>As a side effect, the global optimum may not be in the parameter space that QMIX needs to search at all due to the monotonicity of the mixing network. One effective way is to estimate the $Q_{tot}$ as accurately as possible in the hope that it could find the global optimum, this probably explains why $Q(\lambda)$ in the previous section could result in such a performance improvement in SMAC. On the other hand, we could delicately design the reward function to be approximately monotonic when we use QMIX to solve cooperative multi-agent tasks. Then adapting the algorithm to the test environment is not a good idea, after all, we still need to figure out how to use QMIX more effectively or develop other more efficient algorithms.</p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we revisited the performance of the QMIX as a baseline algorithm in the SMAC environment. We found that the application of hyperparameters and other RL techniques have a great impact on the effectiveness of QMIX. We evaluated the effect of optimizer, number of rollout processes, replay buffer size, eligibility traces, hidden size and the degree of annealed exploration on QMIX, and tried to explain their role in MARL. Furthermore, we dived into the monotonicity in QMIX, and found the absolute operation in mixing network would decrease the parameter searching space of the joint state-action area exponentially by $(\frac{1}{2})^{N}$, which would make QMIX find the satisfying policy more quickly but with the drawback of inaccurate evaluated joint value function of optimal policy. We hope that our findings will stimulate some inspiration for the value decomposition method in MARL and provoke the community to think about the performance of QMIX as a new benchmark.</p> <h2 id="authorship-credit-attribution-and-acknowledgement">Authorship, Credit Attribution and Acknowledgement</h2> <p>Jian Hu was responsible for the key ideas, open source code and all experiments, as well as the first draft of the paper.</p> <p>Siying Wang was responsible for the writing of the blog.</p> <p>Siyang Jiang participated in writing the first draft of the paper.</p> <p>Weixun Wang provided feedback on revisions.</p> <p>Siyang Jiang was supported by the fund which aims to improve scientific research capability of key construction disciplines in Guangdong province “Light-weight federal learning paradigm and its application” (No:2022ZDJS058) and Foundation for Distinguished Young Talents in Higher Education of Guangdong, China. (NO. 2022KQNCX084)</p> <h2 id="appendix">Appendix</h2> <h3 id="a-pseudo-code-of-ac-mix-">A Pseudo-code of AC-MIX<a id="A"> </a></h3> <p>In this subsection, we show the pseudo-code for the training procedure of AC-MIX. (1) Training the critic network with offline samples and 1-step TD error loss improves the sample efficiency for critic networks; (2) We find that policy networks are sensitive to old sample reuse. Training policy networks end-to-end and critic with TD($\lambda$) and online samples improve the learning stability of AC-MIX.</p> <div id="algorithm_riit" class="img-height-600 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2023/assets/img/2023-05-01-riit/algorithm_riit.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2023/assets/img/2023-05-01-riit/algorithm_riit.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2023/assets/img/2023-05-01-riit/algorithm_riit.svg-1400.webp"/> <img src="/2023/assets/img/2023-05-01-riit/algorithm_riit.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="b-hyperparameters">B HYPERPARAMETERS</h3> <p>In this subsection, we present our hyperparameters tuning process. We get the optimal hyperparameters for each algorithm by grid search, shown in Table <a href="#t3">3</a>.</p> <div class="caption"> Table 3: Hyperparameters Search on SMAC. The bold type indicates the selected hyperparameters. </div> <table style="text-align: center; width: 700px; margin: 0 auto; margin-bottom:20px; margin-top:20px;"><a name="t3"> </a> <thead> <tr> <td><b>Tricks</b></td> <td><b>QMIX</b></td> <td><b>AC-MIX</b></td> </tr> </thead> <tbody> <tr> <td>Optimizer</td> <td><b>Adam</b>,RMSProp</td> <td><b>Adam</b>,RMSProp</td> </tr> <tr> <td>Learning Rates</td> <td>0.0005, <b>0.001</b></td> <td>0.0005, <b>0.001</b></td> </tr> <tr> <td>Batch Size (episodes)</td> <td>32, 64, <b>128</b></td> <td>32, <b>64</b> </td> </tr> <tr> <td>Replay Buffer Size</td> <td><b>5000</b>, 10000, 20000</td> <td>2000, <b>5000</b>, 10000</td> </tr> <tr> <td>Q(λ)/TD(λ)</td> <td>0, 0.3, <b>0.6</b>, 0.9</td> <td>0.3, <b>0.6</b>, 0.8</td> </tr> <tr> <td>Entropy/Adaptive Entropy</td> <td>-</td> <td>0.005, 0.01, <b>0.03</b>, 0.06</td> </tr> <tr> <td>ε Anneal Steps</td> <td>50K, <b>100K, 500K</b>, 1000K</td> <td>-</td> </tr> </tbody> </table> <p><br/></p> <p><strong>Rollout Processes Number</strong>. For SMAC, 8 rollout processes for parallel sampling are used to obtain as many samples as possible from the environments at a high rate. And 4 rollout processes are used for Predator-Prey-2.</p> <p><strong>Other Settings</strong>. We set all discount factors $\gamma$ = 0.99. We update the target network every 200 episodes.</p>]]></content><author><name>Jian Hu</name></author><summary type="html"><![CDATA[QMIX, a very classical multi-agent reinforcement learning (MARL) algorithm, is often considered to be a weak performance baseline due to its representation capability limitations. However, we found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) testbed. Furthermore, the key factor of the monotonicity constraint of QMIX was found in this post, we tried to explain its role and corroborated its superior performance by combining it with another actor-critic style algorithm. We have open-sourced the code at https://github.com/hijkzzz/pymarl2 for researchers to evaluate the effects of these proposed techniques.]]></summary></entry></feed>