
@article{cranmerDiscoveringSymbolicModels2020,
	title = {Discovering symbolic models from deep learning with inductive biases},
	volume = {33},
	url = {https://arxiv.org/abs/2006.11287},
	pages = {17429--17442},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Cranmer, Miles and Sanchez Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
	date = {2020},
	keywords = {Read},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\JEEDWUR8\\Cranmer et al. - 2020 - Discovering symbolic models from deep learning wit.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\E48UYH7S\\c9f2f917078bd2db12f23c3b413d9cba-Abstract.html:text/html},
}

@inproceedings{thorpeGRANDGraphNeural2021,
	title = {{GRAND}++: Graph neural diffusion with a source term},
	url = {https://openreview.net/forum?id=EMxu-dzvJk},
	shorttitle = {{GRAND}++},
	booktitle = {International Conference on Learning Representations},
	author = {Thorpe, Matthew and Nguyen, Tan Minh and Xia, Hedi and Strohmer, Thomas and Bertozzi, Andrea and Osher, Stanley and Wang, Bao},
	date = {2021},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\2QIWI8WZ\\Thorpe et al. - 2021 - GRAND++ Graph neural diffusion with a source term.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\3GMFQGTG\\forum.html:text/html},
}

@article{rackauckasUniversalDifferentialEquations2020,
	title = {Universal differential equations for scientific machine learning},
	url = {https://arxiv.org/abs/2001.04385},
	journaltitle = {{arXiv} preprint {arXiv}:2001.04385},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	date = {2020},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\TP2LKX6F\\Rackauckas et al. - 2020 - Universal differential equations for scientific ma.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\32JEYVDU\\2001.html:text/html},
}

@inproceedings{longPdenetLearningPdes2018,
	title = {Pde-net: Learning pdes from data},
	shorttitle = {Pde-net},
	pages = {3208--3216},
	booktitle = {International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Long, Zichao and Lu, Yiping and Ma, Xianzhong and Dong, Bin},
	date = {2018},
	keywords = {Read},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\CMQ99MW6\\Long et al. - 2018 - Pde-net Learning pdes from data.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\H6G7ZYTJ\\long18a.html:text/html},
}

@article{longPDENetLearningPDEs2019,
	title = {{PDE}-Net 2.0: Learning {PDEs} from data with a numeric-symbolic hybrid deep network},
	volume = {399},
	shorttitle = {{PDE}-Net 2.0},
	pages = {108925},
	journaltitle = {Journal of Computational Physics},
	author = {Long, Zichao and Lu, Yiping and Dong, Bin},
	date = {2019},
	note = {Publisher: Elsevier},
	keywords = {Read},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\QJC68VKL\\Long et al. - 2019 - PDE-Net 2.0 Learning PDEs from data with a numeri.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\BYZWV625\\S0021999119306308.html:text/html},
}

@article{yuDeepRitzMethod2018,
	title = {The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems},
	volume = {6},
	shorttitle = {The deep Ritz method},
	pages = {1--12},
	number = {1},
	journaltitle = {Communications in Mathematics and Statistics},
	author = {Yu, Bing and E, Weinan},
	date = {2018},
	note = {Publisher: Springer},
	keywords = {Read},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\ZZ75V2AV\\Yu - 2018 - The deep Ritz method a deep learning-based numeri.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\JECIGXPC\\s40304-018-0127-z.html:text/html},
}

@article{liPhysicsinformedNeuralOperator2021,
	title = {Physics-informed neural operator for learning partial differential equations},
	url = {https://arxiv.org/abs/2111.03794},
	journaltitle = {{arXiv} preprint {arXiv}:2111.03794},
	author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	date = {2021},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\QDGI4XMG\\Li et al. - 2021 - Physics-informed neural operator for learning part.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\6PUYLHMX\\2111.html:text/html},
}

@article{raissiPhysicsinformedNeuralNetworks2019,
	title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	shorttitle = {Physics-informed neural networks},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	pages = {686--707},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	urldate = {2022-11-06},
	date = {2019-02-01},
	langid = {english},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	file = {Physics-informed neural networks_Raissi2019.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Physics-informed neural networks_Raissi2019.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\9YAPMGAE\\S0021999118307125.html:text/html},
}

@misc{pilvaLearningTimedependentPDE2022,
	title = {Learning time-dependent {PDE} solver using Message Passing Graph Neural Networks},
	url = {http://arxiv.org/abs/2204.07651},
	doi = {10.48550/arXiv.2204.07651},
	abstract = {One of the main challenges in solving time-dependent partial differential equations is to develop computationally efficient solvers that are accurate and stable. Here, we introduce a graph neural network approach to finding efficient {PDE} solvers through learning using message-passing models. We first introduce domain invariant features for {PDE}-data inspired by classical {PDE} solvers for an efficient physical representation. Next, we use graphs to represent {PDE}-data on an unstructured mesh and show that message passing graph neural networks ({MPGNN}) can parameterize governing equations, and as a result, efficiently learn accurate solver schemes for linear/nonlinear {PDEs}. We further show that the solvers are independent of the initial trained geometry, i.e. the trained solver can find {PDE} solution on different complex domains. Lastly, we show that a recurrent graph neural network approach can find a temporal sequence of solutions to a {PDE}.},
	number = {{arXiv}:2204.07651},
	publisher = {{arXiv}},
	author = {Pilva, Pourya and Zareei, Ahmad},
	urldate = {2022-12-06},
	date = {2022-04-15},
	eprinttype = {arxiv},
	eprint = {2204.07651 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\XX28KIPJ\\2204.html:text/html;Learning time-dependent PDE solver using Message Passing Graph Neural Networks_Pilva2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning time-dependent PDE solver using Message Passing Graph Neural Networks_Pilva2022.pdf:application/pdf},
}

@inproceedings{iakovlevLearningContinuoustimePDEs2021,
	title = {Learning continuous-time {PDEs} from sparse data with graph neural networks},
	url = {https://openreview.net/forum?id=aUX5Plaq7Oy},
	abstract = {The behavior of many dynamical systems follow complex, yet still unknown partial differential equations ({PDEs}). While several machine learning methods have been proposed to learn {PDEs} directly from data, previous methods are limited to discrete-time approximations or make the limiting assumption of the observations arriving at regular grids. We propose a general continuous-time differential model for dynamical systems whose governing equations are parameterized by message passing graph neural networks. The model admits arbitrary space and time discretizations, which removes constraints on the locations of observation points and time intervals between the observations. The model is trained with continuous-time adjoint method enabling efficient neural {PDE} inference. We demonstrate the model's ability to work with unstructured grids, arbitrary time steps, and noisy observations. We compare our method with existing approaches on several well-known physical systems that involve first and higher-order {PDEs} with state-of-the-art predictive performance.},
	eventtitle = {International Conference on Learning Representations},
	author = {Iakovlev, Valerii and Heinonen, Markus and Lähdesmäki, Harri},
	urldate = {2022-12-06},
	date = {2021-01-29},
	langid = {english},
	file = {Learning continuous-time PDEs from sparse data with graph neural networks_Iakovlev2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning continuous-time PDEs from sparse data with graph neural networks_Iakovlev2021.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\2HW8TE82\\forum.html:text/html},
}

@article{eliasofPdegcnNovelArchitectures2021,
	title = {Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations},
	volume = {34},
	url = {https://arxiv.org/abs/2108.01938},
	shorttitle = {Pde-gcn},
	pages = {3836--3849},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Eliasof, Moshe and Haber, Eldad and Treister, Eran},
	date = {2021},
	file = {Pde-gcn_Eliasof2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\PDE-GCN_Elisasof2021.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P49BEK56\\1f9f9d8ff75205aa73ec83e543d8b571-Abstract.html:text/html},
}

@misc{kovachkiNeuralOperatorLearning2022,
	title = {Neural Operator: Learning Maps Between Function Spaces},
	url = {http://arxiv.org/abs/2108.08481},
	doi = {10.48550/arXiv.2108.08481},
	shorttitle = {Neural Operator},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations ({PDEs}). We consider standard {PDEs} such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional {PDE} solvers.},
	number = {{arXiv}:2108.08481},
	publisher = {{arXiv}},
	author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	urldate = {2022-12-06},
	date = {2022-10-06},
	eprinttype = {arxiv},
	eprint = {2108.08481 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\CGRPH5AU\\2108.html:text/html;Neural Operator_Kovachki2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Neural Operator_Kovachki2022.pdf:application/pdf},
}

@misc{guptaMultispatiotemporalscaleGeneralizedPDE2022,
	title = {Towards Multi-spatiotemporal-scale Generalized {PDE} Modeling},
	url = {http://arxiv.org/abs/2209.15616},
	doi = {10.48550/arXiv.2209.15616},
	abstract = {Partial differential equations ({PDEs}) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. Various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators ({FNOs}), which give a natural handle over local \& global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, generalizing across different equation parameters or time-scales still remains a challenge. In this work, we make a comprehensive comparison between various {FNO}, {ResNet}, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. We further analyze the design considerations for using {FNO} layers to improve performance of U-Net architectures without major degradation of computational cost. Finally, we show promising results on generalization to different {PDE} parameters and time-scales with a single surrogate model. Source code for our {PyTorch} benchmark framework is available at https://github.com/microsoft/pdearena.},
	number = {{arXiv}:2209.15616},
	publisher = {{arXiv}},
	author = {Gupta, Jayesh K. and Brandstetter, Johannes},
	urldate = {2022-12-06},
	date = {2022-11-15},
	eprinttype = {arxiv},
	eprint = {2209.15616 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Gupta and Brandstetter - 2022 - Towards Multi-spatiotemporal-scale Generalized PDE.pdf:C\:\\Users\\yolan\\Zotero\\storage\\94FD6ZFE\\Gupta and Brandstetter - 2022 - Towards Multi-spatiotemporal-scale Generalized PDE.pdf:application/pdf},
}

@misc{cuomoScientificMachineLearning2022,
	title = {Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next},
	url = {http://arxiv.org/abs/2201.05624},
	doi = {10.48550/arXiv.2201.05624},
	shorttitle = {Scientific Machine Learning through Physics-Informed Neural Networks},
	abstract = {Physics-Informed Neural Networks ({PINN}) are neural networks ({NNs}) that encode model equations, like Partial Differential Equations ({PDE}), as a component of the neural network itself. {PINNs} are nowadays used to solve {PDEs}, fractional equations, integral-differential equations, and stochastic {PDEs}. This novel methodology has arisen as a multi-task learning framework in which a {NN} must fit observed data while reducing a {PDE} residual. This article provides a comprehensive review of the literature on {PINNs}: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla {PINN}, as well as many other variants, such as physics-constrained neural networks ({PCNN}), variational hp-{VPINN}, and conservative {PINN} ({CPINN}). The study indicates that most research has focused on customizing the {PINN} through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which {PINNs} have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method ({FEM}), advancements are still possible, most notably theoretical issues that remain unresolved.},
	number = {{arXiv}:2201.05624},
	publisher = {{arXiv}},
	author = {Cuomo, Salvatore and di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	urldate = {2022-12-08},
	date = {2022-06-07},
	eprinttype = {arxiv},
	eprint = {2201.05624 [physics]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Physics - Data Analysis, Statistics and Probability, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\8WVADYWX\\2201.html:text/html;Scientific Machine Learning through Physics-Informed Neural Networks_Cuomo2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Scientific Machine Learning through Physics-Informed Neural Networks_Cuomo2022.pdf:application/pdf},
}

@article{luDeepONetLearningNonlinear2021,
	title = {{DeepONet}: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	url = {http://arxiv.org/abs/1910.03193},
	doi = {10.1038/s42256-021-00302-5},
	shorttitle = {{DeepONet}},
	abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks ({DeepONets}) to learn operators accurately and efficiently from a relatively small dataset. A {DeepONet} consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,{\textbackslash}dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that {DeepONet} significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
	pages = {218--229},
	number = {3},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	urldate = {2022-12-08},
	date = {2021-03-18},
	eprinttype = {arxiv},
	eprint = {1910.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P6U86A7R\\1910.html:text/html;DeepONet_Lu2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\DeepONet_Lu2021.pdf:application/pdf},
}

@misc{brandstetterLiePointSymmetry2022,
	title = {Lie Point Symmetry Data Augmentation for Neural {PDE} Solvers},
	url = {http://arxiv.org/abs/2202.07643},
	abstract = {Neural networks are increasingly being used to solve partial differential equations ({PDEs}), replacing slower numerical solvers. However, a critical issue is that neural {PDE} solvers require high-quality ground truth data, which usually must come from the very solvers they are designed to replace. Thus, we are presented with a proverbial chicken-and-egg problem. In this paper, we present a method, which can partially alleviate this problem, by improving neural {PDE} solver sample complexity -- Lie point symmetry data augmentation ({LPSDA}). In the context of {PDEs}, it turns out that we are able to quantitatively derive an exhaustive list of data transformations, based on the Lie point symmetry group of the {PDEs} in question, something not possible in other application areas. We present this framework and demonstrate how it can easily be deployed to improve neural {PDE} solver sample complexity by an order of magnitude.},
	number = {{arXiv}:2202.07643},
	publisher = {{arXiv}},
	author = {Brandstetter, Johannes and Welling, Max and Worrall, Daniel E.},
	urldate = {2023-01-07},
	date = {2022-05-29},
	eprinttype = {arxiv},
	eprint = {2202.07643 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\SPLAYTDF\\2202.html:text/html;Lie Point Symmetry Data Augmentation for Neural PDE Solvers_Brandstetter2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Lie Point Symmetry Data Augmentation for Neural PDE Solvers_Brandstetter2022.pdf:application/pdf},
}

@misc{liNeuralOperatorGraph2020,
	title = {Neural Operator: Graph Kernel Network for Partial Differential Equations},
	url = {http://arxiv.org/abs/2003.03485},
	shorttitle = {Neural Operator},
	abstract = {The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations ({PDEs}) and their solutions. In this context, such learned networks can generalize among different approximation methods for the {PDE} (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.},
	number = {{arXiv}:2003.03485},
	publisher = {{arXiv}},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	urldate = {2023-01-07},
	date = {2020-03-06},
	eprinttype = {arxiv},
	eprint = {2003.03485 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\FVPQV4EA\\2003.html:text/html;Neural Operator_Li2020.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Neural Operator_Li2020.pdf:application/pdf},
}

@online{AutoregressiveModelsDeep2019,
	title = {Autoregressive Models in Deep Learning — A Brief Survey},
	url = {https://www.georgeho.org/deep-autoregressive-models/},
	abstract = {My current project involves working with deep autoregressive models: a class of remarkable neural networks that aren’t usually seen on a first pass through deep learning. These notes are a quick write-up of my reading and research: I assume basic familiarity with deep learning, and aim to highlight general trends and similarities across autoregressive models, instead of commenting on individual architectures.
tldr: Deep autoregressive models are sequence models, yet feed-forward (i.},
	titleaddon = {⁂ George Ho},
	urldate = {2023-02-07},
	date = {2019-03-09},
	langid = {english},
	note = {Section: blog},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\DIARI5SK\\deep-autoregressive-models.html:text/html},
}

@article{bar-sinaiLearningDatadrivenDiscretizations2019,
	title = {Learning data-driven discretizations for partial differential equations},
	volume = {116},
	url = {https://www.pnas.org/doi/10.1073/pnas.1814058116},
	doi = {10.1073/pnas.1814058116},
	abstract = {The numerical solution of partial differential equations ({PDEs}) is challenging because of the need to resolve spatiotemporal features over wide length- and timescales. Often, it is computationally intractable to resolve the finest features in the solution. The only recourse is to use approximate coarse-grained representations, which aim to accurately represent long-wavelength dynamics while properly accounting for unresolved small-scale physics. Deriving such coarse-grained equations is notoriously difficult and often ad hoc. Here we introduce data-driven discretization, a method for learning optimized approximations to {PDEs} based on actual solutions to the known underlying equations. Our approach uses neural networks to estimate spatial derivatives, which are optimized end to end to best satisfy the equations on a low-resolution grid. The resulting numerical methods are remarkably accurate, allowing us to integrate in time a collection of nonlinear equations in 1 spatial dimension at resolutions 4× to 8× coarser than is possible with standard finite-difference methods.},
	pages = {15344--15349},
	number = {31},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Bar-Sinai, Yohai and Hoyer, Stephan and Hickey, Jason and Brenner, Michael P.},
	urldate = {2023-02-07},
	date = {2019-07-30},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Learning data-driven discretizations for partial differential equations_Bar-Sinai2019.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning data-driven discretizations for partial differential equations_Bar-Sinai2019.pdf:application/pdf},
}

@inproceedings{hsiehLearningNeuralPDE2019,
	title = {Learning Neural {PDE} Solvers with Convergence Guarantees},
	url = {https://openreview.net/forum?id=rklaWn0qK7},
	abstract = {Partial differential equations ({PDEs}) are widely used across the physical and computational sciences. Decades of research and engineering went into designing fast iterative solution methods. Existing solvers are general purpose, but may be sub-optimal for specific classes of problems. In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain. We achieve this goal by learning to modify the updates of an existing solver using a deep neural network. Crucially, our approach is proven to preserve strong correctness and convergence guarantees. After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.},
	eventtitle = {International Conference on Learning Representations},
	author = {Hsieh, Jun-Ting and Zhao, Shengjia and Eismann, Stephan and Mirabella, Lucia and Ermon, Stefano},
	urldate = {2023-02-07},
	date = {2019-04-25},
	langid = {english},
	file = {Learning Neural PDE Solvers with Convergence Guarantees_Hsieh2019.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning Neural PDE Solvers with Convergence Guarantees_Hsieh2019.pdf:application/pdf},
}

@misc{ruthottoDeepNeuralNetworks2018,
	title = {Deep Neural Networks Motivated by Partial Differential Equations},
	url = {http://arxiv.org/abs/1804.04272},
	doi = {10.48550/arXiv.1804.04272},
	abstract = {Partial differential equations ({PDEs}) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, {PDE}-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain {PDEs}. Posing image processing problems in the infinite dimensional setting provides powerful tools for their analysis and solution. Over the last few decades, the reinterpretation of classical image processing problems through the {PDE} lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction. In this paper, we establish a new {PDE}-interpretation of a class of deep convolutional neural networks ({CNN}) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks ({ResNet}), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep {ResNets} still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning. Guided by well-established {PDE} theory, we derive three new {ResNet} architectures that fall into two new classes: parabolic and hyperbolic {CNNs}. We demonstrate how {PDE} theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new {CNN} architectures using numerical experiments.},
	number = {{arXiv}:1804.04272},
	publisher = {{arXiv}},
	author = {Ruthotto, Lars and Haber, Eldad},
	urldate = {2023-02-07},
	date = {2018-12-10},
	eprinttype = {arxiv},
	eprint = {1804.04272 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 65K10, 68T45, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\U7M94NGT\\1804.html:text/html;Deep Neural Networks Motivated by Partial Differential Equations_Ruthotto2018.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Deep Neural Networks Motivated by Partial Differential Equations_Ruthotto2018.pdf:application/pdf},
}

@online{HeatHeatEquation,
	title = {Heat \#1: The Heat Equation (and a Classical Solver) {\textbar} Inductiva Research Labs},
	url = {https://inductiva.ai/blog/article/heat-1-an-introduction},
	urldate = {2023-02-07},
	file = {Heat #1\: The Heat Equation (and a Classical Solver) | Inductiva Research Labs:C\:\\Users\\yolan\\Zotero\\storage\\NZCHEMT5\\heat-1-an-introduction.html:text/html},
}

@misc{goswamiPhysicsInformedDeepNeural2022,
	title = {Physics-Informed Deep Neural Operator Networks},
	url = {http://arxiv.org/abs/2207.05748},
	abstract = {Standard neural networks can approximate general nonlinear operators, represented either explicitly by a combination of mathematical operators, e.g., in an advection-diffusion-reaction partial differential equation, or simply as a black box, e.g., a system-of-systems. The first neural operator was the Deep Operator Network ({DeepONet}), proposed in 2019 based on rigorous approximation theory. Since then, a few other less general operators have been published, e.g., based on graph neural networks or Fourier transforms. For black box systems, training of neural operators is data-driven only but if the governing equations are known they can be incorporated into the loss function during training to develop physics-informed neural operators. Neural operators can be used as surrogates in design problems, uncertainty quantification, autonomous systems, and almost in any application requiring real-time inference. Moreover, independently pre-trained {DeepONets} can be used as components of a complex multi-physics system by coupling them together with relatively light training. Here, we present a review of {DeepONet}, the Fourier neural operator, and the graph neural operator, as well as appropriate extensions with feature expansions, and highlight their usefulness in diverse applications in computational mechanics, including porous media, fluid mechanics, and solid mechanics.},
	number = {{arXiv}:2207.05748},
	publisher = {{arXiv}},
	author = {Goswami, Somdatta and Bora, Aniruddha and Yu, Yue and Karniadakis, George Em},
	urldate = {2023-02-07},
	date = {2022-07-17},
	eprinttype = {arxiv},
	eprint = {2207.05748 [cs, math]},
	note = {version: 2},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@online{UnderstandingLSTMNetworks,
	title = {Understanding {LSTM} Networks -- colah's blog},
	url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2023-02-07},
	file = {Understanding LSTM Networks -- colah's blog:C\:\\Users\\yolan\\Zotero\\storage\\EL4NIQWC\\2015-08-Understanding-LSTMs.html:text/html},
}

@online{WaveNetGenerativeModel,
	title = {{WaveNet}: A generative model for raw audio},
	url = {https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio},
	shorttitle = {{WaveNet}},
	abstract = {This post presents {WaveNet}, a deep generative model of raw audio waveforms. We show that {WaveNets} are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50\%.},
	urldate = {2023-02-07},
	langid = {english},
}

@online{joshiTransformersAreGraph2020,
	title = {Transformers are Graph Neural Networks},
	url = {https://graphdeeplearning.github.io/post/transformers-are-gnns/},
	abstract = {Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications?
Besides the obvious ones–recommendation systems at Pinterest, Alibaba and Twitter–a slightly nuanced success story is the Transformer architecture, which has taken the {NLP} industry by storm.
Through this post, I want to establish links between Graph Neural Networks ({GNNs}) and Transformers. I’ll talk about the intuitions behind model architectures in the {NLP} and {GNN} communities, make connections using equations and figures, and discuss how we could work together to drive progress.},
	titleaddon = {{NTU} Graph Deep Learning Lab},
	author = {Joshi, Chaitanya},
	urldate = {2023-02-08},
	date = {2020-02-12},
	langid = {english},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\2RKB8T9W\\transformers-are-gnns.html:text/html},
}

@incollection{hairerClassicalMathematicalTheory1993,
	location = {Berlin, Heidelberg},
	title = {Classical Mathematical Theory},
	isbn = {978-3-540-78862-1},
	url = {https://doi.org/10.1007/978-3-540-78862-1_1},
	series = {Springer Series in Computational Mathematics},
	abstract = {This first chapter contains the classical theory of differential equations, which we judge useful and important for a profound understanding of numerical processes and phenomena. It will also be the occasion of presenting interesting examples of differential equations and their properties.},
	pages = {1--128},
	booktitle = {Solving Ordinary Differential Equations I: Nonstiff Problems},
	publisher = {Springer},
	editor = {Hairer, Ernst and Wanner, Gerhard and Nørsett, Syvert P.},
	urldate = {2023-02-08},
	date = {1993},
	langid = {english},
	doi = {10.1007/978-3-540-78862-1_1},
}

@misc{chamberlainGRANDGraphNeural2021a,
	title = {{GRAND}: Graph Neural Diffusion},
	url = {http://arxiv.org/abs/2106.10934},
	doi = {10.48550/arXiv.2106.10934},
	shorttitle = {{GRAND}},
	abstract = {We present Graph Neural Diffusion ({GRAND}) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks ({GNNs}) as discretisations of an underlying {PDE}. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of {GNNs} that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of {GRAND}, which achieve competitive results on many standard graph benchmarks.},
	number = {{arXiv}:2106.10934},
	publisher = {{arXiv}},
	author = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria and Webb, Stefan and Rossi, Emanuele and Bronstein, Michael M.},
	urldate = {2023-02-08},
	date = {2021-09-22},
	eprinttype = {arxiv},
	eprint = {2106.10934 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\ADI2FQ6V\\2106.html:text/html;GRAND_Chamberlain2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\GRAND_Chamberlain22.pdf:application/pdf},
}

@book{straussPartialDifferentialEquations2007,
	title = {Partial differential equations: An introduction},
	shorttitle = {Partial differential equations},
	publisher = {John Wiley \& Sons},
	author = {Strauss, Walter A.},
	date = {2007},
	file = {Partial differential equations_Strauss2007.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Partial differential equations_Strauss2007.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P22Q24X5\\books.html:text/html},
}

@inproceedings{greenfeldLearningOptimizeMultigrid2019a,
	title = {Learning to Optimize Multigrid {PDE} Solvers},
	url = {https://proceedings.mlr.press/v97/greenfeld19a.html},
	abstract = {Constructing fast numerical solvers for partial differential equations ({PDEs}) is crucial for many scientific disciplines. A leading technique for solving large-scale {PDEs} is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from discretized {PDEs} to prolongation operators for a broad class of 2D diffusion problems. We train a neural network once for the entire class of {PDEs}, using an efficient and unsupervised loss function. Our tests demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2415--2423},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Greenfeld, Daniel and Galun, Meirav and Basri, Ronen and Yavneh, Irad and Kimmel, Ron},
	urldate = {2023-02-08},
	date = {2019-05-24},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Learning to Optimize Multigrid PDE Solvers_Greenfeld2019.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Learning to Optimize Multigrid PDE Solvers_Greenfeld3.pdf:application/pdf;Supplementary PDF:C\:\\Users\\yolan\\Zotero\\storage\\AUFTMT63\\Greenfeld et al. - 2019 - Learning to Optimize Multigrid PDE Solvers.pdf:application/pdf},
}

@misc{liFourierNeuralOperator2021,
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations ({PDEs}), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of {PDEs}, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first {ML}-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional {PDE} solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	number = {{arXiv}:2010.08895},
	publisher = {{arXiv}},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	urldate = {2023-02-08},
	date = {2021-05-16},
	eprinttype = {arxiv},
	eprint = {2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\RNE3GAEH\\2010.html:text/html;Fourier Neural Operator for Parametric Partial Differential Equations_Li2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Fourier Neural Operator for Parametric Partial Differential Equations_Li22.pdf:application/pdf},
}

@book{chakravertyAdvancedNumericalSemiAnalytical2019,
	title = {Advanced Numerical and Semi-Analytical Methods for Differential Equations},
	isbn = {978-1-119-42342-3},
	abstract = {Examines numerical and semi-analytical methods for differential equations that can be used for solving practical {ODEs} and {PDEs} This student-friendly book deals with various approaches for solving differential equations numerically or semi-analytically depending on the type of equations and offers simple example problems to help readers along. Featuring both traditional and recent methods, Advanced Numerical and Semi Analytical Methods for Differential Equations begins with a review of basic numerical methods. It then looks at Laplace, Fourier, and weighted residual methods for solving differential equations. A new challenging method of Boundary Characteristics Orthogonal Polynomials ({BCOPs}) is introduced next. The book then discusses Finite Difference Method ({FDM}), Finite Element Method ({FEM}), Finite Volume Method ({FVM}), and Boundary Element Method ({BEM}). Following that, analytical/semi analytic methods like Akbari Ganji's Method ({AGM}) and Exp-function are used to solve nonlinear differential equations. Nonlinear differential equations using semi-analytical methods are also addressed, namely Adomian Decomposition Method ({ADM}), Homotopy Perturbation Method ({HPM}), Variational Iteration Method ({VIM}), and Homotopy Analysis Method ({HAM}). Other topics covered include: emerging areas of research related to the solution of differential equations based on differential quadrature and wavelet approach; combined and hybrid methods for solving differential equations; as well as an overview of fractal differential equations. Further, uncertainty in term of intervals and fuzzy numbers have also been included, along with the interval finite element method. This book:  Discusses various methods for solving linear and nonlinear {ODEs} and {PDEs} Covers basic numerical techniques for solving differential equations along with various discretization methods Investigates nonlinear differential equations using semi-analytical methods Examines differential equations in an uncertain environment Includes a new scenario in which uncertainty (in term of intervals and fuzzy numbers) has been included in differential equations Contains solved example problems, as well as some unsolved problems for self-validation of the topics covered   Advanced Numerical and Semi Analytical Methods for Differential Equations is an excellent text for graduate as well as post graduate students and researchers studying various methods for solving differential equations, numerically and semi-analytically.},
	pagetotal = {256},
	publisher = {John Wiley \& Sons},
	author = {Chakraverty, Snehashish and Mahato, Nisha and Karunakar, Perumandla and Rao, Tharasi Dilleswar},
	date = {2019-04-16},
	langid = {english},
	note = {Google-Books-{ID}: j6uMDwAAQBAJ},
	keywords = {Mathematics / Differential Equations / General, Mathematics / Mathematical Analysis},
}

@book{bartelsNumericalApproximationPartial,
	title = {Numerical Approximation of Partial Differential Equations},
	url = {https://link.springer.com/book/10.1007/978-3-319-32354-1},
	author = {Bartels, Soren},
	urldate = {2023-02-08},
	langid = {english},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P2T8CAGU\\978-3-319-32354-1.html:text/html},
}

@misc{morrisWeisfeilerLemanGo2022,
	title = {Weisfeiler and Leman go Machine Learning: The Story so far},
	url = {http://arxiv.org/abs/2112.09992},
	doi = {10.48550/arXiv.2112.09992},
	shorttitle = {Weisfeiler and Leman go Machine Learning},
	abstract = {In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.},
	number = {{arXiv}:2112.09992},
	publisher = {{arXiv}},
	author = {Morris, Christopher and Lipman, Yaron and Maron, Haggai and Rieck, Bastian and Kriege, Nils M. and Grohe, Martin and Fey, Matthias and Borgwardt, Karsten},
	urldate = {2023-02-08},
	date = {2022-12-08},
	eprinttype = {arxiv},
	eprint = {2112.09992 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\GR7XCVZA\\2112.html:text/html;Weisfeiler and Leman go Machine Learning_Morris2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Weisfeiler and Leman go Machine Learning_Morris2022.pdf:application/pdf},
}

@article{meurisMachinelearningbasedSpectralMethods2023,
	title = {Machine-learning-based spectral methods for partial differential equations},
	volume = {13},
	rights = {2023 Battelle Memorial Institute},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-26602-3},
	doi = {10.1038/s41598-022-26602-3},
	abstract = {Spectral methods are an important part of scientific computing’s arsenal for solving partial differential equations ({PDEs}). However, their applicability and effectiveness depend crucially on the choice of basis functions used to expand the solution of a {PDE}. The last decade has seen the emergence of deep learning as a strong contender in providing efficient representations of complex functions. In the current work, we present an approach for combining deep neural networks with spectral methods to solve {PDEs}. In particular, we use a deep learning technique known as the Deep Operator Network ({DeepONet}) to identify candidate functions on which to expand the solution of {PDEs}. We have devised an approach that uses the candidate functions provided by the {DeepONet} as a starting point to construct a set of functions that have the following properties: (1) they constitute a basis, (2) they are orthonormal, and (3) they are hierarchical, i.e., akin to Fourier series or orthogonal polynomials. We have exploited the favorable properties of our custom-made basis functions to both study their approximation capability and use them to expand the solution of linear and nonlinear time-dependent {PDEs}. The proposed approach advances the state of the art and versatility of spectral methods and, more generally, promotes the synergy between traditional scientific computing and machine learning.},
	pages = {1739},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Meuris, Brek and Qadeer, Saad and Stinis, Panos},
	urldate = {2023-03-17},
	date = {2023-01-31},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science},
	file = {Machine-learning-based spectral methods for partial differential equations_Meuris2023.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Machine-learning-based spectral methods for partial differential equations_Meuris2023.pdf:application/pdf},
}

@misc{mullerDeepRitzRevisited2020,
	title = {Deep Ritz revisited},
	url = {http://arxiv.org/abs/1912.03937},
	doi = {10.48550/arXiv.1912.03937},
	abstract = {Recently, progress has been made in the application of neural networks to the numerical analysis of partial differential equations ({PDEs}). In the latter the variational formulation of the Poisson problem is used in order to obtain an objective function - a regularised Dirichlet energy - that was used for the optimisation of some neural networks. In this notes we use the notion of \${\textbackslash}Gamma\$-convergence to show that {ReLU} networks of growing architecture that are trained with respect to suitably regularised Dirichlet energies converge to the true solution of the Poisson problem. We discuss how this approach generalises to arbitrary variational problems under certain universality assumptions of neural networks and see that this covers some nonlinear stationary {PDEs} like the \$p\$-Laplace.},
	number = {{arXiv}:1912.03937},
	publisher = {{arXiv}},
	author = {Müller, Johannes and Zeinhofer, Marius},
	urldate = {2023-04-18},
	date = {2020-01-10},
	eprinttype = {arxiv},
	eprint = {1912.03937 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Analysis of {PDEs}, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\XBIJ4KBN\\1912.html:text/html;Deep Ritz revisited_Müller2020.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Deep Ritz revisited_Müller2020.pdf:application/pdf},
}

@misc{krishnapriyanCharacterizingPossibleFailure2021,
	title = {Characterizing possible failure modes in physics-informed neural networks},
	url = {http://arxiv.org/abs/2109.01050},
	doi = {10.48550/arXiv.2109.01050},
	abstract = {Recent work in scientific machine learning has developed so-called physics-informed neural network ({PINN}) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing {PINN} methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in {PINNs}, which involves {PDE}-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the {NN} architecture, but that the {PINN}'s setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the {PINN}'s loss term starts from a simple {PDE} regularization, and becomes progressively more complex as the {NN} gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular {PINN} training.},
	number = {{arXiv}:2109.01050},
	publisher = {{arXiv}},
	author = {Krishnapriyan, Aditi S. and Gholami, Amir and Zhe, Shandian and Kirby, Robert M. and Mahoney, Michael W.},
	urldate = {2023-04-19},
	date = {2021-11-11},
	eprinttype = {arxiv},
	eprint = {2109.01050 [physics]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Physics - Computational Physics, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\A5ED9VLI\\2109.html:text/html;Characterizing possible failure modes in physics-informed neural networks_Krishnapriyan2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Characterizing possible failure modes in physics-informed neural networks_Krishnapriyan2021.pdf:application/pdf},
}

@misc{pathakFourCastNetGlobalDatadriven2022,
	title = {{FourCastNet}: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators},
	url = {http://arxiv.org/abs/2202.11214},
	doi = {10.48550/arXiv.2202.11214},
	shorttitle = {{FourCastNet}},
	abstract = {{FourCastNet}, short for Fourier Forecasting Neural Network, is a global data-driven weather forecasting model that provides accurate short to medium-range global predictions at \$0.25{\textasciicircum}\{{\textbackslash}circ\}\$ resolution. {FourCastNet} accurately forecasts high-resolution, fast-timescale variables such as the surface wind speed, precipitation, and atmospheric water vapor. It has important implications for planning wind energy resources, predicting extreme weather events such as tropical cyclones, extra-tropical cyclones, and atmospheric rivers. {FourCastNet} matches the forecasting accuracy of the {ECMWF} Integrated Forecasting System ({IFS}), a state-of-the-art Numerical Weather Prediction ({NWP}) model, at short lead times for large-scale variables, while outperforming {IFS} for variables with complex fine-scale structure, including precipitation. {FourCastNet} generates a week-long forecast in less than 2 seconds, orders of magnitude faster than {IFS}. The speed of {FourCastNet} enables the creation of rapid and inexpensive large-ensemble forecasts with thousands of ensemble-members for improving probabilistic forecasting. We discuss how data-driven deep learning models such as {FourCastNet} are a valuable addition to the meteorology toolkit to aid and augment {NWP} models.},
	number = {{arXiv}:2202.11214},
	publisher = {{arXiv}},
	author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
	urldate = {2023-04-19},
	date = {2022-02-22},
	eprinttype = {arxiv},
	eprint = {2202.11214 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\VTU4JUES\\2202.html:text/html;FourCastNet_Pathak2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\FourCastNet_Pathak2022.pdf:application/pdf},
}

@misc{rahmanUshapedNeuralOperators2022,
	title = {U-{NO}: U-shaped Neural Operators},
	url = {http://arxiv.org/abs/2204.11127},
	doi = {10.48550/arXiv.2204.11127},
	shorttitle = {U-{NO}},
	abstract = {Neural operators generalize classical neural networks to maps between infinite-dimensional spaces, e.g. function spaces. Prior works on neural operators proposed a series of novel architectures to learn such maps and demonstrated unprecedented success in learning solution operators of partial differential equations. Due to their close proximity to fully connected architectures, these models mainly suffer from high memory usage and are generally limited to shallow deep learning models. In this paper, we propose U-shaped Neural Operator (U-{NO}), a U-shaped memory enhanced architecture that allows for deeper neural operators. U-{NOs} exploit the problem structures in function predictions and demonstrate fast training, data efficiency, and robustness with respect to hyperparameters choices. We study the performance of U-{NO} on {PDE} benchmarks, namely, Darcy's flow law and the Navier-Stokes equations. We show that U-{NO} results in an average of 14\% and 34\% prediction improvement on Darcy's flow and turbulent Navier-Stokes equations, respectively, over the state of art. On Navier-Stokes 3D spatio-temporal operator learning task, we show U-{NO} provides 40\% improvement over the state of art methods.},
	number = {{arXiv}:2204.11127},
	publisher = {{arXiv}},
	author = {Rahman, Md Ashiqur and Ross, Zachary E. and Azizzadenesheli, Kamyar},
	urldate = {2023-04-19},
	date = {2022-05-26},
	eprinttype = {arxiv},
	eprint = {2204.11127 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\P3P2FFK8\\2204.html:text/html;U-NO_Rahman2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\U-NO_Rahman2022.pdf:application/pdf},
}

@misc{tripuraWaveletNeuralOperator2022,
	title = {Wavelet neural operator: a neural operator for parametric partial differential equations},
	url = {http://arxiv.org/abs/2205.02191},
	doi = {10.48550/arXiv.2205.02191},
	shorttitle = {Wavelet neural operator},
	abstract = {With massive advancements in sensor technologies and Internet-of-things, we now have access to terabytes of historical data; however, there is a lack of clarity in how to best exploit the data to predict future events. One possible alternative in this context is to utilize operator learning algorithm that directly learn nonlinear mapping between two functional spaces; this facilitates real-time prediction of naturally arising complex evolutionary dynamics. In this work, we introduce a novel operator learning algorithm referred to as the Wavelet Neural Operator ({WNO}) that blends integral kernel with wavelet transformation. {WNO} harnesses the superiority of the wavelets in time-frequency localization of the functions and enables accurate tracking of patterns in spatial domain and effective learning of the functional mappings. Since the wavelets are localized in both time/space and frequency, {WNO} can provide high spatial and frequency resolution. This offers learning of the finer details of the parametric dependencies in the solution for complex problems. The efficacy and robustness of the proposed {WNO} are illustrated on a wide array of problems involving Burger's equation, Darcy flow, Navier-Stokes equation, Allen-Cahn equation, and Wave advection equation. Comparative study with respect to existing operator learning frameworks are presented. Finally, the proposed approach is used to build a digital twin capable of predicting Earth's air temperature based on available historical data.},
	number = {{arXiv}:2205.02191},
	publisher = {{arXiv}},
	author = {Tripura, Tapas and Chakraborty, Souvik},
	urldate = {2023-04-19},
	date = {2022-05-04},
	eprinttype = {arxiv},
	eprint = {2205.02191 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\GTAD4QIJ\\2205.html:text/html;Wavelet neural operator_Tripura2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Wavelet neural operator_Tripura2022.pdf:application/pdf},
}

@misc{fanaskovSpectralNeuralOperators2022,
	title = {Spectral Neural Operators},
	url = {http://arxiv.org/abs/2205.10573},
	doi = {10.48550/arXiv.2205.10573},
	abstract = {A plentitude of applications in scientific computing requires the approximation of mappings between Banach spaces. Recently introduced Fourier Neural Operator ({FNO}) and Deep Operator Network ({DeepONet}) can provide this functionality. For both of these neural operators, the input function is sampled on a given grid (uniform for {FNO}), and the output function is parametrized by a neural network. We argue that this parametrization leads to 1) opaque output that is hard to analyze and 2) systematic bias caused by aliasing errors in the case of {FNO}. The alternative, advocated in this article, is to use Chebyshev and Fourier series for both domain and codomain. The resulting Spectral Neural Operator ({SNO}) has transparent output, never suffers from aliasing, and may include many exact (lossless) operations on functions. The functionality is based on well-developed fast, and stable algorithms from spectral methods. The implementation requires only standard numerical linear algebra. Our benchmarks show that for many operators, {SNO} is superior to {FNO} and {DeepONet}.},
	number = {{arXiv}:2205.10573},
	publisher = {{arXiv}},
	author = {Fanaskov, V. and Oseledets, I.},
	urldate = {2023-04-19},
	date = {2022-05-21},
	eprinttype = {arxiv},
	eprint = {2205.10573 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\GBFLYYLP\\2205.html:text/html;Spectral Neural Operators_Fanaskov2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Spectral Neural Operators_Fanaskov2022.pdf:application/pdf},
}

@inproceedings{umSolverintheLoopLearningDifferentiable2020,
	title = {Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative {PDE}-Solvers},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/43e4e6a6f341e00671e123714de019a8-Abstract.html},
	shorttitle = {Solver-in-the-Loop},
	abstract = {Finding accurate solutions to partial differential equations ({PDEs}) is a crucial task in all scientific and engineering disciplines. It has recently been shown that machine learning methods can improve the solution accuracy by correcting for effects not captured by the discretized {PDE}. We target the problem of reducing numerical errors of iterative {PDE} solvers and compare different learning approaches for finding complex correction functions. We find that previously used learning approaches are significantly outperformed by methods that integrate the solver into the training loop and thereby allow the model to interact with the {PDE} during training. This provides the model with realistic input distributions that take previous corrections into account, yielding improvements in accuracy with stable rollouts of several hundred recurrent evaluation steps and surpassing even tailored supervised variants. We highlight the performance of the differentiable physics networks for a wide variety of {PDEs}, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows.},
	pages = {6111--6122},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Um, Kiwon and Brand, Robert and Fei, Yun (Raymond) and Holl, Philipp and Thuerey, Nils},
	urldate = {2023-04-20},
	date = {2020},
	file = {Solver-in-the-Loop_Um2020.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Solver-in-the-Loop_Um2020.pdf:application/pdf},
}

@misc{thuereyPhysicsbasedDeepLearning2022,
	title = {Physics-based Deep Learning},
	url = {http://arxiv.org/abs/2109.05237},
	doi = {10.48550/arXiv.2109.05237},
	abstract = {This digital book contains a practical and comprehensive introduction of everything related to deep learning in the context of physical simulations. As much as possible, all topics come with hands-on code examples in the form of Jupyter notebooks to quickly get started. Beyond standard supervised learning from data, we'll look at physical loss constraints, more tightly coupled learning algorithms with differentiable simulations, as well as reinforcement learning and uncertainty modeling. We live in exciting times: these methods have a huge potential to fundamentally change what computer simulations can achieve.},
	number = {{arXiv}:2109.05237},
	publisher = {{arXiv}},
	author = {Thuerey, Nils and Holl, Philipp and Mueller, Maximilian and Schnell, Patrick and Trost, Felix and Um, Kiwon},
	urldate = {2023-04-22},
	date = {2022-04-25},
	eprinttype = {arxiv},
	eprint = {2109.05237 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\87RXN4BR\\2109.html:text/html},
}

@misc{mengWhenPhysicsMeets2022,
	title = {When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning},
	url = {http://arxiv.org/abs/2203.16797},
	doi = {10.48550/arXiv.2203.16797},
	shorttitle = {When Physics Meets Machine Learning},
	abstract = {Physics-informed machine learning ({PIML}), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in {PIML} and summarize them from three aspects: (1) motivations of {PIML}, (2) physics knowledge in {PIML}, (3) methods of physics knowledge integration in {PIML}. We also discuss current challenges and corresponding research opportunities in {PIML}.},
	number = {{arXiv}:2203.16797},
	publisher = {{arXiv}},
	author = {Meng, Chuizheng and Seo, Sungyong and Cao, Defu and Griesemer, Sam and Liu, Yan},
	urldate = {2023-04-22},
	date = {2022-03-31},
	eprinttype = {arxiv},
	eprint = {2203.16797 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\8LRB557Q\\2203.html:text/html;When Physics Meets Machine Learning_Meng2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\When Physics Meets Machine Learning_Meng2022.pdf:application/pdf},
}

@article{lagarisArtificialNeuralNetworks1998,
	title = {Artificial Neural Networks for Solving Ordinary and Partial Differential Equations},
	volume = {9},
	issn = {10459227},
	url = {http://arxiv.org/abs/physics/9705023},
	doi = {10.1109/72.712178},
	abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the boundary (or initial) conditions and contains no adjustable parameters. The second part is constructed so as not to affect the boundary conditions. This part involves a feedforward neural network, containing adjustable parameters (the weights). Hence by construction the boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single {ODE}'s, to systems of coupled {ODE}'s and also to {PDE}'s. In this article we illustrate the method by solving a variety of model problems and present comparisons with finite elements for several cases of partial differential equations.},
	pages = {987--1000},
	number = {5},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	shortjournal = {{IEEE} Trans. Neural Netw.},
	author = {Lagaris, I. E. and Likas, A. and Fotiadis, D. I.},
	urldate = {2023-04-22},
	date = {1998-09},
	eprinttype = {arxiv},
	eprint = {physics/9705023},
	keywords = {Physics - Computational Physics, Nonlinear Sciences - Cellular Automata and Lattice Gases, Quantum Physics},
	file = {Artificial Neural Networks for Solving Ordinary and Partial Differential_Lagaris1998.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Artificial Neural Networks for Solving Ordinary and Partial Differential_Lagaris1998.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\F6JAN3NB\\9705023.html:text/html},
}

@article{maierKnownOperatorLearning2022,
	title = {Known operator learning and hybrid machine learning in medical imaging—a review of the past, the present, and the future},
	volume = {4},
	issn = {2516-1091},
	url = {https://dx.doi.org/10.1088/2516-1091/ac5b13},
	doi = {10.1088/2516-1091/ac5b13},
	abstract = {In this article, we perform a review of the state-of-the-art of hybrid machine learning in medical imaging. We start with a short summary of the general developments of the past in machine learning and how general and specialized approaches have been in competition in the past decades. A particular focus will be the theoretical and experimental evidence pro and contra hybrid modelling. Next, we inspect several new developments regarding hybrid machine learning with a particular focus on so-called known operator learning and how hybrid approaches gain more and more momentum across essentially all applications in medical imaging and medical image analysis. As we will point out by numerous examples, hybrid models are taking over in image reconstruction and analysis. Even domains such as physical simulation and scanner and acquisition design are being addressed using machine learning grey box modelling approaches. Towards the end of the article, we will investigate a few future directions and point out relevant areas in which hybrid modelling, meta learning, and other domains will likely be able to drive the state-of-the-art ahead.},
	pages = {022002},
	number = {2},
	journaltitle = {Progress in Biomedical Engineering},
	shortjournal = {Prog. Biomed. Eng.},
	author = {Maier, Andreas and Köstler, Harald and Heisig, Marco and Krauss, Patrick and Yang, Seung Hee},
	urldate = {2023-04-22},
	date = {2022-03},
	langid = {english},
	note = {Publisher: {IOP} Publishing},
	file = {Known operator learning and hybrid machine learning in medical imaging—a review_Maier2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Known operator learning and hybrid machine learning in medical imaging—a review_Maier2022.pdf:application/pdf},
}

@article{dissanayakeNeuralnetworkbasedApproximationsSolving1994,
	title = {Neural-network-based approximations for solving partial differential equations},
	volume = {10},
	pages = {195--201},
	number = {3},
	journaltitle = {communications in Numerical Methods in Engineering},
	author = {Dissanayake, {MWMG} and Phan-Thien, Nhan},
	date = {1994},
	note = {Publisher: Wiley Online Library},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\LQ5IEJ3E\\cnm.html:text/html},
}

@article{psichogiosHybridNeuralNetworkfirst1992,
	title = {A hybrid neural network-first principles approach to process modeling},
	volume = {38},
	pages = {1499--1511},
	number = {10},
	journaltitle = {{AIChE} Journal},
	author = {Psichogios, Dimitris C. and Ungar, Lyle H.},
	date = {1992},
	note = {Publisher: Wiley Online Library},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\R3X4RN9W\\aic.html:text/html},
}

@article{wangWhenWhyPINNs2022,
	title = {When and why {PINNs} fail to train: A neural tangent kernel perspective},
	volume = {449},
	shorttitle = {When and why {PINNs} fail to train},
	pages = {110768},
	journaltitle = {Journal of Computational Physics},
	author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
	date = {2022},
	note = {Publisher: Elsevier},
	file = {Full Text:C\:\\Users\\yolan\\Zotero\\storage\\CZHUCBLD\\S002199912100663X.html:text/html},
}

@article{babuskaHpVersionFinite1987,
	title = {The hp version of the finite element method with quasiuniform meshes},
	volume = {21},
	pages = {199--238},
	number = {2},
	journaltitle = {{ESAIM}: Mathematical Modelling and Numerical Analysis-Modélisation Mathématique et Analyse Numérique},
	author = {Babuška, Ivo and Suri, Manil},
	date = {1987},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\PJQISGRS\\M2AN_1987__21_2_199_0.html:text/html;The hp version of the finite element method with quasiuniform meshes_Babuška1987.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\The hp version of the finite element method with quasiuniform meshes_Babuška1987.pdf:application/pdf},
}

@article{bungartzSparseGrids2004,
	title = {Sparse grids},
	volume = {13},
	pages = {147--269},
	journaltitle = {Acta numerica},
	author = {Bungartz, Hans-Joachim and Griebel, Michael},
	date = {2004},
	note = {Publisher: Cambridge University Press},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\RWMG6QGP\\47EA2993DB84C9D231BB96ECB26F615C.html:text/html;Sparse grids_Bungartz2004.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Sparse grids_Bungartz2004.pdf:application/pdf},
}

@book{gheorghiuSpectralMethodsDifferential2007,
	title = {Spectral methods for differential problems},
	publisher = {Casa Cărţii de Ştiinţă Cluj-Napoca},
	author = {Gheorghiu, Călin Ioan},
	date = {2007},
	file = {Spectral methods for differential problems_Gheorghiu2007.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Spectral methods for differential problems_Gheorghiu2007.pdf:application/pdf},
}

@book{shenSpectralMethodsAlgorithms2011,
	title = {Spectral methods: algorithms, analysis and applications},
	volume = {41},
	shorttitle = {Spectral methods},
	publisher = {Springer Science \& Business Media},
	author = {Shen, Jie and Tang, Tao and Wang, Li-Lian},
	date = {2011},
	file = {Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\S6J89D36\\books.html:text/html},
}

@article{shenEfficientSpectralSparse2010,
	title = {Efficient spectral sparse grid methods and applications to high-dimensional elliptic problems},
	volume = {32},
	pages = {3228--3250},
	number = {6},
	journaltitle = {{SIAM} Journal on Scientific Computing},
	author = {Shen, Jie and Yu, Haijun},
	date = {2010},
	note = {Publisher: {SIAM}},
	file = {Efficient spectral sparse grid methods and applications to high-dimensional_Shen2010.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Efficient spectral sparse grid methods and applications to high-dimensional_Shen2010.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\L2YZQILQ\\100787842.html:text/html},
}

@article{shenEfficientSpectralSparse2012,
	title = {Efficient spectral sparse grid methods and applications to high-dimensional elliptic equations {II}. Unbounded domains},
	volume = {34},
	pages = {A1141--A1164},
	number = {2},
	journaltitle = {{SIAM} Journal on Scientific Computing},
	author = {Shen, Jie and Yu, Haijun},
	date = {2012},
	note = {Publisher: {SIAM}},
	file = {Efficient spectral sparse grid methods and applications to high-dimensional_Shen2012.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Efficient spectral sparse grid methods and applications to high-dimensional_Shen2012.pdf:application/pdf;Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\9AR3NE7R\\110834950.html:text/html},
}

@inproceedings{zengerSparseGrids1991,
	title = {Sparse grids},
	pages = {86},
	booktitle = {Proceedings of the Research Workshop of the Israel Science Foundation on Multiscale Phenomenon, Modelling and Computation},
	author = {Zenger, Christoph and Hackbusch, W.},
	date = {1991},
	file = {Sparse grids_Zenger1991.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Sparse grids_Zenger1991.pdf:application/pdf},
}

@article{childsHighprecisionQuantumAlgorithms2021,
	title = {High-precision quantum algorithms for partial differential equations},
	volume = {5},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2002.07868},
	doi = {10.22331/q-2021-11-10-574},
	abstract = {Quantum computers can produce a quantum encoding of the solution of a system of differential equations exponentially faster than a classical algorithm can produce an explicit description. However, while high-precision quantum algorithms for linear ordinary differential equations are well established, the best previous quantum algorithms for linear partial differential equations ({PDEs}) have complexity \${\textbackslash}mathrm\{poly\}(1/{\textbackslash}epsilon)\$, where \${\textbackslash}epsilon\$ is the error tolerance. By developing quantum algorithms based on adaptive-order finite difference methods and spectral methods, we improve the complexity of quantum algorithms for linear {PDEs} to be \${\textbackslash}mathrm\{poly\}(d, {\textbackslash}log(1/{\textbackslash}epsilon))\$, where \$d\$ is the spatial dimension. Our algorithms apply high-precision quantum linear system algorithms to systems whose condition numbers and approximation errors we bound. We develop a finite difference algorithm for the Poisson equation and a spectral algorithm for more general second-order elliptic equations.},
	pages = {574},
	journaltitle = {Quantum},
	shortjournal = {Quantum},
	author = {Childs, Andrew M. and Liu, Jin-Peng and Ostrander, Aaron},
	urldate = {2023-04-25},
	date = {2021-11-10},
	eprinttype = {arxiv},
	eprint = {2002.07868 [quant-ph]},
	keywords = {Mathematics - Numerical Analysis, Quantum Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\yolan\\Zotero\\storage\\WY8USRPC\\2002.html:text/html;High-precision quantum algorithms for partial differential equations_Childs2021.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\High-precision quantum algorithms for partial differential equations_Childs2021.pdf:application/pdf},
}

@inproceedings{brandstetterMessagePassingNeural2022a,
	title = {Message Passing Neural {PDE} Solvers},
	url = {https://openreview.net/forum?id=vSix3HPYKSU},
	abstract = {The numerical solution of partial differential equations ({PDEs}) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and {WENO} schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed, and accuracy.},
	eventtitle = {International Conference on Learning Representations},
	author = {Brandstetter, Johannes and Worrall, Daniel E. and Welling, Max},
	urldate = {2023-04-25},
	date = {2022-01-28},
	langid = {english},
	file = {Message Passing Neural PDE Solvers_Brandstetter2022.pdf:C\:\\Users\\yolan\\OneDrive - University College London\\Research\\PDEs\\Message Passing Neural PDE Solvers_Brandstetter22.pdf:application/pdf},
}
